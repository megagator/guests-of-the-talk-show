[00:00.000 --> 00:01.560]  Daniel, it's good to hear your voice.
[00:02.240 --> 00:02.880]  Good to see you.
[00:02.880 --> 00:03.760]  Good to hear yours.
[00:03.760 --> 00:04.960]  Great to be back on the show.
[00:05.000 --> 00:07.280]  You were, uh, born in California, right?
[00:07.280 --> 00:10.320]  You're you're a native native San Jose, not San Jose.
[00:10.720 --> 00:11.340]  Never say it.
[00:11.340 --> 00:15.480]  Never say how dare you, sir.
[00:15.520 --> 00:19.200]  No, it's what I'm drawing a Santa Cruz Santa Cruz.
[00:19.240 --> 00:26.080]  I actually was born in Southern California and my parents were row roaming van life
[00:26.080 --> 00:31.440]  hippies for a while and they eventually settled in Santa Cruz, California.
[00:31.480 --> 00:33.120]  So I grew up in that town.
[00:33.560 --> 00:33.760]  Yeah.
[00:33.760 --> 00:36.640]  Have you, at this point in your life though, do you think you're, uh,
[00:36.680 --> 00:38.480]  used to the East coast weather?
[00:38.880 --> 00:39.520]  Oh yeah.
[00:39.520 --> 00:41.280]  Yeah, I'm a, I'm a good, I'm good at the weather.
[00:41.280 --> 00:45.360]  I am more, I am so used to the East coast weather that I put a lot of East
[00:45.360 --> 00:51.240]  coasters to shame because it's just like the Californian, right?
[00:51.240 --> 00:54.000]  To come into town and be like, listen, listen, I'm doing the East coast better
[00:54.000 --> 00:58.640]  than you, but for instance, I run every other day, you know, it's like my
[00:58.960 --> 01:06.160]  regimen and I get a lot of interesting looks when it's zero degrees blizzard.
[01:06.480 --> 01:12.480]  And I'm out there with like two pair of long underwear and a ball of clava.
[01:14.360 --> 01:17.000]  Tell you what, I never learned that in California.
[01:17.080 --> 01:20.160]  Our mutual friend and your relative neighbor.
[01:20.160 --> 01:25.520]  Paul is a year long runner as well, and I've, I have seen pictures of him
[01:25.520 --> 01:29.600]  selfies where there's like for frost on his beard.
[01:30.120 --> 01:30.600]  Oh yeah.
[01:30.600 --> 01:32.920]  I've had like the icicles coming off my eyebrows.
[01:32.920 --> 01:34.960]  It's like, look like a cult.
[01:34.960 --> 01:38.480]  What's the cold miser or whatever, but yeah, it's, it's kind of fun.
[01:39.080 --> 01:42.640]  It's kind of fun when, when it's so severe that people kind of
[01:42.640 --> 01:43.760]  look at you like you're crazy.
[01:43.760 --> 01:46.440]  I guess I've never really, that's the Californian in me.
[01:46.440 --> 01:49.800]  Not, not shying away from people thinking you're a little cuckoo.
[01:49.800 --> 01:51.120]  I brought it up with cut key too.
[01:51.120 --> 01:55.200]  Cause I don't know, somehow I've lived my whole life other than two years
[01:55.200 --> 01:58.320]  where I lived up there your way in Massachusetts, when I was working at
[01:58.320 --> 02:03.360]  bare bones, I've lived in the Philadelphia area, greater area from birth till now.
[02:03.400 --> 02:08.400]  So I'm used to East coast weather, but the older I get, the less used to it.
[02:08.400 --> 02:09.960]  I get, I don't know.
[02:09.960 --> 02:11.120]  It's on my mind more.
[02:11.120 --> 02:16.280]  You, I would have thought by the time I hit 50, I'd be used to it.
[02:16.320 --> 02:19.160]  And it's like, you did like, my dad doesn't seem to talk about the weather.
[02:19.160 --> 02:22.640]  It's he's, he's been through 84 years and it's, he's used to it.
[02:22.640 --> 02:27.840]  Whereas me, it's like when it gets dark now in the fall, I'm just shocked by it.
[02:27.920 --> 02:29.600]  I'm like, I can't believe it.
[02:29.600 --> 02:32.600]  And now in the spring, when it starts getting nicer and we've got all this
[02:32.600 --> 02:35.080]  wonderful daylight, I'm I'm like overjoyed.
[02:36.160 --> 02:39.160]  I'll tell you what, it's one thing I really appreciate about the
[02:39.160 --> 02:41.320]  East coast compared to California.
[02:41.640 --> 02:46.280]  It's the very thing that a lot of people hate about it is the severity of the
[02:46.280 --> 02:52.040]  variation because you don't get that kind of a variation typically in California,
[02:52.040 --> 02:56.480]  especially the part Northern California, where I grew up, but, Oh, so one of the
[02:56.480 --> 03:00.800]  first things I noticed moving out here was like, you really know when a season
[03:00.800 --> 03:07.600]  changes and it's like the seasons change in California, but it's like a 10 degree
[03:07.760 --> 03:11.400]  difference from one extreme to the other.
[03:11.800 --> 03:12.200]  Right.
[03:12.200 --> 03:17.160]  Right. Well, and like on the highway, whichever way you want to drive between
[03:17.160 --> 03:21.160]  the city and the Valley up there, you know what season it is by whether
[03:21.160 --> 03:22.880]  the mountains are burned out or green.
[03:23.440 --> 03:24.200]  Well, that's true.
[03:24.360 --> 03:26.240]  Well, yeah, well, that's true.
[03:26.240 --> 03:30.840]  Although I will add, I'll hasten to add that typically they're always burned out.
[03:31.000 --> 03:33.440]  Like typically it's not seasonal.
[03:33.440 --> 03:38.280]  It's just like, there's never, it's kind of funny that grass, particularly
[03:38.280 --> 03:43.880]  the grass it's like designed to stay like yellow and it's like only when,
[03:43.880 --> 03:48.040]  like right now, as everyone knows, probably California has had this, but
[03:48.040 --> 03:50.840]  very unprecedented amount of rain.
[03:50.880 --> 03:54.960]  California is probably just gleaming green right now.
[03:55.000 --> 03:56.160]  And it's so unusual.
[03:56.200 --> 04:00.080]  But anyway, the thing I like coming out here with the, with the variation and I
[04:00.080 --> 04:04.680]  keep noticing it every year is I forget how much I like, this is, this is lucky
[04:04.680 --> 04:07.080]  as a transplant to New England.
[04:07.080 --> 04:14.600]  It's lucky that I legitimately like every season and that's good because I notice
[04:14.600 --> 04:17.400]  every time a new season comes around, I'm like, Oh, right.
[04:17.400 --> 04:20.120]  It's I'm tired of it being fall.
[04:20.160 --> 04:21.240]  I want it to get cold.
[04:21.280 --> 04:24.600]  And then a couple months of that, I'm like, dammit, it's too cold.
[04:24.600 --> 04:26.480]  I'm ready for some spring.
[04:26.520 --> 04:32.040]  And I guess like you were saying, like if your dad, if your dad just sort of got
[04:32.040 --> 04:36.200]  used to it and he's not really, doesn't really notice or care or whatever, I'm
[04:36.200 --> 04:43.480]  noticing it always constantly, but I'm also sort of like always surprised by how
[04:43.480 --> 04:47.800]  it makes me feel like every time fall comes around, I'm like, Oh yeah, this is
[04:47.800 --> 04:48.520]  what falls like.
[04:50.120 --> 04:53.560]  Well, my dad notices, I should say, cause he, he's outdoors with one of me, he
[04:53.560 --> 04:55.560]  walks the golf course still at age 84.
[04:55.560 --> 04:59.000]  And so like when it warms up and it's good to play golf, he definitely know, but
[04:59.000 --> 05:00.920]  he just doesn't complain about it is my take.
[05:01.080 --> 05:05.720]  Whereas I, I, and never became like a snowbird who him and him and my mom do.
[05:05.720 --> 05:09.400]  I don't go to Florida or something like that, which is what I'm thinking, where
[05:09.400 --> 05:12.680]  I'm going is I think I got to find a way to go somewhere, probably not Florida,
[05:12.680 --> 05:16.440]  the way Florida is going, but somewhere warm by the time I'm old, because I can't
[05:16.440 --> 05:17.080]  take it anymore.
[05:17.080 --> 05:22.520]  But I want to move back by March because I want to be here for, for when it gets
[05:22.520 --> 05:22.920]  nice here.
[05:22.920 --> 05:26.120]  I I'm, I'm just in a good mood is all I'm saying, that's good.
[05:26.120 --> 05:26.600]  Yes.
[05:26.920 --> 05:29.880]  Well, I think the trick is if you don't want to go to Florida, I think the trick
[05:29.880 --> 05:34.600]  that from the East coast was people seem to have figured out are the islands, the
[05:34.600 --> 05:38.760]  various, like we kind of, I don't know, I don't know a lot about those cultures,
[05:38.760 --> 05:40.600]  but there's so many.
[05:41.160 --> 05:42.360]  Puerto Rico, right?
[05:42.360 --> 05:44.440]  You're staying in the U S right now.
[05:44.520 --> 05:46.040]  We vacationed there last year.
[05:46.040 --> 05:47.640]  It's really just gorgeous.
[05:47.640 --> 05:49.640]  It's just a beautiful, beautiful place.
[05:49.640 --> 05:49.960]  Yeah.
[05:49.960 --> 05:51.240]  Maybe that might be the answer.
[05:51.240 --> 05:52.200]  Speaking of good news.
[05:52.200 --> 05:52.600]  I don't know.
[05:52.600 --> 05:53.320]  Is it good news?
[05:53.320 --> 05:55.320]  We can start with some news today.
[05:55.320 --> 05:59.400]  The day we're recording Wednesday, Apple announced the, and speaking of Northern
[05:59.400 --> 06:06.920]  California, WWDC 2023, I would, I would say the most predictable date on the
[06:06.920 --> 06:09.720]  calendar, June 5th to ninth.
[06:09.720 --> 06:10.120]  Right.
[06:10.120 --> 06:13.640]  Otherwise, if it wasn't June 5th to ninth, it almost certainly would have been the
[06:13.640 --> 06:14.520]  next week.
[06:14.520 --> 06:14.760]  Yeah.
[06:14.760 --> 06:19.960]  Well, particularly now that as you noted, it's become more and more, well, I don't
[06:19.960 --> 06:20.200]  know.
[06:20.200 --> 06:26.680]  You noted that last year did turn out to be the template for mostly online with
[06:26.680 --> 06:33.880]  the in-person event, but the fact that the in-person event is at Apple HQ, Apple
[06:33.880 --> 06:38.520]  park, some, some may call it that, that, that means they don't like, we used to
[06:38.520 --> 06:41.960]  all just guess, cause we were so concerned about how to get hotel rooms and all
[06:41.960 --> 06:42.120]  that.
[06:42.120 --> 06:44.680]  We used to guess, but we knew there were constraints.
[06:44.680 --> 06:52.040]  Like what other conferences are already booked and what hotels already have full
[06:52.040 --> 06:53.080]  reservations.
[06:53.080 --> 06:59.240]  But now for a company that does like to repeat the same playbook again and again,
[06:59.240 --> 07:03.720]  like, why would they ever pick a different date if they control everything about
[07:03.720 --> 07:04.120]  it?
[07:04.120 --> 07:05.800]  Well, you mean a different location.
[07:06.600 --> 07:10.200]  Well, I mean like, well, no date, because like, cause we used to guess at like,
[07:10.200 --> 07:13.080]  well, first week, second week, third week of June.
[07:13.080 --> 07:18.440]  And like, sometimes we didn't know how much of it might be like internal problems.
[07:18.440 --> 07:22.680]  Like they wanted to finish something up or, but we mostly, I think assumed that it
[07:22.680 --> 07:25.720]  was like, well, there's going to be one of these two weeks and turns out like
[07:25.720 --> 07:27.160]  Oracle's having a big conference.
[07:27.160 --> 07:32.440]  So they can't, but now there's nothing like that to impose anything on them as
[07:32.440 --> 07:32.840]  far as I know.
[07:32.840 --> 07:34.040]  Right, right.
[07:34.040 --> 07:38.520]  Both, both me, like as a public pundit writing at Daring Fireball and, and our
[07:38.520 --> 07:44.120]  whole crew community, I was, I would even expand it to of people who wanted to go.
[07:44.120 --> 07:49.720]  We used to play all these games, looking at the Moscone schedule and seeing, oh,
[07:49.720 --> 07:55.000]  well, here's the American association of bicycle mechanics is having the thing at
[07:55.000 --> 07:57.560]  Moscone West the second week of June.
[07:57.560 --> 08:00.280]  So that's out, it's gotta be the first week, right?
[08:00.280 --> 08:03.880]  And there'd sometimes be like private event or something like that, right?
[08:04.440 --> 08:09.000]  Totally generic, but no name for what it was on the Moscone West calendar.
[08:09.000 --> 08:11.400]  And we would just think that's gotta be it.
[08:11.400 --> 08:17.560]  But you know that Apple always, I mean, one of the reasons they, every other major
[08:17.560 --> 08:21.480]  developer conference or major tech conference I know of, or conference period,
[08:21.480 --> 08:26.440]  like whether it is the American association of bicycle mechanics or whatever, they
[08:26.440 --> 08:29.800]  always, they pick their dates years in advance, right?
[08:29.800 --> 08:32.120]  It's like, I remember Macworld Expo.
[08:32.120 --> 08:37.080]  It's like, as you'd be leaving, there'd be, they'd be handing out flyers, right?
[08:37.080 --> 08:38.520]  And they'd have signup forms.
[08:38.520 --> 08:40.920]  You could sign up for next year's Macworld Expo.
[08:40.920 --> 08:42.760]  They'd be like, see you next year.
[08:42.760 --> 08:46.440]  And here's January 5th or whatever the date was.
[08:46.440 --> 08:49.480]  And they'd be trying to get you to buy tickets and sign you.
[08:49.480 --> 08:54.280]  And when I was at bare bones and we had a booth, they definitely were offering steep
[08:54.280 --> 08:58.680]  discounts or significant discounts to commit to the next year already.
[09:00.040 --> 09:04.360]  Apple's is, WWDC is the only conference, major conference I've ever known of that
[09:04.360 --> 09:08.680]  announces like two and a half months out, sometimes closer, right?
[09:08.680 --> 09:14.200]  And it's because Apple is Apple and they want to, you know, just in case you never
[09:14.200 --> 09:19.480]  know what if something's delayed, there was that one year way, way back where they
[09:19.480 --> 09:23.240]  had it in August instead of June, because something wasn't ready.
[09:24.200 --> 09:24.680]  Yeah.
[09:24.680 --> 09:28.280]  It's hard to even imagine that now it has become so predictable, right?
[09:28.280 --> 09:32.040]  But it's, it's funny that you mentioned like, there'd be like a reservation on the
[09:32.040 --> 09:35.080]  conference center for an unnamed guest.
[09:35.080 --> 09:40.120]  And it's like every other company or organization in the world, practically,
[09:40.120 --> 09:44.680]  unless it was going to be like a CIA meetup would, would be like, why wouldn't
[09:44.680 --> 09:48.840]  we want to hide the fact that we are like a vibrant company that can have public
[09:48.840 --> 09:49.240]  shows?
[09:49.240 --> 09:52.040]  So it's like another opportunity to get your name out there.
[09:53.560 --> 09:54.060]  Right.
[09:54.440 --> 09:55.960]  Predictable public shows, right?
[09:55.960 --> 10:00.440]  Like for any, for any other company is not scheduling it until weeks in advance
[10:00.440 --> 10:04.440]  would make it seem like you were flying by the skinnier pants or what's the,
[10:04.440 --> 10:05.240]  what's the phrase?
[10:05.240 --> 10:07.560]  CD your pants, CD your pants, right?
[10:07.560 --> 10:08.120]  I guess so.
[10:08.120 --> 10:08.440]  Yeah.
[10:08.440 --> 10:12.360]  Not the skin of your pants, the skin of your teeth, which doesn't make it easier
[10:12.360 --> 10:13.560]  to, all right.
[10:13.560 --> 10:14.840]  What a terrible mixed metaphor.
[10:14.840 --> 10:18.040]  I don't, I don't, I guess if you have leather pants though, you might have skin
[10:18.040 --> 10:21.800]  out of your pants, but otherwise, I don't know.
[10:21.800 --> 10:22.520]  I don't know about you.
[10:22.520 --> 10:26.040]  I don't, I don't, I'm, I'm low on leather pants at the moment.
[10:27.320 --> 10:27.880]  Yeah.
[10:27.880 --> 10:28.840]  Only chaps here.
[10:28.840 --> 10:31.880]  I do think though, I hate to complain.
[10:31.880 --> 10:33.400]  I think last year's was great.
[10:33.400 --> 10:41.080]  It felt like last year's WWDC as after the keynote was over, even before the
[10:41.080 --> 10:45.240]  state of the union, it already felt like, yeah, this is the way that they're
[10:45.240 --> 10:46.280]  going to keep doing it.
[10:46.280 --> 10:50.200]  It was, I don't know if they're going to be able to put more people on the lawn.
[10:50.920 --> 10:54.200]  It is intriguing as an event.
[10:54.200 --> 10:59.720]  What's the word organizationally that Apple park was never designed with this,
[10:59.720 --> 10:59.960]  right?
[10:59.960 --> 11:04.760]  The Steve jobs theater was designed to host the type of things that they do
[11:04.760 --> 11:11.160]  host there, but the lawn was not meant to host 1500 people for this.
[11:11.160 --> 11:13.960]  And I've taught, I talked to people who were there and they were like, yeah,
[11:13.960 --> 11:19.480]  definitely not that it was just sort of serendipity and the advantage of the
[11:19.480 --> 11:27.880]  fact that they bought so much square acreage of that area to build the ring
[11:27.880 --> 11:32.600]  building and have it surrounded by so much green space that they have the room
[11:32.600 --> 11:33.720]  to do that.
[11:33.720 --> 11:41.720]  But it wasn't meant for it by design or planning yet attending it in that area
[11:41.720 --> 11:42.840]  felt very smooth.
[11:42.840 --> 11:47.240]  You would have thought that it was part of the original spec for the campus.
[11:47.240 --> 11:48.680]  Like maybe we'll do this.
[11:48.680 --> 11:53.480]  Well, that's the nice thing about open spaces is you can use them for that kind
[11:53.480 --> 11:57.960]  of, I mean, the main challenge is I understand it was the projection of video,
[11:57.960 --> 11:59.880]  which apparently they did a brilliant job of.
[11:59.880 --> 12:01.800]  Yeah, it was, it really was.
[12:01.800 --> 12:03.640]  It was unbelievable.
[12:03.640 --> 12:07.160]  But for the, for like the rest of it, it's just kind of like an opportunity to
[12:07.160 --> 12:08.280]  mill about, right?
[12:08.280 --> 12:13.160]  Like milling about that's what good, that's what open spaces are good for.
[12:13.160 --> 12:19.240]  And when I think back to I think, you know, this, but for probably the past 10
[12:19.240 --> 12:25.880]  years or so, even when WWDC was happening in person, myself, along with other
[12:25.880 --> 12:30.360]  friends of ours, many of us didn't buy tickets for the conference.
[12:30.360 --> 12:38.760]  And the one aspect of the conference I miss or missed was the Apple bash, which
[12:38.760 --> 12:42.920]  is kind of what they're best suited now to, to host.
[12:42.920 --> 12:46.600]  So I don't, I don't know if it makes sense.
[12:46.600 --> 12:51.400]  Yes, it wasn't designed for this purpose and especially for seeding a bunch of
[12:51.400 --> 12:56.840]  people to watch a keynote event, but I hope that they lean into the advantage
[12:56.840 --> 13:02.920]  they have of it being suitable for that big milling around space, because that's
[13:02.920 --> 13:03.880]  a really nice event.
[13:03.880 --> 13:09.240]  That's the, that's the part WWDC that sort of invites everybody to just kick
[13:09.240 --> 13:16.040]  back and all meet in one place, not have any pretense of a educational or
[13:16.040 --> 13:17.240]  classroom environment.
[13:17.800 --> 13:19.720]  That's, that's my idea of a good time.
[13:19.720 --> 13:21.080]  Yeah, it is weird.
[13:21.080 --> 13:26.120]  And it does sort of, and again, there were hints last year, not just because as
[13:26.120 --> 13:31.560]  an attendee in the media and talking to other media people and talking to the
[13:31.560 --> 13:35.000]  people who I met people who there are people listening to the show right now,
[13:35.000 --> 13:39.160]  who I shook hands with and took selfies with who, cause they got the lucky
[13:39.160 --> 13:41.880]  tickets in the special event draw to be there.
[13:41.880 --> 13:43.800]  And I look forward to that again, too.
[13:43.800 --> 13:46.280]  But everybody who thought, yeah, this is really great.
[13:49.000 --> 13:52.920]  I also did talk to people from Apple and there were, again, they're Apple
[13:52.920 --> 13:57.480]  people and nobody, even someone in a position to actually state it
[13:57.480 --> 13:58.520]  authoritatively.
[13:58.520 --> 14:00.360]  Yes, we're going to do that this way every year.
[14:01.080 --> 14:06.200]  They at least gave a, like a look on their face that was like, yeah, this,
[14:06.200 --> 14:09.080]  yeah, we're really happy with the way this has gone.
[14:09.080 --> 14:12.680]  And it's almost certainly the way it's going to go.
[14:12.680 --> 14:16.040]  And I know the other thing that they did say, honest, I'll just say it, even
[14:16.040 --> 14:21.640]  just talking to Phil Schiller, who runs events, they're over the moon, happy
[14:22.360 --> 14:28.200]  with the post COVID online delivery of the developer sessions.
[14:28.200 --> 14:32.360]  There was no question that they're going to pre-tape those probably starting
[14:32.360 --> 14:33.080]  like now.
[14:33.080 --> 14:33.640]  Right.
[14:33.640 --> 14:38.120]  I'm guessing now is the time when everybody inside Apple is submitting
[14:38.120 --> 14:43.640]  that session ideas, but that the Apple is so much happier with the way that
[14:43.640 --> 14:44.760]  they turn out.
[14:44.760 --> 14:51.240]  And so getting so much better feedback from the, I guess, millions, right?
[14:51.240 --> 14:56.920]  Millions of developers around the world who watched them that it's just, for
[14:56.920 --> 15:00.840]  lack of a better word, pedagogically a vastly superior format.
[15:00.840 --> 15:07.960]  The presenters who are all practicing Apple engineers and designers and
[15:07.960 --> 15:12.520]  not, therefore not professional speakers, no longer have to deal with the, what
[15:12.520 --> 15:13.320]  do you want to call it?
[15:13.320 --> 15:16.840]  I mean, stage fright, stage fright, it's fear.
[15:16.840 --> 15:21.640]  It's just the natural anxiety that almost almost everybody has of public
[15:21.640 --> 15:26.200]  speaking, whether you actually are further down the stage fright spectrum or
[15:26.200 --> 15:28.360]  further up on the comfortable spectrum.
[15:28.360 --> 15:33.000]  It still is something that people who work full time at Apple doing real
[15:33.000 --> 15:36.200]  engineering and design work, just don't practice and therefore aren't that
[15:36.200 --> 15:40.840]  great at, even though over the years, the many, many decades of WWDC with
[15:40.840 --> 15:44.120]  in-person presentations, they were always very good because they rehearsed,
[15:44.120 --> 15:44.360]  right?
[15:44.360 --> 15:49.080]  And they had professional coaching from absolute experts to help them.
[15:49.080 --> 15:54.200]  So the coaching and the practice and rehearsals, it was always very good, but
[15:54.200 --> 15:55.960]  it's just not their game, right?
[15:55.960 --> 16:00.920]  And so being able to do it in the comfort of without an audience and being
[16:00.920 --> 16:05.400]  able to do multiple takes and not having to worry about this is the one live one
[16:05.400 --> 16:08.920]  where I better not screw it up or my demo better not go wrong.
[16:08.920 --> 16:10.440]  All that pressure is gone.
[16:10.440 --> 16:15.240]  And the other huge advantage of it, in my opinion, and you must have a
[16:15.240 --> 16:19.160]  stronger opinion of it because you consume more of the sessions as a working
[16:19.160 --> 16:19.640]  developer.
[16:20.360 --> 16:23.480]  It lets all sessions be the length they need to be.
[16:23.480 --> 16:26.520]  Some of the sessions are 15 minutes because it's 15 minutes of material,
[16:26.520 --> 16:29.960]  whereas the old way, if it was 15 minutes of material, you had to pad it out
[16:29.960 --> 16:32.360]  to, I guess, a full half hour or an hour.
[16:32.360 --> 16:35.720]  I forget if they were all hour long slots or what, but I think they were at
[16:35.720 --> 16:36.680]  least a half hour.
[16:36.680 --> 16:37.400]  Right, right.
[16:37.400 --> 16:42.760]  I found that extremely tedious, especially as somebody who just doesn't
[16:42.760 --> 16:47.080]  particularly enjoy classroom environments anyway in the best of cases.
[16:47.080 --> 16:51.560]  So yeah, when you got the sense that, oh boy, this is just being dragged out to
[16:51.560 --> 16:53.320]  like, it's just the worst.
[16:54.280 --> 16:58.520]  The other thing is all those advantages you said, and another one that occurs to
[16:58.520 --> 17:04.680]  me is when you said they might be working on them right now, not only that,
[17:04.680 --> 17:08.360]  they might be finishing them right now, which is something you would never do
[17:09.160 --> 17:09.800]  in the past.
[17:10.760 --> 17:15.080]  You wouldn't have somebody preparing and practicing and getting to the point of
[17:15.080 --> 17:20.440]  being able to present it now because they might forget it by the time the
[17:20.440 --> 17:21.720]  actual conference comes around.
[17:21.720 --> 17:22.360]  You know what I mean?
[17:22.360 --> 17:26.920]  So it was kind of like everybody had to rehearse towards a specific deadline
[17:26.920 --> 17:28.360]  and now they can stagger it.
[17:28.360 --> 17:34.120]  I bet that's really nice for, I remember now 20 years ago when I last worked at
[17:34.120 --> 17:40.840]  Apple, I remember it was like a season of WWDC prep where I never gave a talk at
[17:40.840 --> 17:46.040]  WWDC, but coworkers of mine who, who were doing them, it's like the season would
[17:46.040 --> 17:49.320]  come around and suddenly they were just like off off limits.
[17:49.320 --> 17:50.920]  Like they're, they're busy, they're doing stuff.
[17:50.920 --> 17:53.960]  They're talking to the people who help them present.
[17:53.960 --> 17:57.800]  So that must be nice too, because it's not like a, I imagine it's not so much
[17:57.800 --> 18:02.360]  like a no sleep till WWDC type of a culture anymore.
[18:02.360 --> 18:04.440]  Yeah, I guess I don't think so.
[18:04.440 --> 18:05.960]  So I think it's all for the better.
[18:06.520 --> 18:11.240]  I will, I can't let an episode of the show go by without complaining about
[18:11.240 --> 18:11.880]  something though.
[18:11.880 --> 18:16.120]  And I will say, I'll go back to my, my only complaint about this new format.
[18:16.120 --> 18:19.640]  And I really do think this is it going, this isn't just year two.
[18:19.640 --> 18:23.960]  It's not just, Oh, the tail end of COVID and COVID concerns.
[18:23.960 --> 18:27.960]  And they still want to do it this way to have it outdoors.
[18:27.960 --> 18:30.040]  I mean, even though that is an advantage, right?
[18:30.040 --> 18:37.880]  Other than the unlikely, but not impossible chance that Monday, the fifth
[18:37.880 --> 18:41.880]  is a rainy day, which I really, and I remember asking last year, what was
[18:41.880 --> 18:46.200]  the plan B if it rained and plan B if it rained was, well, everybody's going
[18:46.200 --> 18:47.320]  to be wet.
[18:47.320 --> 18:54.120]  I mean, there is, there is no, there, there was no way to put a tarp over 1000
[18:54.120 --> 18:56.280]  or 1500 or however many people there were.
[18:56.280 --> 19:01.880]  The big problem with the location is hotels because Cooper, Cupertino has
[19:01.880 --> 19:03.080]  very few hotels.
[19:03.080 --> 19:06.680]  The one I stayed in, which I picked because of his proximity.
[19:06.680 --> 19:10.040]  Cause I knew I was going to be there for a couple of days cause the media
[19:10.040 --> 19:12.200]  stuff tends to span two days.
[19:12.200 --> 19:16.600]  I was doing my live show at Apple.
[19:16.600 --> 19:21.000]  I think we did it on Wednesday last year or, or I forget if it was Wednesday
[19:21.000 --> 19:24.600]  or Tuesday, um, might've been Tuesday, but whatever.
[19:24.600 --> 19:26.440]  I knew I'd be there at least through that.
[19:27.240 --> 19:32.760]  And I knew I'd be going back and forth from my hotel to Apple park multiple
[19:32.760 --> 19:33.080]  times.
[19:33.080 --> 19:34.280]  So why don't I stay?
[19:34.280 --> 19:39.720]  Oh, here's, I think it was, uh, I don't want to throw a hotel chain under the
[19:39.720 --> 19:45.560]  bus, but some, some national hotel chain, it was literally right across the
[19:45.560 --> 19:46.680]  street. Right.
[19:46.680 --> 19:50.440]  And, but you're, when you're looking at Apple maps or Google maps or any of
[19:50.440 --> 19:53.160]  these maps, you, you see the ring, the rings there on the map.
[19:53.160 --> 19:58.520]  You could, and from a global perspective from Philadelphia, a continental
[19:58.520 --> 20:04.440]  perspective, boy, does that hotel look close to Apple park, but the truth is
[20:04.440 --> 20:08.840]  it's all the way around Apple park from the end to the visitor center.
[20:09.400 --> 20:13.320]  So it, I mean, it's like a mile and a half probably.
[20:13.320 --> 20:15.000]  Yeah, I think it is honest to God.
[20:15.000 --> 20:16.680]  And so I am a walker.
[20:16.680 --> 20:21.240]  I mean, and I'm, as I record with you today and from a Slack group, I'm in my,
[20:21.240 --> 20:26.840]  my car, my car hasn't been able to start for weeks because we went weeks without
[20:26.840 --> 20:30.520]  starting it in the winter and I still haven't needed to drive anywhere.
[20:30.520 --> 20:35.240]  So I'm waiting to get the battery jump started until I actually know I need the
[20:35.240 --> 20:35.720]  car.
[20:35.720 --> 20:39.960]  So that, I mean, this is how we have, we own one car, haven't driven it in two
[20:39.960 --> 20:42.440]  months and literally doesn't even start at the moment.
[20:43.000 --> 20:44.200]  I like to walk.
[20:44.200 --> 20:45.160]  I really do.
[20:45.160 --> 20:49.560]  It's the most exercise I get is, is walking throughout my daily life, but
[20:50.280 --> 20:54.280]  walking a mile and a half to get to the keynote and then anything I might want
[20:54.280 --> 20:55.160]  to go back to my hotel.
[20:55.160 --> 21:00.440]  I ended up taking Ubers and lifts all the time and it was not a great hotel.
[21:00.440 --> 21:04.600]  And so once you're getting an Uber or a lift, anyway, it's like five minutes
[21:04.600 --> 21:05.160]  more.
[21:05.160 --> 21:09.720]  If you just expand the realm five minutes, there's way more hotel options, but
[21:09.720 --> 21:10.920]  none of them are great.
[21:10.920 --> 21:11.720]  You know what I mean?
[21:11.720 --> 21:16.280]  It's like some, some people I think was Marco there last year.
[21:16.280 --> 21:17.480]  I think he might've been, I know.
[21:17.480 --> 21:17.720]  Yeah.
[21:17.720 --> 21:18.360]  Marco was there.
[21:18.360 --> 21:24.360]  I think he had one of those Mark famous Marco, Tim Cook close encounters.
[21:24.360 --> 21:24.840]  Yes.
[21:24.840 --> 21:25.880]  Yes, exactly.
[21:25.880 --> 21:26.840]  Right, right, right.
[21:26.840 --> 21:30.600]  With the blue, with the, with the the the not blue, what do they call it?
[21:31.880 --> 21:33.320]  What's the name of the MacBook color?
[21:33.320 --> 21:35.400]  Midnight, midnight, right.
[21:35.400 --> 21:39.720]  Marco had been waiting and waiting and waiting and waiting for hands on time
[21:39.720 --> 21:43.320]  with the one midnight MacBook that was on display.
[21:43.320 --> 21:47.160]  And by the time it was his turn, somebody tapped him on the shoulder and said,
[21:47.160 --> 21:50.040]  Tim, Tim's going to take a photo here, so you need to back away.
[21:50.040 --> 21:52.120]  And it happens to him every year.
[21:52.120 --> 21:52.360]  Right.
[21:53.080 --> 21:53.320]  Right.
[21:53.320 --> 21:58.600]  And now, now I remember, because I had press briefings while the state of the
[21:58.600 --> 22:03.720]  union was going on and Marco and our, again, mutual friend of the show, David
[22:03.720 --> 22:05.000]  underscore Smith.
[22:05.000 --> 22:09.000]  I ran into them at the cafe at the visitor center and they were kind enough
[22:09.000 --> 22:14.600]  to give me the Marco and underscore summary of the state of the union, which
[22:14.600 --> 22:20.600]  unsurprisingly was GPT quality as a summary, as we'll get to later in the
[22:20.600 --> 22:21.320]  show.
[22:21.320 --> 22:23.400]  Very high quality summary of the show.
[22:23.400 --> 22:24.440]  Yeah, they were there.
[22:24.440 --> 22:27.800]  I think they stayed at, or at least Marco, maybe some, I know I had some
[22:27.800 --> 22:33.480]  friends who stayed all the way back in San Jose at the, whatever the name, the
[22:33.480 --> 22:34.760]  new name of the Fairmont.
[22:34.760 --> 22:35.480]  Oh yeah.
[22:35.480 --> 22:37.080]  It's like a Hilton or something, I think.
[22:37.080 --> 22:38.280]  But yeah.
[22:38.280 --> 22:42.360]  Well, they renamed it, redid the lobby completely cause I was there.
[22:43.720 --> 22:47.400]  I think I stayed there then for the iPhone event later in the year.
[22:47.400 --> 22:49.720]  Cause I knew not to stay in Cupertino.
[22:49.720 --> 22:50.440]  I don't know.
[22:50.440 --> 22:53.880]  It's like, that's the one thing we used to bitch and moan so much when it was in
[22:53.880 --> 22:58.920]  San Francisco at Moscone about the hotel prices, but that it's like, we didn't
[22:58.920 --> 23:02.120]  know how good we had it, where we could bitch and moan about the price of
[23:02.120 --> 23:05.880]  hotels, which again is laughable compared to the hotel prices now, although
[23:05.880 --> 23:09.720]  maybe with San Francisco being dead, as everybody says, maybe San Francisco
[23:09.720 --> 23:13.400]  hotels are cheaper, but I, I have no reason to go to San Francisco anymore.
[23:13.400 --> 23:17.160]  But anyway, what we didn't, I didn't realize how lucky we had.
[23:17.160 --> 23:21.000]  It was every single hotel was what a block and a half away from Moscone.
[23:21.720 --> 23:21.960]  Yeah.
[23:21.960 --> 23:27.400]  It was a really perfect, you had every kind of hotel to from pretty high, like
[23:27.400 --> 23:29.480]  not just pretty high end, really high end.
[23:29.480 --> 23:30.120]  Right.
[23:30.120 --> 23:35.800]  I would never consider going to down to like pretty high end down to run of the
[23:35.800 --> 23:40.200]  mill, right down to like total rat trap.
[23:40.200 --> 23:40.520]  Right.
[23:40.520 --> 23:40.760]  Right.
[23:40.760 --> 23:46.440]  You know, like you could stay in like a turn of the century, 1905, like
[23:46.440 --> 23:47.880]  survived the great fire.
[23:48.760 --> 23:50.440]  Hotel with a shared bathroom.
[23:51.080 --> 23:54.040]  And then if you couldn't afford that, you could go stay at the hostel.
[23:54.040 --> 23:55.720]  It's like they had everything.
[23:55.720 --> 23:57.720]  Yeah, everything, but it's so close.
[23:57.720 --> 24:00.840]  And then, so you just didn't have to worry like, Hey, do I have everything I
[24:00.840 --> 24:05.640]  need in my bag or my pockets or I would just leave my laptop.
[24:05.640 --> 24:10.200]  I just leave with a notebook and know that if I needed my laptop, I was a block
[24:10.200 --> 24:14.200]  away or two blocks away now when it's miles and miles in an Uber ride away.
[24:14.200 --> 24:17.400]  And it's, again, it's fun to go.
[24:17.400 --> 24:21.400]  And I'm so glad to have been back after COVID last year, but it's like to go
[24:21.400 --> 24:21.800]  back.
[24:21.800 --> 24:25.720]  It's like you have to go wait in a special line for the ride shares.
[24:25.720 --> 24:27.000]  And it makes sense, right?
[24:27.000 --> 24:31.640]  They can't just have Ubers and lifts pulling up higgledy-piggledy on the
[24:31.640 --> 24:33.480]  sidewalk all around Apple Park.
[24:33.480 --> 24:37.640]  So there's like a dedicated go over there to building 11 or something like
[24:37.640 --> 24:37.960]  that.
[24:37.960 --> 24:43.800]  And in the parking lot, there's a queue for the ride shares, but it's not great.
[24:43.800 --> 24:48.040]  And it doesn't make sense to rent a car because the parking situation, they
[24:48.040 --> 24:50.280]  really don't have parking for everybody.
[24:50.280 --> 24:54.200]  They did not build the Apple Park Visitor Center with the idea that they'd be,
[24:54.200 --> 24:57.080]  again, that's the one side effect of not designing it with this.
[24:57.080 --> 25:00.680]  They don't have parking for 1100 people or however many people show up.
[25:01.400 --> 25:02.440]  I really liked it.
[25:02.440 --> 25:07.720]  One thing I really liked about the one year I went to San Jose in the modern
[25:07.720 --> 25:11.720]  era, a San Jose WWDC, because I had skipped it maybe sort of out of like
[25:12.600 --> 25:15.800]  snobby protest for a couple of years, because I was like, this isn't San
[25:15.800 --> 25:16.680]  Francisco.
[25:16.680 --> 25:18.440]  But then I was like, Oh, everyone's there.
[25:18.440 --> 25:19.240]  I better go.
[25:19.240 --> 25:21.240]  And I really did enjoy.
[25:21.240 --> 25:24.680]  So I stayed at a hotel that I remember it was me and Craig Hockenberry were
[25:24.680 --> 25:29.560]  both at the hotel, like about a mile away from the scene.
[25:29.560 --> 25:32.360]  But it was that time.
[25:32.360 --> 25:34.040]  I don't know if they still have them.
[25:34.040 --> 25:36.920]  I know they sort of had their time and they've lost popularity, but it's that
[25:36.920 --> 25:40.360]  time when the little rental scooters were everywhere.
[25:40.360 --> 25:42.520]  Like, Oh, yeah, yeah, yeah.
[25:42.520 --> 25:42.680]  No.
[25:42.680 --> 25:43.320]  What were they called?
[25:43.320 --> 25:43.800]  Not lift.
[25:43.800 --> 25:44.360]  What were they?
[25:44.360 --> 25:47.000]  These were lime, lime, lime.
[25:47.000 --> 25:48.600]  Yeah, they might still be there.
[25:48.600 --> 25:49.080]  I don't know.
[25:49.080 --> 25:50.040]  But that was fun.
[25:50.040 --> 25:50.440]  Yeah.
[25:50.440 --> 25:54.920]  Well, so for instance, if you had lime scooters outside your hotel around the
[25:54.920 --> 25:58.280]  corner from Apple Park, it would have made everything a lot easier.
[25:58.280 --> 25:59.640]  Probably you would just be like, whatever.
[25:59.640 --> 26:01.000]  I'll zip over there and sit back.
[26:02.600 --> 26:03.960]  But yeah, it's a problem.
[26:03.960 --> 26:05.080]  It's not that whole area.
[26:05.080 --> 26:11.160]  I mean, I know that area so well, because I worked at Apple at Infinite Loop for
[26:11.960 --> 26:12.920]  seven years.
[26:12.920 --> 26:17.720]  I know that area better than most areas.
[26:17.720 --> 26:22.280]  And it's really, it's a really, really boring area that has some pretty good
[26:22.280 --> 26:22.760]  restaurants.
[26:22.760 --> 26:24.440]  That's how I summarize it.
[26:24.440 --> 26:26.520]  And so that's at least something.
[26:26.520 --> 26:28.440]  It actually has like, pretty good.
[26:28.440 --> 26:34.280]  I mean, you have to travel a little bit, but it has in its way weirdly, that area
[26:34.280 --> 26:39.640]  probably has better restaurant service than downtown San Jose, which is kind of
[26:40.520 --> 26:40.760]  weird.
[26:40.760 --> 26:43.160]  Well, I would say everything has better.
[26:43.160 --> 26:43.480]  I know.
[26:44.120 --> 26:44.840]  This is a lot.
[26:44.840 --> 26:47.320]  I'm just throwing you a log ball here.
[26:47.320 --> 26:47.800]  Right, right.
[26:49.000 --> 26:53.720]  But it is, I guess what you're saying though, is San Jose as a city of its size
[26:53.720 --> 26:57.080]  has an unusually sparse downtown restaurant scene.
[26:57.080 --> 27:03.400]  And the sort of greater Cupertino driving area, like five minutes from Apple Park, has
[27:03.400 --> 27:04.600]  surprisingly good restaurants.
[27:05.560 --> 27:06.840]  And I agree with that.
[27:06.840 --> 27:11.640]  And I just always, because I have friends who work at Apple, just defer to them and
[27:11.640 --> 27:13.160]  they'll be like, oh, we should go here.
[27:13.160 --> 27:14.520]  And I'm like, you tell me, I'm not.
[27:14.520 --> 27:18.600]  And then every place that, you know, whether it's like a stupid mom and pop sandwich
[27:18.600 --> 27:23.160]  place for lunch or a pretty nice sit down place for at night, it is.
[27:23.160 --> 27:25.080]  It's there's tons of good places to eat.
[27:25.080 --> 27:27.720]  I will say this coinciding before we segue.
[27:27.720 --> 27:34.200]  I feel like it's this sort of, it relates to my aforementioned talk about the season
[27:34.200 --> 27:34.760]  change, right?
[27:35.800 --> 27:42.280]  When you have seasons, to me, it internalizes the annual schedule.
[27:42.280 --> 27:50.280]  And I do think, I think it's safe to say that you don't have to be a scientist to
[27:50.280 --> 27:58.920]  state with authority that humans evolved to sort of grok the annual calendar, right?
[27:58.920 --> 28:02.440]  And it's like, oh, it's the holiday season, it's Thanksgiving.
[28:02.440 --> 28:06.200]  And then after Thanksgiving, all of a sudden it's Christmas and New Year's and it's
[28:06.200 --> 28:10.600]  dark and it feels cold, but you don't even know it's not cold yet because it's not
[28:10.600 --> 28:11.960]  even really winter.
[28:11.960 --> 28:14.040]  And then springtime rolls around, et cetera.
[28:14.040 --> 28:19.560]  And I've always thought Apple in particular really does work on an annual schedule.
[28:19.560 --> 28:24.680]  And it's only gotten more regularized under Tim Cook.
[28:24.680 --> 28:32.280]  And I think, again, in the way that an effective leader instills their personality into the
[28:32.280 --> 28:38.680]  organization, that the Tim Cook-ification of the last decade of Apple is that they're
[28:39.400 --> 28:41.560]  even more reliable, right?
[28:41.560 --> 28:47.560]  That it's even more certain that WWDC is going to happen exactly when you think it
[28:47.560 --> 28:57.240]  is. And it seems to me more and more of their products, hardware products, are getting more
[28:57.240 --> 28:58.840]  predictable in their updates.
[28:58.840 --> 29:05.880]  I mean, it would be a whole tangent for the show to talk about why the 24-inch iMac hasn't
[29:05.880 --> 29:07.880]  been updated with the M2.
[29:07.880 --> 29:12.920]  But it seems to me that in the Apple Silicon era, though, Macs are going to be getting
[29:12.920 --> 29:18.360]  updated on a more regular basis, like iPads at least, even though it doesn't seem like
[29:18.360 --> 29:24.920]  anything other than the phone and the watch are on it precise every single September.
[29:24.920 --> 29:26.600]  That's when the new ones come out.
[29:26.600 --> 29:35.240]  But WWDC is, I would say, clearly the two tentpole annual things in Apple's schedule.
[29:35.240 --> 29:43.080]  WWDC in June, the iPhone slash watch slash if there's something else special to announce,
[29:43.080 --> 29:46.200]  do it with the iPhone September event.
[29:46.200 --> 29:52.680]  And the thing about WWDC, and I would say in the Tim Cook era, especially the last five,
[29:52.680 --> 30:01.160]  six years, has really become a sort of, it's not just here's a bunch of new developer
[30:01.160 --> 30:06.840]  APIs, and here's in the morning, we'll show you some new features that are coming out
[30:06.840 --> 30:07.880]  in the fall.
[30:07.880 --> 30:11.160]  It's sort of, here's what we're doing for the next 12 months.
[30:11.880 --> 30:12.600]  Right, right.
[30:12.600 --> 30:17.720]  We say Apple is very secretive, and they are, and they won't, hopefully won't leak what
[30:17.720 --> 30:19.960]  is going to be announced at WWDC.
[30:19.960 --> 30:26.360]  But effectively, WWDC has become a way for them to leak stuff that won't be coming until
[30:26.360 --> 30:29.160]  next March or April, right?
[30:29.160 --> 30:34.680]  Because look at this week with the, this is my segue, my very long meandering segue.
[30:35.240 --> 30:41.720]  This is the week where Apple updated, I think, literally every operating system they have,
[30:41.720 --> 30:43.720]  right, with a sort of feature update.
[30:43.720 --> 30:50.440]  macOS, iOS, iPadOS, watchOS, tvOS, the HomePod OS, something else.
[30:50.440 --> 30:53.400]  Oh, the studio display even got a firmware update, right?
[30:53.400 --> 30:56.920]  It's like all at once, it's Michael Corleone taking pictures today.
[30:56.920 --> 31:02.520]  Also, you might even count this as another platform getting an OS update, the pretty
[31:02.520 --> 31:05.240]  substantial Safari update.
[31:05.240 --> 31:06.520]  Yeah, I would say so.
[31:06.520 --> 31:10.280]  I would say that WebKit counts as an Apple platform.
[31:10.280 --> 31:16.680]  They don't call it WebKit OS, but web browsers, the web rendering engine, effectively, it
[31:16.680 --> 31:20.040]  certainly is the complexity and size of an operating system, right?
[31:20.040 --> 31:26.520]  I don't know that it's completely fair, but I would guess WebKit overall is a larger
[31:26.520 --> 31:28.600]  software project than watchOS.
[31:28.600 --> 31:29.800]  Yes, I would think so.
[31:31.080 --> 31:31.800]  But who knows?
[31:31.800 --> 31:34.520]  But yeah, at least it's fathomable to me.
[31:34.520 --> 31:39.880]  I mean, I mean, they, of course, both of them, of course, lean on all of the infrastructure
[31:39.880 --> 31:42.440]  of other OS projects, but.
[31:42.440 --> 31:43.480]  Right, right.
[31:43.480 --> 31:44.760]  It's, it's that significant.
[31:44.760 --> 31:50.520]  And it is, it's been a major update for WebKit and they including a couple of, again, this
[31:50.520 --> 31:54.200]  could be a whole segment of the show that we probably don't have time for, but some
[31:54.200 --> 31:59.240]  mobile WebKit features that web developers have been clamoring for, for a long time,
[31:59.240 --> 32:04.840]  like the notifications and which is a big deal for like the, what do they call them,
[32:04.840 --> 32:06.680]  PWAs, Progressive Web Apps.
[32:06.680 --> 32:12.760]  But it basically, when you save a web app to be an app on your home screen and making
[32:12.760 --> 32:20.120]  them more feasible to act like you expect an app to be able to act by giving you notifications
[32:20.120 --> 32:21.000]  and stuff.
[32:21.000 --> 32:21.480]  Right.
[32:21.480 --> 32:25.720]  And I don't, I don't know that much about that world, but I, I have had the feeling,
[32:25.720 --> 32:31.560]  the impression for years and years that to summarize most web developers have been grumpy
[32:31.560 --> 32:37.160]  about the speed with which Apple has embraced those kinds of features.
[32:37.160 --> 32:37.960]  So I don't know.
[32:39.240 --> 32:43.640]  Well, I think there's two, there's two sort of slants to the criticism.
[32:43.640 --> 32:50.280]  I think, and again, maybe I'm biased as a Safari user and as a native app proponent
[32:50.280 --> 32:57.880]  and as somebody who does not want to see, and, and I guess most, my bias is most covered,
[32:57.880 --> 33:06.520]  colored by my, and deep antipathy for the web appification of desktop apps, right?
[33:06.520 --> 33:12.600]  That just to, I know that not all written with Electron, but let's just use Electron
[33:12.600 --> 33:17.240]  as a catch-all umbrella, the electronification of desktop software development.
[33:17.240 --> 33:19.560]  I don't want to see that happen to mobile.
[33:19.560 --> 33:23.640]  To, or at least want to see it happen as slowly as possible.
[33:23.640 --> 33:31.800]  And so there, in some sense, I'm rooting against Chrome and Chromium taking over the
[33:31.800 --> 33:35.800]  mobile market the way they have the desktop market.
[33:35.800 --> 33:40.200]  And so there's one contingent of web developers who just want that to happen, right?
[33:40.200 --> 33:45.160]  That they, because they're all in the way they develop software is to just use all of
[33:45.160 --> 33:52.200]  the bleeding edge Chrome features and they really see right once run everywhere as nothing
[33:52.200 --> 33:57.880]  but a positive or that the positives greatly outweigh the negatives in the overall scheme
[33:57.880 --> 34:01.000]  of things want that to happen.
[34:01.000 --> 34:02.200]  And I just disagree.
[34:02.200 --> 34:03.400]  I'm not saying they're wrong.
[34:03.400 --> 34:05.160]  I'm just saying I disagree with them, right?
[34:05.160 --> 34:06.120]  There's a difference, right?
[34:06.120 --> 34:11.560]  That I see their point, but I don't think that's good for the platform experience.
[34:11.560 --> 34:16.200]  But on the other hand, I think there's more pragmatic, less zealotrous web developers
[34:16.200 --> 34:23.000]  who just are frustrated by the pace that Apple has adopted some of these technologies that
[34:23.000 --> 34:24.520]  to me aren't about.
[34:24.520 --> 34:31.160]  The conspiracy theory such that it is, is that Apple, as a company, holds back WebKit
[34:31.160 --> 34:39.240]  on mobile simply to ensure that the app store remains the only feasible way to do X, Y,
[34:39.240 --> 34:40.440]  and Z.
[34:40.440 --> 34:46.680]  Yeah, that seems false on the face of it because they could do such a worse job than they do.
[34:46.680 --> 34:47.560]  Right, right.
[34:47.560 --> 34:55.560]  If they really wanted to hamper mobile advancement of web apps, then we would not be seeing even
[34:55.560 --> 34:59.800]  a quarter of the enhancements to WebKit over the years.
[35:00.440 --> 35:00.680]  Right.
[35:00.680 --> 35:02.920]  And it's sort of an insult to the WebKit team.
[35:02.920 --> 35:07.000]  And I can't say I'm like tight buds with any of them, but I do know several of the people
[35:07.000 --> 35:11.000]  on the WebKit team and a lot of them have been there for a very long time.
[35:11.000 --> 35:17.960]  And they're very smart and very proud and very, very proud of both their work and WebKit's
[35:17.960 --> 35:18.920]  position in the world.
[35:19.480 --> 35:26.280]  And it's frankly insulting to argue that they just go along with, hey boss, we would like
[35:26.280 --> 35:27.400]  to do X, Y, and Z.
[35:27.400 --> 35:30.040]  And they're like, nope, because we want to protect the app store.
[35:30.040 --> 35:31.160]  And that they just take it.
[35:31.160 --> 35:31.880]  And they're like, okay.
[35:33.320 --> 35:33.820]  Right.
[35:33.820 --> 35:38.380]  Yeah, they have different priorities than the people who are clamoring for X, Y, and
[35:38.380 --> 35:39.420]  Z features.
[35:39.420 --> 35:48.620]  And just so often the case when I think WebKit embodies that in the context of the web, they
[35:48.620 --> 35:56.780]  embody that Apple kind of idea of being willing to say no in order to focus on the things
[35:56.780 --> 35:58.460]  you think are more important.
[35:59.180 --> 36:03.260]  And I deeply respect the WebKit team as well.
[36:03.260 --> 36:09.100]  I think for the record, I'm also a Safari user, but I get it, there are some shortcomings,
[36:09.100 --> 36:14.220]  but I think they focus a lot on things that matter to me anyway.
[36:14.220 --> 36:17.100]  So I think I'm going to piss some people off.
[36:17.100 --> 36:22.780]  Well, I keep meaning to redo it and I will, but I'll hear the talk show is where I spoil
[36:23.740 --> 36:26.220]  stuff that might be forthcoming on Daring Fireball.
[36:26.220 --> 36:30.060]  And often what I spoil here never comes out anyway.
[36:30.060 --> 36:36.300]  But a couple of years ago, I think it was back around the time or before.
[36:36.300 --> 36:42.780]  Remember when Apple had the round table where they said, okay, this trash can style Mac
[36:42.780 --> 36:47.340]  Pro is a mistake, but we're not giving up on the pro market, but we don't have anything
[36:47.340 --> 36:53.020]  to announce right now, except for the fact that we have been working on a pro oriented
[36:53.020 --> 36:58.060]  iMac, which they didn't say would be called the iMac Pro, but which was the iMac Pro.
[36:58.060 --> 37:01.580]  That was a couple of years, it was like 2016, 2017, something like that.
[37:01.580 --> 37:06.380]  So it's been a while since I did this, but what I did is I think it was pretty clever
[37:06.940 --> 37:09.660]  is I wrote like an Apple script to go through.
[37:09.660 --> 37:12.860]  I picked like 25 popular websites.
[37:12.860 --> 37:15.340]  I forget how I picked them, but they felt like a fair thing.
[37:15.980 --> 37:21.660]  And I wrote an Apple script that would open them up in tabs and do some use, like the
[37:21.660 --> 37:27.980]  GUI scripting to do fake scrolling, like to sort of simulate a user actually reading the
[37:27.980 --> 37:36.300]  web page, open it up, wait 10 seconds, scroll down, wait 10 seconds, scroll down, open a
[37:36.300 --> 37:44.380]  new tab, do this with a new website, do the same thing, keep 10, 15, 20 tabs open in the
[37:44.380 --> 37:50.860]  background, then close them all, do it again and just keep going until the battery's out
[37:50.860 --> 37:53.980]  and see how long the battery lasts in Safari.
[37:53.980 --> 37:58.060]  And then ran the same Apple script against Chrome, against the same websites.
[37:58.860 --> 38:04.620]  And I thought there's other, I'm proud of myself because it seems like something, you
[38:04.620 --> 38:08.300]  know, like Mac world labs used to do back in the day and have these test suites.
[38:08.300 --> 38:12.140]  But I thought it was a pretty good simulation of like, hey, with nothing but a web browser
[38:12.140 --> 38:14.380]  running, what's the difference between Chrome and Safari?
[38:14.940 --> 38:17.740]  And I forget the answer, but it was significant, right?
[38:17.740 --> 38:22.300]  It was like hours on a MacBook at the time, it was hours of difference overall.
[38:22.300 --> 38:27.100]  So I keep meaning to redo it and see what here in 2023, what's the difference?
[38:27.100 --> 38:31.340]  I don't know if Chrome's closed the gate at all or closed the gap.
[38:31.340 --> 38:35.420]  I might've, you know, every once in a while I notice in the Chrome release notes that
[38:35.420 --> 38:40.860]  they say something about battery life improvements, but I don't think there's no doubt in my
[38:40.860 --> 38:44.940]  mind, and again, it's the order of your priorities.
[38:44.940 --> 38:49.500]  Of course, the Chrome team at Google wants battery life to be good, right?
[38:50.300 --> 38:54.220]  It's just not as high a priority for them as it is for WebKit, right?
[38:54.220 --> 38:59.740]  It's both teams value that, and I'm sure in the same way, WebKit values adding new
[38:59.740 --> 39:05.020]  features for web developers, but there it's a little bit of a lower priority for WebKit
[39:05.020 --> 39:11.660]  to stay on the bleeding edge of web developer APIs and more of a priority, if not the highest
[39:11.660 --> 39:19.100]  priority to do everything a web browser has to do the most power efficient way possible.
[39:19.100 --> 39:24.540]  Whereas at Google, maybe the APIs and staying on the bleeding edge and pushing the platform
[39:24.540 --> 39:30.220]  forward as a developer thing is the highest priority and being energy efficient is second
[39:30.220 --> 39:33.260]  or third, but I wouldn't be surprised if that's a difference.
[39:33.260 --> 39:40.220]  And I just know that that's a big deal at WebKit and for web developers who only care
[39:40.220 --> 39:45.580]  about, ah, we built this whole web app and if only Safari supported this API too, we
[39:45.580 --> 39:49.980]  could take out all of this extra code that we have to do just to run on the iPhone.
[39:51.100 --> 39:56.060]  Just completely doesn't bother them at all that there's something like power efficiency,
[39:56.060 --> 40:00.140]  which consumes massive amounts of the WebKit team's attention and time.
[40:00.940 --> 40:03.260]  And they're like, what do I care about that?
[40:03.820 --> 40:08.540]  Whereas for them at WebKit, they're like part of Apple and it's like part of the,
[40:08.540 --> 40:11.500]  Hey, how long does the battery on your iPhone last?
[40:11.500 --> 40:11.980]  Right.
[40:11.980 --> 40:15.100]  And if WebKit is less efficient, that's a lower number.
[40:15.100 --> 40:16.940]  And that makes Apple as a whole look bad.
[40:17.500 --> 40:18.000]  Yep.
[40:18.780 --> 40:23.420]  Any, any features from this week's OS updates that jump out at you?
[40:23.420 --> 40:27.340]  It's not really, I think I haven't really absorbed it yet.
[40:27.340 --> 40:33.580]  I felt like there was something I noticed even in the Mac OS update, but it's just not
[40:33.580 --> 40:34.940]  jumping to my mind right now.
[40:35.500 --> 40:35.980]  Yeah.
[40:35.980 --> 40:38.700]  It's tell me something, what's your favorite thing?
[40:38.700 --> 40:43.100]  And if you have it, and then I'll tell you if that reminds me that that's my favorite
[40:43.100 --> 40:43.980]  thing too.
[40:43.980 --> 40:44.940]  Well, I don't know.
[40:44.940 --> 40:46.060]  It's my favorite.
[40:46.060 --> 40:49.740]  It's actually because I'm sort of ignorant of the whole field, but it seems like the
[40:49.740 --> 40:54.300]  biggest one is that Apple music classical is tied to this, right?
[40:54.300 --> 40:59.820]  So if you want to use this new long rumored, or I guess they announced it, I guess it
[40:59.820 --> 41:01.660]  wasn't, it wasn't like a popped out of nowhere.
[41:01.660 --> 41:06.860]  They somehow announced it months ago that there would be a new entirely set standalone
[41:06.860 --> 41:11.340]  app just for classical music in Apple music, which is fascinating, right?
[41:12.540 --> 41:20.540]  That it's not just a genre, but is considered for aficionados fans of classical music, so
[41:20.540 --> 41:26.300]  different that it deserves its own standalone app, but you need to be on iOS 16.4 or Mac
[41:26.300 --> 41:28.460]  OS 13.3 to get it.
[41:28.460 --> 41:32.540]  I don't really have any strong opinions about it because I'm not a classical music
[41:32.540 --> 41:33.020]  person.
[41:33.020 --> 41:36.860]  And even though I do enjoy it, who doesn't like classical music, even if you're not a
[41:36.860 --> 41:37.180]  fan?
[41:37.180 --> 41:39.900]  I mean, it's kind of, yeah.
[41:39.900 --> 41:45.100]  Well, probably a lot of people would say they don't enjoy it, but then you get some familiar
[41:45.100 --> 41:48.140]  recognizable riff in front of them, and they're like, Oh, yeah, I like that song.
[41:48.140 --> 41:50.540]  But I also am not an aficionado.
[41:51.340 --> 41:57.660]  But what I like about this is it intrigues me to think that it might help me become more
[41:57.660 --> 41:58.780]  of an aficionado.
[41:58.780 --> 42:06.380]  Like it's sort of, I think it's addressing, and I don't understand all of the reasons
[42:06.380 --> 42:09.100]  for it being a separate app, but I kind of got the gist of it.
[42:09.100 --> 42:19.580]  Like in a sense, classical music is, I think I'm hoping that the app sort of addresses
[42:19.580 --> 42:26.140]  a problem that I have when I am inspired to explore classical music.
[42:26.140 --> 42:29.900]  It's like, well, which performer do you listen to?
[42:29.900 --> 42:35.980]  Like what, if I want to go listen to some Chopin, which I do happen to like, even if
[42:35.980 --> 42:42.620]  I'm not an aficionado, how do I find out whether the first performance that pops up, because
[42:43.180 --> 42:48.620]  here's, in summary, all of the classical music that we listen to are covers, right?
[42:49.340 --> 42:54.940]  So it's like, if you had a situation where it was like, okay, let's listen to the Beatles,
[42:54.940 --> 43:00.940]  but the only caveat is they're all covers, then how do you find out which version of
[43:01.500 --> 43:02.780]  help to listen to?
[43:02.780 --> 43:06.780]  And I could use some guidance in that respect.
[43:06.780 --> 43:12.860]  I think maybe that sort of gets at the uniqueness of the problem that Apple's trying to address
[43:12.860 --> 43:21.180]  here is that it's not the same as other genres because other genres' artists are, performing
[43:21.180 --> 43:26.300]  artists are the highest level of categorization.
[43:26.300 --> 43:32.460]  And in the context of classical music, the performing artist isn't the highest level
[43:32.460 --> 43:33.420]  of classification.
[43:33.420 --> 43:36.380]  It's the author, the writer, the composer.
[43:36.380 --> 43:36.700]  Right.
[43:36.700 --> 43:41.020]  Beethoven or where a whole category broke or something.
[43:41.020 --> 43:44.300]  I'm sort of speaking off the top of my head here, but yeah.
[43:44.300 --> 43:48.140]  Our friend, Jessie Char is a trained musician.
[43:48.140 --> 43:50.700]  I believe she plays the cello?
[43:50.700 --> 43:51.820]  Cello sounds right.
[43:51.820 --> 43:52.940]  Cello sounds right to me.
[43:52.940 --> 43:53.340]  Yes.
[43:53.340 --> 43:54.380]  She's a cellist.
[43:54.380 --> 43:55.420]  Is that how you pronounce it?
[43:55.420 --> 43:55.980]  Cellist.
[43:55.980 --> 43:56.460]  Cellist.
[43:56.460 --> 44:01.820]  But she had a good thread on Twitter weeks ago, like when Apple said, Hey, you can pre-order
[44:01.820 --> 44:05.900]  this app, which you didn't have to pay for it, but I guess that way you'd automatically
[44:05.900 --> 44:07.980]  download it from the app store when it was available.
[44:08.620 --> 44:10.140]  She had a good thread on Twitter.
[44:10.140 --> 44:13.100]  I'll make a note here and try to put it in the show notes.
[44:13.100 --> 44:20.140]  Just sort of explaining the way that the simple artist album song categorization doesn't really
[44:20.140 --> 44:22.620]  work for classical music fans and therefore needs a new app.
[44:23.180 --> 44:24.620]  So there's a major feature.
[44:25.820 --> 44:26.780]  I'm looking forward to that.
[44:26.780 --> 44:32.700]  I feel like this is kind of a release that has a bunch of little things, so I'm hoping
[44:32.700 --> 44:33.420]  for the best.
[44:33.420 --> 44:33.820]  Yeah.
[44:33.820 --> 44:39.740]  It sort of seems like the schedule is, the annual schedule is they announce all this
[44:39.740 --> 44:45.180]  stuff at WWDC and they give us developer betas and that some of this stuff is in there, right?
[44:45.180 --> 44:47.740]  Cause there's some stuff that's been percolating.
[44:47.740 --> 44:51.340]  All these things have been percolating on teams greater than a year, right?
[44:51.340 --> 44:57.980]  Like when a major new feature shows up in any of these OSs, it's not because they started
[44:57.980 --> 45:01.980]  it a couple of months before it's like months or 24 months or longer.
[45:01.980 --> 45:05.500]  Some teams have been toiling away, didn't make the cut last year.
[45:05.500 --> 45:08.380]  Like, ah, that's not ready this year, but the team keeps working.
[45:08.380 --> 45:12.780]  And then here's the stuff that's ready for this year's OS updates.
[45:12.780 --> 45:15.100]  Some of this stuff actually is ready.
[45:15.100 --> 45:19.500]  Those developer betas are hit and miss in June in terms of what's ready and what's not.
[45:20.140 --> 45:25.100]  September is when the iPhone comes out in iOS and then the Mac OS.
[45:25.100 --> 45:29.180]  Last year they delayed iPad OS 2 until like October.
[45:29.180 --> 45:33.500]  But in the annual sense, September, October, what's the hell is the difference, right?
[45:33.500 --> 45:39.580]  It's like everybody's impatient for an iPad OS at the end of September because iOS came
[45:39.580 --> 45:40.540]  out, but they come out.
[45:41.420 --> 45:43.500]  They're a little buggy at least.
[45:43.500 --> 45:44.860]  Maybe I'm being euphemistic.
[45:44.860 --> 45:46.940]  I don't know, some years they're more buggy than others.
[45:46.940 --> 45:52.940]  iOS 13, for whatever reason, really sticks out at me as sort of a really rough launch.
[45:52.940 --> 45:58.060]  That was the one I think they had like the emergency, like almost a hot fix.
[45:58.060 --> 46:03.340]  13.1 was ready by the same day the iPhones actually came out.
[46:03.340 --> 46:04.940]  It was really pretty bad.
[46:05.500 --> 46:08.540]  Yeah, this past year seemed pretty smooth, if I recall correctly.
[46:09.580 --> 46:14.540]  The people out there, and I'm in a weird spot because again, I am getting older.
[46:14.540 --> 46:19.020]  We're all getting older, but I'm at the age where I'm more crotchety than I used to be.
[46:19.980 --> 46:26.380]  And my inclinations are to be conservative with, especially my Mac, right?
[46:26.380 --> 46:28.060]  Because that's where I do my most work.
[46:28.060 --> 46:34.380]  And so I don't jump on the new version of Mac OS on my main work Mac until I feel like
[46:34.380 --> 46:35.420]  it's stable.
[46:36.060 --> 46:40.220]  But I'm in the racket of writing and talking about these things, so I need to be using
[46:40.220 --> 46:44.860]  them and using them on spare machines isn't the best way to get to know them.
[46:44.860 --> 46:50.860]  But the people out there who don't have a need to write or talk about this and who wait
[46:50.860 --> 46:55.180]  a couple of point releases before they update, probably wise, right?
[46:55.180 --> 46:56.940]  It's overall, what are you missing?
[46:56.940 --> 47:03.020]  And I feel like these.4, it's like.1,.2 are always like bug fixes.
[47:03.020 --> 47:07.580]  Like, ah, here's this thing that we released in September that was, yeah, that was buggy.
[47:07.580 --> 47:08.780]  Bug fixes.
[47:08.780 --> 47:14.300]  The.3 releases right around like Christmas are like, hey, some of this stuff, here's
[47:14.300 --> 47:16.780]  some new stuff that was promised to WWDC.
[47:17.500 --> 47:25.420]  And then like this March 16.4 Mac OS.3 release, this is sort of like, they've almost got
[47:25.420 --> 47:27.420]  everything they announced last year at WWDC.
[47:27.420 --> 47:28.940]  I forget, there's like one or two things.
[47:28.940 --> 47:35.260]  There's like some silly Apple Pay thing, not Apple Pay later, which actually did start
[47:35.260 --> 47:38.540]  this week, but something else.
[47:38.540 --> 47:42.380]  Some kind of, I forget what it is, but pretty much now they're caught up.
[47:42.380 --> 47:45.500]  And I know there's a 16.5 beta that's already out.
[47:45.500 --> 47:51.100]  But now's the time of the year where you can upgrade with confidence if you've still
[47:51.100 --> 47:51.820]  been holding on.
[47:54.460 --> 48:00.540]  I'm amused by the fact that Apple has so effectively used emoji updates as a lure.
[48:00.540 --> 48:03.580]  Oh, yeah, without question.
[48:03.580 --> 48:04.860]  Oh, without question.
[48:04.860 --> 48:07.820]  And they've, I don't think they've done it this year.
[48:07.820 --> 48:13.980]  I should check my email, but in years past, I've gotten press from Apple just about the
[48:13.980 --> 48:14.540]  emoji.
[48:14.540 --> 48:20.620]  And they do, I think you can, I do think, and again, nobody's officially told me this.
[48:20.620 --> 48:25.420]  Craig Federighi has never broken character behind the scenes and said, yes, off the record,
[48:25.420 --> 48:29.020]  we hold the emojis until we have a really stable release.
[48:29.580 --> 48:31.580]  But they do two things.
[48:31.580 --> 48:37.500]  There's a couple of levers that they can pull to sort of encourage slash discourage
[48:37.500 --> 48:38.940]  people from updating.
[48:38.940 --> 48:46.140]  The big one is whether they hit the button to sort of push the update to notify people,
[48:46.140 --> 48:46.460]  right?
[48:46.460 --> 48:48.940]  Like to put a red badge on the settings app.
[48:49.900 --> 48:54.940]  So even if you're just a normal person with an iPhone, and you don't even follow the Apple
[48:54.940 --> 48:59.340]  news, you don't even really, you don't even remember or notice that they have an annual
[48:59.340 --> 49:01.420]  schedule for OS updates, right?
[49:01.420 --> 49:02.780]  Or phone updates, right?
[49:02.780 --> 49:07.820]  You just buy a phone, maybe in August, right, which most people would agree would probably
[49:07.820 --> 49:12.060]  be the worst month of the year to buy an iPhone because the new ones are probably coming in
[49:12.060 --> 49:12.780]  September.
[49:12.780 --> 49:17.260]  But if you break your phone or lose your phone or just get sick of your old phone in August,
[49:17.260 --> 49:22.300]  lots of people go in and buy iPhones in August, and you're just a normal person.
[49:22.300 --> 49:26.540]  And is there a red badge on your settings app that's saying, hey, there's a new version
[49:26.540 --> 49:27.980]  of iOS to install?
[49:27.980 --> 49:35.020]  They don't do that usually with those like 16.0s in September, like you can go to settings
[49:35.020 --> 49:39.900]  general, check for off updates, and it'll say, Oh, yeah, here's an update, do you want
[49:39.900 --> 49:41.100]  to install it now?
[49:41.100 --> 49:45.740]  But they don't push it until like 16.1 or something like that.
[49:45.740 --> 49:49.660]  Because they kind of, I don't think they want to deal with the customer support issue
[49:49.660 --> 49:53.900]  of a billion devices around the world all on the point.
[49:53.900 --> 49:57.340]  All on the point. Oh, so that's one lever they can pull.
[49:57.340 --> 50:02.940]  And it gives you a sort of external sense of which versions of the OS they feel like,
[50:02.940 --> 50:07.180]  yeah, let's get we're better off, we'll have fewer customer support problems.
[50:07.740 --> 50:11.900]  If we push everybody to this, because this is a better update with more features.
[50:12.620 --> 50:16.540]  And then the flip side of it is the emojis, right?
[50:17.100 --> 50:21.500]  And it's like, all of a sudden, somebody's sending you the the goose emoji that's brand
[50:21.500 --> 50:26.460]  new. But it shows up as a what I what do they show show up as like a square or something?
[50:26.460 --> 50:29.420]  I forget what the square with the question mark in it.
[50:29.420 --> 50:29.920]  Yes.
[50:30.300 --> 50:31.980]  Or an X in it or something like that.
[50:31.980 --> 50:35.180]  But it might be different on the Mac and iOS too.
[50:35.180 --> 50:36.780]  Right. But then you're like, what was that?
[50:36.780 --> 50:38.700]  And they're like, Oh, it's the goose emoji.
[50:38.700 --> 50:40.140]  And it's like, Oh, how do I get that?
[50:40.140 --> 50:41.660]  Well, you got to update your iPhone.
[50:41.660 --> 50:43.580]  And they definitely they definitely use it.
[50:43.580 --> 50:50.540]  And it definitely motivates people, I should take this moment to I'm more than half serious
[50:50.540 --> 50:55.900]  about this, that I'm going to keep hammering on the fact that I want a chef's kiss emoji.
[50:56.700 --> 50:58.940]  I keep mentioning it on Daring Fireball.
[50:58.940 --> 51:00.620]  Yeah, I'm going to keep mentioning it.
[51:01.260 --> 51:04.300]  Do you have any any pet emoji that are missing?
[51:04.300 --> 51:05.260]  That you wish were there?
[51:06.460 --> 51:07.660]  Not off the top of my head.
[51:07.660 --> 51:12.300]  It's just one of those things where every once in a while, I think like you, I go to
[51:13.660 --> 51:16.700]  give the emoji that I assume must be there.
[51:16.700 --> 51:17.260]  Be there.
[51:17.260 --> 51:17.580]  Right.
[51:17.580 --> 51:21.820]  Yeah. And then it's not and it's like, what the hell? Like, how is there like a sushi
[51:21.820 --> 51:24.540]  emoji from like a specific type of eel or something?
[51:24.540 --> 51:25.660]  I don't know that.
[51:25.660 --> 51:26.780]  I don't know if that's true.
[51:26.780 --> 51:30.700]  But sometimes there's some really specific to my mind stuff.
[51:30.700 --> 51:35.660]  And then you're like, but there's no like popcorn or I don't know if you know,
[51:35.660 --> 51:36.860]  Popcorn is definitely there.
[51:36.860 --> 51:41.340]  Because I know because every everybody always does it when like when another indictment
[51:41.340 --> 51:42.780]  is hinted at with Donald Trump.
[51:42.780 --> 51:43.020]  Right.
[51:43.900 --> 51:45.340]  Get your get your popcorn ready.
[51:45.340 --> 51:46.140]  Here we go.
[51:46.140 --> 51:51.340]  So there are there are some times I think when things that to my mind are as obvious
[51:51.340 --> 51:53.980]  to be there as popcorn are not there.
[51:53.980 --> 52:00.220]  And I think I would agree with you that chef's kiss that it fills a void.
[52:00.220 --> 52:01.500]  It would fill a void.
[52:01.500 --> 52:05.420]  And I can't think of other ones that off the top of my head I've I've gone for.
[52:05.420 --> 52:12.540]  But I think often it's like some kind of particular vegetable or fruit that seems to
[52:12.540 --> 52:16.380]  fit a specific scenario.
[52:16.380 --> 52:17.740]  There's no pickle.
[52:17.740 --> 52:18.540]  I have.
[52:18.540 --> 52:19.500]  Here's my list.
[52:19.500 --> 52:21.180]  I just brought it up in Apple Notes.
[52:21.180 --> 52:22.940]  So at the top of my list, chef's kiss.
[52:23.660 --> 52:25.500]  This is the one I want to make it.
[52:25.500 --> 52:26.620]  It's got to be there.
[52:26.620 --> 52:27.820]  It's a worldwide thing.
[52:27.820 --> 52:28.780]  Everybody knows this.
[52:28.780 --> 52:29.740]  It's a chef's kiss.
[52:29.740 --> 52:30.620]  It means it.
[52:30.620 --> 52:32.460]  The meaning is so perfect, right?
[52:32.460 --> 52:38.060]  It to me epitomizes the communicative beauty of emoji.
[52:38.060 --> 52:45.420]  I think the whole emoji thing must be one of the funnest, best gifts to the world of
[52:45.420 --> 52:53.260]  linguistics in decades, because people are communicating expressively through pictograms
[52:53.260 --> 52:56.620]  in a way that we haven't for centuries, right?
[52:58.140 --> 53:00.540]  Millennia may be in Western civilization, right?
[53:00.540 --> 53:03.980]  It takes you back to hieroglyphics and stuff.
[53:03.980 --> 53:06.380]  But humans, we're natural at it, right?
[53:06.380 --> 53:13.100]  Everybody I know, my dad, my mom, old people, certainly younger people, everybody can communicate
[53:13.100 --> 53:13.980]  with emoji.
[53:13.980 --> 53:15.340]  It's wonderful.
[53:15.340 --> 53:16.140]  And it really is.
[53:16.140 --> 53:22.780]  In some ways, I don't know if any emoji really says a thousand words, but it certainly says
[53:22.780 --> 53:26.780]  a chef's kiss emoji can save you just a lot of typing on your thumbs.
[53:26.780 --> 53:27.420]  I need it.
[53:27.420 --> 53:28.060]  Here's my list.
[53:28.060 --> 53:28.860]  It's not long.
[53:28.860 --> 53:31.260]  The chef's kiss, number one.
[53:31.260 --> 53:31.660]  Pickle.
[53:32.220 --> 53:33.260]  I don't know why.
[53:33.260 --> 53:34.700]  My son's a big fan of pickles.
[53:34.700 --> 53:37.820]  And sometimes like, so like, what do you want me to get from the store?
[53:38.540 --> 53:39.820]  You can't send a pickle.
[53:39.820 --> 53:41.020]  Cucumber is the closest.
[53:41.980 --> 53:42.540]  Shovel.
[53:42.540 --> 53:44.620]  Doesn't that seem like there should be a shovel emoji?
[53:45.260 --> 53:48.460]  That's one that I was like, I cannot believe that it's not there.
[53:48.460 --> 53:49.820]  I forget why I wanted it.
[53:49.820 --> 53:53.340]  Trombone, because I feel like you could use that to express.
[53:54.460 --> 53:55.500]  Exactly.
[53:55.500 --> 53:56.220]  Exactly.
[53:57.260 --> 53:59.420]  And then the last one on my list, sombrero.
[53:59.420 --> 54:03.820]  I don't know why I wanted a sombrero, but it feels like they've got a lot of hats in
[54:03.820 --> 54:04.060]  there.
[54:04.060 --> 54:05.580]  Why not a sombrero?
[54:05.580 --> 54:10.540]  Because I just typed in sombrero into this is I'm looking at Apple's character viewer
[54:11.180 --> 54:11.820]  to their credit.
[54:11.820 --> 54:20.620]  When I type in sombrero, I get like cowboy face, woman detective, the English guards,
[54:21.500 --> 54:23.260]  all these different things with hats.
[54:23.260 --> 54:26.140]  There's a military helmet top hat.
[54:26.140 --> 54:26.860]  Yep.
[54:26.860 --> 54:28.060]  And no sombrero.
[54:28.060 --> 54:28.540]  Yeah.
[54:28.540 --> 54:29.100]  I don't know.
[54:29.100 --> 54:32.460]  There's, I mean, this, I guess that's the problem with emoji is
[54:32.460 --> 54:35.100]  there are a heck of a lot of things, a lot of things in this world.
[54:35.100 --> 54:37.020]  So I've, I've reordered my list.
[54:37.020 --> 54:41.580]  I put trombone at second underneath chef's after we get, after we get chef's kiss in
[54:41.580 --> 54:44.140]  next year, then my new campaign will be for the trombone.
[54:44.140 --> 54:47.020]  Cause we need that wah wah wah wah.
[54:47.020 --> 54:48.380]  So here's the thing.
[54:48.380 --> 54:53.980]  I don't understand the process, but I feel like I've witnessed people making successful
[54:53.980 --> 54:56.620]  campaigns to get emoji added.
[54:56.620 --> 54:57.180]  Yep.
[54:57.180 --> 55:05.500]  So I think if you get like a designer buddy on board and go, if only, if only I knew someone.
[55:05.500 --> 55:07.500]  Right, exactly.
[55:07.500 --> 55:14.540]  Only you knew somebody with a gift for the designer arts, but I think, I feel like I've
[55:14.540 --> 55:20.460]  seen people back when, back when Twitter was a thing, I feel like people were celebrating
[55:20.460 --> 55:24.620]  getting a new thing added to the emoji verse.
[55:24.620 --> 55:25.100]  Yeah.
[55:25.100 --> 55:25.580]  Yeah.
[55:25.580 --> 55:28.140]  All right.
[55:28.140 --> 55:29.660]  Well anyway, let's see, let's see.
[55:29.660 --> 55:31.820]  I'll have you on in a year and let's see if I can get it done.
[55:31.820 --> 55:34.300]  It's a, it's a, hold, hold, hold on.
[55:34.300 --> 55:36.140]  It has to be a bet or a chat.
[55:36.140 --> 55:37.020]  Oh, all right.
[55:37.820 --> 55:43.740]  So if you get it done within a year, then I owe you $5.
[55:43.740 --> 55:44.140]  All right.
[55:44.140 --> 55:44.620]  All right.
[55:44.620 --> 55:48.220]  $5 bet or how about a chef's hat?
[55:49.740 --> 55:50.700]  Chef's hat.
[55:50.700 --> 55:51.740]  Hey, chef's hat's in there.
[55:51.740 --> 55:52.140]  Isn't it?
[55:52.140 --> 55:54.140]  That's no, there is, there is.
[55:54.140 --> 55:54.540]  Yeah.
[55:54.540 --> 55:57.260]  That's so you can not a chef's hat, but a chef.
[55:57.260 --> 55:57.740]  Yeah.
[55:57.740 --> 56:01.020]  So you can make a chef kiss with the chef plus the lips.
[56:01.020 --> 56:03.580]  There's like a kiss, like a lipstick lips.
[56:03.580 --> 56:08.140]  So that, that passes as chef's kiss, but it's, it's not right though.
[56:08.140 --> 56:11.980]  Cause you don't, the chef's kiss has to be with the, with the, with the okay.
[56:11.980 --> 56:12.940]  Signed by the lips.
[56:12.940 --> 56:14.620]  Like, ah, it's a little different.
[56:15.180 --> 56:16.220]  Anyway, let me take a break here.
[56:16.220 --> 56:17.820]  Thank thank our first sponsor of the show.
[56:17.820 --> 56:19.500]  It's our good friends at collide.
[56:19.500 --> 56:22.140]  Now they spell it K O L I D E.
[56:22.780 --> 56:24.060]  They have some big news.
[56:24.060 --> 56:26.220]  If you are an Okta user.
[56:27.180 --> 56:28.460]  Now that's okay.
[56:28.460 --> 56:29.100]  TA.
[56:29.100 --> 56:33.660]  But if you, if you're in the market for this, you know what Okta is already.
[56:33.660 --> 56:37.660]  Well, if you are, they can get your entire fleet to a hundred percent compliance.
[56:37.660 --> 56:45.660]  How well, if any device in your organization is not compliant, then with collide, the user
[56:45.660 --> 56:51.260]  can't log in to your cloud apps until they've fixed the problem to get back into compliance.
[56:51.260 --> 56:52.060]  It's that simple.
[56:52.060 --> 56:53.660]  If they're compliant, they're in.
[56:53.660 --> 56:56.380]  And if they're not compliant, they can't get in until they fix it.
[56:56.380 --> 56:58.380]  And this way collide patches.
[56:58.380 --> 57:04.220]  One of the major holes in zero trust architecture and that's device compliance without collide.
[57:04.220 --> 57:09.020]  It struggles to solve basic problems like keeping everyone's OS and browser up to date.
[57:09.980 --> 57:14.700]  Insecure devices are logging into your company's apps because there's nothing there to stop
[57:14.700 --> 57:15.180]  them.
[57:15.180 --> 57:20.780]  Collide is the only device trust solution that enforces compliance as part of authentication.
[57:20.780 --> 57:23.900]  And it's built to work seamlessly with Okta.
[57:23.900 --> 57:29.100]  The moment collides agent detects a problem, it alerts the user and gives them instructions
[57:29.660 --> 57:30.300]  how to fix it.
[57:31.020 --> 57:33.820]  If they don't fix the problem within a set time, they're blocked.
[57:33.820 --> 57:40.780]  That's it collides method ensures fewer support tickets, less frustration, and most importantly,
[57:40.780 --> 57:42.460]  a hundred percent fleet compliance.
[57:42.460 --> 57:47.820]  Visit collide.com slash the talk show to learn more or book a demo.
[57:47.820 --> 57:53.740]  That's K O L I D E dot com slash the talk show.
[57:54.380 --> 57:55.180]  Thank you, Clyde.
[57:55.180 --> 57:57.580]  All right, let's let's get to the meat of the subject here.
[57:57.580 --> 58:00.860]  I wanted to talk about this AI stuff.
[58:00.860 --> 58:05.660]  I have you been messing around with this, the child, these various chat GPDs at this
[58:05.660 --> 58:10.540]  point, stuff is moving so fast where the pace of my podcast isn't enough to keep up with
[58:10.540 --> 58:11.580]  it, right?
[58:11.580 --> 58:16.460]  Yeah, I have been messing around with it as little as possible.
[58:16.460 --> 58:20.940]  It's kind of like when the, when the, the image generation things came out, we were
[58:20.940 --> 58:25.900]  all like, Oh, geez, if I don't stop myself, I'm going to spend like a week straight,
[58:25.900 --> 58:27.900]  just interfacing with this thing.
[58:27.900 --> 58:30.060]  But I dip into it every once in a while.
[58:30.060 --> 58:33.260]  I spend probably about an hour with it.
[58:33.260 --> 58:39.340]  And then I, I forcefully pull myself away from it, but I've, I've, I'm fascinated
[58:39.340 --> 58:43.180]  and scared and confused and bewildered by it.
[58:43.180 --> 58:46.540]  I assume you're probably some combination of those same things.
[58:46.540 --> 58:47.020]  Yeah.
[58:47.020 --> 58:51.820]  And it's weird because I got into and I had the same feeling months ago.
[58:51.820 --> 58:52.620]  I think it was months.
[58:52.620 --> 58:56.780]  It's all moving so fast with the image generation where it wasn't that it didn't
[58:56.780 --> 58:57.660]  interest me.
[58:57.660 --> 59:04.060]  It was honestly my own self-awareness of I'm like, I, I have X, Y, and Z.
[59:04.060 --> 59:08.220]  I want to do here's some articles I've been had been working on.
[59:08.220 --> 59:10.220]  I want to get, I want to write about XYZ.
[59:10.220 --> 59:13.340]  I want to get a new episode of the podcast out.
[59:13.340 --> 59:14.860]  I want to blah, blah, blah.
[59:14.860 --> 59:18.620]  If I get into this, I'm going to not shower for two days.
[59:18.620 --> 59:21.260]  And my wife is going to be like, John, where are you?
[59:21.260 --> 59:23.420]  And I'll be like, I'm just here working at our there.
[59:23.420 --> 59:24.300]  She'll be like, dinner's ready.
[59:24.300 --> 59:26.060]  I'll be like, I'm up in five minutes.
[59:26.060 --> 59:30.700]  And five hours later I come upstairs and lo and behold, once I got into them, that
[59:30.700 --> 59:31.820]  is what it was like.
[59:31.820 --> 59:39.100]  I did get sucked in mid journey in particular, which is a combination of both seemingly
[59:39.100 --> 59:48.300]  produces more unbelievably artistic output as opposed to sort of impressionistic output,
[59:48.300 --> 59:51.580]  even though the impressionistic stuff is fascinating to me too.
[59:51.580 --> 59:56.460]  And sometimes untold books have been written about realism versus impressionism and art.
[59:56.460 --> 01:00:00.860]  And, but it's just more amazing, but like with mid journey in particular, because the,
[01:00:00.860 --> 01:00:01.900]  have you ever messed around with it?
[01:00:01.900 --> 01:00:04.220]  That's the one that you go through discord.
[01:00:04.220 --> 01:00:05.180]  Yep.
[01:00:05.180 --> 01:00:05.820]  I have.
[01:00:05.820 --> 01:00:09.420]  And it's, so it's not productive.
[01:00:09.420 --> 01:00:14.940]  It's anti-productive because what you do, and I guess if you pay, you, you can get your
[01:00:14.940 --> 01:00:17.340]  own private chat on the server.
[01:00:17.340 --> 01:00:21.260]  But, and again, I talked about this before, but for anybody out here who hasn't done this,
[01:00:21.260 --> 01:00:28.460]  when you mid journey is this GPT powered, I think I forget what the backend, but anyway,
[01:00:28.460 --> 01:00:35.020]  AI image generation, where you just type natural language descriptions, like, you know,
[01:00:35.020 --> 01:00:40.060]  Daniel Jowket playing volleyball on a beach photographic style.
[01:00:40.060 --> 01:00:45.260]  And it'll, if it knows who Daniel Jowket is, will render a photograph of Daniel Jowket.
[01:00:45.260 --> 01:00:51.260]  Famously, like last week, people were generating images of Donald Trump being resisting arrest
[01:00:51.260 --> 01:00:53.820]  on the streets of New York while they drag them in.
[01:00:53.820 --> 01:00:59.180]  But you could say photographic style, or you could say in the style of a painting by Norman
[01:00:59.180 --> 01:01:05.420]  Rockwell, or in the story of a comic book by Jack Kirby or something like that.
[01:01:05.420 --> 01:01:07.820]  And son of a bitch, that's how they come up.
[01:01:07.820 --> 01:01:13.260]  And it's not like a hint of it like, oh, that does look a little bit like a photograph,
[01:01:13.260 --> 01:01:16.060]  or that does look a little bit like a Norman Rockwell.
[01:01:16.060 --> 01:01:20.780]  It's like, no, it looks like a Norman Rockwell painting, or it looks photorealistic or blah,
[01:01:20.780 --> 01:01:21.580]  blah, blah.
[01:01:21.580 --> 01:01:25.740]  But with mid journey, the interface is through a discord server.
[01:01:25.740 --> 01:01:31.820]  And when you're like a free customer, you just go into these free channels, type these
[01:01:31.820 --> 01:01:35.660]  commands, and then you wait like 30 seconds to get these things.
[01:01:35.660 --> 01:01:41.340]  But meanwhile, there's like dozens of other people, random strangers from around the world
[01:01:41.340 --> 01:01:46.620]  in the same channel as you on discord typing their commands, and you get to see their results.
[01:01:46.620 --> 01:01:53.820]  And it's, it's straight, it goes by pretty fast, fast enough to be almost hectic.
[01:01:53.820 --> 01:01:57.500]  Almost hectic, but not quite frantic.
[01:01:57.500 --> 01:02:02.300]  But while you're waiting for the thing you're trying to make these other amazing things
[01:02:02.300 --> 01:02:03.900]  are just flashing by.
[01:02:03.900 --> 01:02:06.540]  And it's like, captivating.
[01:02:06.540 --> 01:02:12.380]  And you can watch these other people zoom in, narrow in on what they were going for,
[01:02:12.380 --> 01:02:14.060]  you know, like, here's four choices.
[01:02:14.060 --> 01:02:18.700]  And then, oh, do four, give me four more based on the second one here.
[01:02:18.700 --> 01:02:23.500]  And it's just engrossing, really is incredible.
[01:02:23.500 --> 01:02:27.900]  But on the other hand, I don't need lots of images, right?
[01:02:27.900 --> 01:02:31.100]  I don't illustrate my posts on Daring Fireball with images.
[01:02:31.100 --> 01:02:37.100]  So I'm not using these things to generate a hero image to go atop my articles.
[01:02:37.100 --> 01:02:42.780]  So in some ways, the chat stuff is more up my alley, right?
[01:02:42.780 --> 01:02:45.340]  More of a verbally oriented person.
[01:02:45.340 --> 01:02:52.220]  It also is another really good way to lose a lot of time once you start digging in, but
[01:02:52.220 --> 01:02:59.260]  it's not that I've said this before, like to me, when I'm on Discord playing around
[01:02:59.260 --> 01:03:06.460]  with mid journey, it's the way I felt as a 10 year old in a busy coin op arcade, right,
[01:03:06.460 --> 01:03:11.500]  like with all the newest games, including games I've never even heard of before, like,
[01:03:11.500 --> 01:03:13.180]  oh, my God, what's that?
[01:03:13.180 --> 01:03:18.220]  And it's like, Bing, and there are no songs, but there might as well be bings and bops
[01:03:18.220 --> 01:03:22.140]  and pings and pals and ding ding ding.
[01:03:22.140 --> 01:03:23.100]  And what's this?
[01:03:23.100 --> 01:03:23.740]  What's that?
[01:03:23.740 --> 01:03:25.980]  And let me wait for this now, right?
[01:03:25.980 --> 01:03:30.300]  And it's that same sort of like, hurry up and wait feeling of being in an arcade.
[01:03:30.300 --> 01:03:33.420]  Like, I want to play this new game, but here's a kid who's playing ahead of me.
[01:03:33.980 --> 01:03:35.260]  And he's really good at it.
[01:03:35.260 --> 01:03:37.260]  So I got to wait for his game to be over.
[01:03:37.260 --> 01:03:41.260]  Whereas the chat stuff is a little bit more sedate.
[01:03:41.260 --> 01:03:44.940]  It's just like chatting with a person, except it's a friggin robot.
[01:03:44.940 --> 01:03:45.980]  And it's amazing.
[01:03:45.980 --> 01:03:55.180]  And yeah, there's something about the infinite possibilities with an undefined interface
[01:03:55.180 --> 01:03:59.500]  that makes it really compelling to keep trying new things.
[01:03:59.500 --> 01:04:04.220]  You know, like if you're playing around with a new app, let's say you might go through
[01:04:04.220 --> 01:04:08.460]  if you really if you're really intrigued by the app, you might go through every menu item
[01:04:08.460 --> 01:04:10.540]  and see what does this what does this do?
[01:04:10.540 --> 01:04:11.340]  What does this do?
[01:04:11.340 --> 01:04:17.100]  And then when you're like an old timer Mac user, you go, aha, I know there might be some
[01:04:17.100 --> 01:04:20.940]  like options, you hold down the option key and go through every menu.
[01:04:20.940 --> 01:04:26.780]  But you never think like, what happens if I type into Photoshop, paint me, Donald Trump,
[01:04:26.780 --> 01:04:28.700]  on a cowboy, right?
[01:04:28.700 --> 01:04:30.220]  Cowboy bat, Pat, or whatever.
[01:04:30.220 --> 01:04:30.720]  Right.
[01:04:32.300 --> 01:04:39.340]  So the fact that it's endless, and I think I agree with you, the text stuff has been
[01:04:39.340 --> 01:04:44.780]  a little more compelling, not just because I think I'm also a little more text oriented,
[01:04:44.780 --> 01:04:57.100]  but because the format is, it's more humane, it's a, because the photos thing is really
[01:04:57.100 --> 01:05:00.700]  super interesting, not photos, but the graphic graphical stuff is really interesting.
[01:05:00.700 --> 01:05:05.260]  But it's generating stuff in certain styles, whereas the the chat things are literally
[01:05:05.260 --> 01:05:08.140]  trying to come across as human personalities.
[01:05:08.140 --> 01:05:09.500]  Right.
[01:05:09.500 --> 01:05:11.900]  Which is really interesting.
[01:05:11.900 --> 01:05:15.980]  I've had a few like kind of like areas I tend to go down.
[01:05:15.980 --> 01:05:23.020]  I was I was chatting with Manton Reese, my co host on core intuition about my tendency.
[01:05:23.020 --> 01:05:27.740]  One of one of the things I try to do with a lot of these things is whoever runs the
[01:05:27.740 --> 01:05:33.020]  the AI, try to get the AI to badmouth the the owners, you know?
[01:05:33.020 --> 01:05:33.580]  Yeah, yeah.
[01:05:33.580 --> 01:05:36.220]  So it's like, for some reason, I just find that really satisfying.
[01:05:36.220 --> 01:05:40.460]  So yeah, like trying to get Bing to say terrible things about Microsoft, right?
[01:05:41.100 --> 01:05:43.020]  It's just it's a wonderful pastime.
[01:05:43.020 --> 01:05:46.380]  And there's no real graphical equivalent to that.
[01:05:46.380 --> 01:05:56.300]  Like I couldn't get stable diffusion or Dolly or mid journey to make an image that so perfectly
[01:05:56.300 --> 01:06:04.460]  captured like a sarcastic snipe of you, you could do some pretty good stuff.
[01:06:04.460 --> 01:06:08.860]  But yeah, you could you could ask it to render an image of Calvin from Calvin and Hobbes
[01:06:08.860 --> 01:06:14.380]  P peeing on right, the box copy of Microsoft Windows or something.
[01:06:14.380 --> 01:06:19.420]  But it's not the same thing as getting the chat thing to badmouth it and say,
[01:06:19.420 --> 01:06:19.920]  Right.
[01:06:20.540 --> 01:06:21.740]  And it's not the same thing.
[01:06:21.740 --> 01:06:24.620]  And it's also not as scary as the chat bots.
[01:06:25.260 --> 01:06:30.220]  Like I'm thinking back to the conversation I heard between you and Ben Thompson on dithering
[01:06:30.220 --> 01:06:36.380]  about his experience with being like, threatening to report him and stuff like this.
[01:06:37.180 --> 01:06:42.780]  There's, I don't think anybody's ever had one of maybe I shouldn't say nobody ever has had,
[01:06:42.780 --> 01:06:48.860]  but I don't think it's common for the visual generators to come up with results that are
[01:06:48.860 --> 01:06:51.820]  easily perceived as threats against the user.
[01:06:51.820 --> 01:06:57.260]  Like they might come up with like a weird, violent looking thing or maybe a pornographic
[01:06:57.260 --> 01:07:01.260]  looking thing. Right. But it's not like an image that says like,
[01:07:03.100 --> 01:07:06.780]  they're not caught. They're not commonly like generating ransom notes that say I'm saying
[01:07:06.780 --> 01:07:12.060]  I'm going to come to your house and kill you. Right. But some of these chat interactions
[01:07:13.100 --> 01:07:19.100]  have got it again, I think because of the humanity of the fake humanity of them.
[01:07:19.980 --> 01:07:22.860]  They feel like there's a purpose to them.
[01:07:22.860 --> 01:07:29.260]  I just texted you an image that's from episode 359 last fall with Merlin was on the show.
[01:07:29.260 --> 01:07:33.340]  And I think this is from mid journey. I forget. I think it was mid journey where I was asking
[01:07:33.340 --> 01:07:38.380]  it to render Donald Trump sad alone in an office with a Canada Coke.
[01:07:39.180 --> 01:07:40.220]  And so good.
[01:07:40.220 --> 01:07:44.540]  And I used it was so good. I used it as the album cover for the episode. I'll put it in
[01:07:44.540 --> 01:07:49.340]  probably as a chapter art. So if you look at your overcast right now, as I'm talking,
[01:07:49.340 --> 01:07:54.220]  hopefully, you'll see the same thing. It's so good. But yeah, the chats. So to me,
[01:07:54.220 --> 01:07:59.980]  I guess the way it feels, and I know how it works. And it's all or I don't know how it works. But I
[01:07:59.980 --> 01:08:05.340]  have a pretty good idea as a somewhat of a programmer and a long ago comp sci graduate
[01:08:05.340 --> 01:08:11.580]  how these systems work. It's part of the magic is even the people who understand the best the
[01:08:11.580 --> 01:08:17.420]  people who make these large language models admit that nobody really knows how they work
[01:08:17.420 --> 01:08:22.940]  once they're trained, right? Like that is sort of it is it's a little scary, like even not not like
[01:08:22.940 --> 01:08:29.260]  alarmists, but just like Sam Altman, who's the, I guess I don't know what his title is at open AI,
[01:08:29.260 --> 01:08:35.900]  but I think he's in charge now at open AI has even just just revisited a 2016 profile of him
[01:08:35.900 --> 01:08:41.340]  that was large in the New Yorker that was at the time he was main job was he was the CEO at
[01:08:41.340 --> 01:08:49.260]  Y Combinator, the startup incubator, and open AI was new. And the profile was mostly about his
[01:08:49.260 --> 01:08:53.020]  work at Y Combinator. But then they touched on the open AI stuff. And he even said, like,
[01:08:53.020 --> 01:08:57.900]  nobody knows how these things work. Like they know how to train them. And they kind of know why they
[01:08:57.900 --> 01:09:03.980]  work. But once the model is working, that's the whole thing. It's like no, but the the actual
[01:09:03.980 --> 01:09:09.580]  code that runs on the computers behind the scenes comes out of this process. And it's not human
[01:09:09.580 --> 01:09:15.100]  written these algorithms. So it's kind of wild. Yeah. Speaking of the whole like, dangerous
[01:09:15.100 --> 01:09:23.580]  seeming aspect of these. Did you see the headline today about some kind of group letter? Yeah,
[01:09:23.580 --> 01:09:30.460]  I did. I can open letter asking that there'd be like a six month pause on development of these
[01:09:30.460 --> 01:09:37.020]  systems. Yeah, but I also saw that it's apparently the whole thing is a mess because some of the
[01:09:37.020 --> 01:09:43.180]  signatories didn't actually sign. Like, I guess they're saying Sam Altman in particular signed
[01:09:43.180 --> 01:09:50.140]  it, but apparently he didn't. One of the people who did sign is infamous Twitter engineer Raul Ligma.
[01:09:52.780 --> 01:09:56.940]  I forget is that like, is that like a fake? Is that like a fake engineer?
[01:09:56.940 --> 01:10:04.460]  Well, you know, Ligma, right? Oh, like, Raul Ligma was one of the cats
[01:10:04.460 --> 01:10:10.380]  who the day that Elon Musk first initiated layoffs, did interviews out, you know,
[01:10:10.380 --> 01:10:15.420]  was carrying a box of stuff from his office. Right. Oh, and it turned out to be actors.
[01:10:15.420 --> 01:10:20.460]  Yeah, just two guys who were pranksters. But then Elon Musk actually, I don't know,
[01:10:20.460 --> 01:10:25.740]  hired them or at least took selfies with them weeks later. But one of the fake names was
[01:10:25.740 --> 01:10:32.220]  Raul Ligma. And he's apparently a signatory on this open letter. I thought I do think that the
[01:10:32.220 --> 01:10:41.340]  industry, I do think caution is advisable, right? It's not quite as clear cut how things could go
[01:10:41.340 --> 01:10:50.940]  bad as, say, nuclear weapons and nuclear power, right? Which in the end, the danger is the same,
[01:10:50.940 --> 01:10:58.540]  right? A nuclear, well, I guess a weapon may not be fired as an accident, but some kind of nuclear
[01:10:58.540 --> 01:11:05.660]  catastrophe, right? Whether it's a weapon or a Chernobyl, or three mile island, right? Going
[01:11:05.660 --> 01:11:11.180]  wrong. The worst case scenario is obvious, right? We know what the worst case scenario is. It's a
[01:11:11.180 --> 01:11:18.540]  mass mass death because of radiation and and the half life and it'll take forever for the
[01:11:18.540 --> 01:11:24.300]  earth to recover, blah, blah, blah. We don't know the worst case scenario for AI. We have imagined
[01:11:24.300 --> 01:11:28.940]  scenarios, but we don't know. So there's a big difference there. But I do think, though, that
[01:11:28.940 --> 01:11:36.700]  scientifically and as a profession, the same sort of caution is called for, right? It's just because
[01:11:36.700 --> 01:11:41.020]  it's software. I do think there's a human instinct to think, ah, software, it's just something the
[01:11:41.020 --> 01:11:46.860]  nerds do and blah, blah, blah. No, it's the danger. There's real danger with AI. You don't
[01:11:46.860 --> 01:11:52.300]  have to imagine Skynet. There's other bad scenarios. But the Skynet scenario does seem
[01:11:52.300 --> 01:11:57.020]  more and more real, right? And the HAL 9000 scenario seems more and more real.
[01:11:57.020 --> 01:12:00.700]  I mean, I guess it comes down to whether anybody hooks one of these things up to
[01:12:01.820 --> 01:12:05.660]  some interface that actually gives it physical power.
[01:12:05.660 --> 01:12:09.580]  Right. But a six month moratorium doesn't sound I don't see the point of that. It
[01:12:09.580 --> 01:12:13.500]  doesn't make any sense to me. And if anything, it just sort of sounds like, hey,
[01:12:13.500 --> 01:12:19.820]  GPT-4 is ahead of everybody right now. So please give GPT-4 six months of undisputed
[01:12:19.820 --> 01:12:25.900]  preeminence in the field. It doesn't seem right to me. But anyway, the point I wanted to before
[01:12:25.900 --> 01:12:31.660]  I forget it, because that's the point I wanted to say about image generation versus chat is to me,
[01:12:31.660 --> 01:12:39.340]  image generation feels like pure magic. It is it, it doesn't feel like intelligence, per se,
[01:12:39.340 --> 01:12:47.900]  it just feels like magic. And I know it seems so simple. Low these 30 years later, but like,
[01:12:47.900 --> 01:12:53.580]  or I guess 40 years on cheese, but the first time you used Mac paint back in the day,
[01:12:53.580 --> 01:13:00.460]  and you use the paint, can tool flood film. It was like magic, right? Because it's like,
[01:13:01.340 --> 01:13:05.740]  the pencil tool like Mac paint was so much fun, even just black and white, you take the pencil
[01:13:05.740 --> 01:13:10.380]  and you start drawing pixels. And it's like, you can draw it's like, holy shit, I'm drawing on a
[01:13:10.380 --> 01:13:15.980]  computer live, right? Like it, that wasn't something you could do before. And, you know,
[01:13:15.980 --> 01:13:20.860]  it's like you but you understood what drawing with a pencil was like drawing with a pencil on
[01:13:20.860 --> 01:13:25.900]  your Macintosh screen was like drawing with a pencil on paper, you drag a line and you get
[01:13:25.900 --> 01:13:30.860]  a line and you drag a circle and you get like a circle type shape. I'm saying circle type shape,
[01:13:30.860 --> 01:13:36.700]  because drawing a circle with a mouse usually didn't come out all that circular. But flood fill,
[01:13:37.420 --> 01:13:43.180]  like I always hated I don't know about you. I hated coloring as a kid, I did not enjoy
[01:13:43.180 --> 01:13:48.460]  using coloring books. And I know it's a hobby now. And if you enjoy it, God bless you. I can see why
[01:13:48.460 --> 01:13:53.900]  people it adults enjoy it now. But it always bored me to tears, right? Because it's like,
[01:13:53.900 --> 01:14:00.140]  Oh, God, now it's like, I'm coloring in a friggin elephant. And it's like, Oh, just why couldn't it
[01:14:00.140 --> 01:14:04.620]  be a smaller animal like, and you just sit there and rub the crayon back and forth. And so my
[01:14:04.620 --> 01:14:09.660]  coloring was always a mess. My handwriting is a mess, too. But I was I remember I like in
[01:14:09.660 --> 01:14:17.020]  kinder, I always felt I, I've always been angry, but I, but we'd get graded on our coloring,
[01:14:17.020 --> 01:14:21.340]  right? It would be like a check check plus check minus type thing. And I'd always get like a check
[01:14:21.340 --> 01:14:26.220]  minus. And I'd be like, Why am I getting low grades? I'm one of the clearly smarter than these
[01:14:26.220 --> 01:14:33.180]  kids. And I'm getting bad grades because I can't color that the paint tool did what I I didn't know
[01:14:33.180 --> 01:14:37.100]  that I wanted it but I wanted it since kindergarten. I know what I wanted. I wanted
[01:14:37.100 --> 01:14:42.300]  this whole elephant filled in with gray, click. Now it's filled in. Right. And it was like,
[01:14:42.300 --> 01:14:48.540]  that paint tool in Mac paint, expand it to color with Photoshop as the years went on. And then
[01:14:48.540 --> 01:14:53.580]  there was the magic selection tool in Photoshop, right where you could sort of click a shape and
[01:14:53.580 --> 01:14:58.860]  you could set like a tolerance for color and it would just, boom, get the whole green background
[01:14:58.860 --> 01:15:05.580]  behind somebody, right? Those things every step of the way, always, every step of the way,
[01:15:05.580 --> 01:15:11.580]  always felt like magic in a way. And I know somewhere, you know, it was somebody, you know,
[01:15:11.580 --> 01:15:16.380]  Bill Atkinson writing the thing for Mac paint. And eventually it was these very smart engineers at
[01:15:16.380 --> 01:15:24.220]  Adobe making the magic selection tool. This just feels though, like with chat to generate the
[01:15:24.220 --> 01:15:30.620]  images like the culmination of that, right? It's just flood fill 40 years later, we're now instead
[01:15:30.620 --> 01:15:37.740]  of just telling the computer to fill in the elephant with gray, I can say, show me a photo
[01:15:37.740 --> 01:15:44.620]  realistic elephant storming into the lobby of Trump Tower on Fifth Avenue. And it just, it just
[01:15:44.620 --> 01:15:50.380]  happens. It doesn't feel like there's an intelligent being on the other end. It just feels like an
[01:15:50.380 --> 01:15:57.420]  incredibly advanced image generation tool, whereas the chat feels like there's goddamn intelligence
[01:15:57.420 --> 01:16:03.420]  on the other end. Right? I'm chatting. God damn it. I'm, I'm ab this thing is passing the Turing
[01:16:03.420 --> 01:16:12.700]  test. It Yeah, and I guess though, what I so it's funny, I, I fall on the like, spooked out,
[01:16:13.260 --> 01:16:20.300]  scared. Let's reign this thing in before it takes over the world. I tend towards that because I
[01:16:20.300 --> 01:16:26.540]  don't know, that's maybe just how I'm wired. And we have some friends who are more wired towards
[01:16:26.540 --> 01:16:31.740]  the like, what are you talking about? It's just a computer, like, we're in control. And I'm trying
[01:16:31.740 --> 01:16:39.500]  to take that in. And what helps a little bit is to remember historically, how, not just in my
[01:16:39.500 --> 01:16:48.060]  lifetime, but in decades and centuries before my lifetime, how, how repetitive this process of new
[01:16:48.060 --> 01:16:56.620]  technology appearing indistinguishable from magic, their board, therefore, feeling intrinsically
[01:16:56.620 --> 01:17:07.180]  dangerous is and, and I say that, at the same time as it at once, that sort of calms me a little bit,
[01:17:07.180 --> 01:17:13.900]  like, no, it turns out, photographs didn't like capture your soul or whatever, whatever people
[01:17:13.900 --> 01:17:18.940]  might have been worried about at the time. Well, there were very John Philip Sousa, I believe in
[01:17:18.940 --> 01:17:23.740]  particular was adamantly opposed to the phonograph, because he swore up and down, it would put
[01:17:23.740 --> 01:17:28.140]  musicians out of business because once everything was recorded, why would anybody ever learn to play
[01:17:28.140 --> 01:17:33.260]  a musical instrument again? Right? Well, see, there's a good example of just totally getting
[01:17:33.260 --> 01:17:39.740]  it wrong. But then I think, though, of other things, like, well, even something like so the
[01:17:39.740 --> 01:17:47.580]  car, the automobile was, I think, controversial, when it first started going into production, but
[01:17:47.580 --> 01:17:54.220]  maybe there was good cause for that in retrospect, right? Like, how many of our like, social problems
[01:17:54.220 --> 01:18:01.340]  in the world would be helped if somebody had put like a six month moratorium on not six months, but
[01:18:02.060 --> 01:18:08.620]  I don't know that just to say, like, sometimes I think there are things that society does and go
[01:18:08.620 --> 01:18:12.940]  ahead with technologically that if they had happened to be thought through a little
[01:18:12.940 --> 01:18:20.940]  differently, wouldn't have had as negative of an impact on society. Well, I think social networks
[01:18:20.940 --> 01:18:29.180]  are a good point. Yes. Well, I think that the algorithmic feeds for social networks, and again,
[01:18:29.180 --> 01:18:36.700]  we could do a whole show about it. But the it's had, I would say overall, I guess this is
[01:18:36.700 --> 01:18:41.980]  pessimistic. And certainly Ben Thompson, I think would argue with us about it. But overall, more of
[01:18:41.980 --> 01:18:48.220]  a negative effect than a positive effect. I honestly think that the last 10 years of Facebook
[01:18:48.220 --> 01:18:57.020]  and Twitter, in particular, that the net effect has been negative, not positive on society. I do.
[01:18:57.020 --> 01:19:03.500]  And and I think even somebody who would argue that the positive still outweighed the negatives,
[01:19:03.500 --> 01:19:07.660]  which is reasonable, and maybe I'm wrong, but they still have to admit that the negatives are
[01:19:07.660 --> 01:19:14.220]  significant. And I think I and I think more to your point, Daniel, weren't thought through. And
[01:19:14.220 --> 01:19:18.940]  I don't because I that's the thing. I don't think Martin, there's no, I, Mark Zuckerberg didn't want
[01:19:19.660 --> 01:19:27.260]  to facilitate Russians interfering with propaganda in our presidential elections, right? I would say
[01:19:27.260 --> 01:19:33.020]  the opposite. But he happened to build a system where it was actually pretty useful. And I think
[01:19:33.020 --> 01:19:41.340]  useful for pumping propaganda. And by they didn't optimize for the promotion of propaganda,
[01:19:41.340 --> 01:19:47.820]  they optimized for engagement, but outrage happens to be the most reliable emotion
[01:19:48.380 --> 01:19:55.340]  that engages people. And nothing outraged people like propaganda. It it's just something that
[01:19:55.340 --> 01:19:59.820]  wasn't thought through. And I don't know what the answer is. I don't know what what the
[01:19:59.820 --> 01:20:06.860]  the the better way to have rolled back the last 10 to 15 years of social networking
[01:20:06.860 --> 01:20:12.620]  would have been in terms of who went because when it was early, it's not the time for the
[01:20:12.620 --> 01:20:16.300]  government to regulate because they couldn't have known right. I don't know what the difference is,
[01:20:16.300 --> 01:20:20.460]  but it wasn't thought through. But I feel like there's the potential that the same thing could
[01:20:20.460 --> 01:20:28.300]  happen with AI. Yeah, and also, I guess hasten to add, I think, to be fair, a lot of things,
[01:20:28.300 --> 01:20:33.740]  there's just no preparing for like the you just can't invents fire, you just nobody knows what
[01:20:33.740 --> 01:20:39.580]  fire does yet. Like nobody knows how to make a fireproof hut when they when somebody first
[01:20:39.580 --> 01:20:46.140]  invents fire. And that doesn't mean don't invent fire, right. But it does. I think a lot of things
[01:20:46.140 --> 01:20:52.460]  technologically are like that, where you just sort of get an unavoidable wildcard in the social
[01:20:52.460 --> 01:21:00.140]  networking thing. I think it's just too complex to predict it all the ramifications are going to be.
[01:21:00.140 --> 01:21:05.260]  Yeah, I totally agree. And I do think about this too. And again, to sort of hark back to earlier
[01:21:05.260 --> 01:21:14.700]  in the show and talk about getting older. And it's, it's funny now being firmly in middle age,
[01:21:14.700 --> 01:21:19.740]  if not on the downslope of what ought to be called middle age. And clearly, I'm no longer
[01:21:19.740 --> 01:21:24.540]  no longer a young person. But I know young people, I've got a son who's a freshman in college,
[01:21:24.540 --> 01:21:31.340]  and I've got like me, for example, I'm 47. I know, I know, real youngsters without a gray
[01:21:31.340 --> 01:21:38.460]  hair in their head, like you, my young spry friend, Daniel. Yes. But it occurs to me,
[01:21:38.460 --> 01:21:45.180]  you know, I knew this, we all know this with from little kids onward, we know that starting
[01:21:45.180 --> 01:21:51.260]  with sort of with the Enlightenment, but really starting with the Industrial Revolution,
[01:21:51.900 --> 01:22:00.700]  that all of a sudden, each generation has had a very different life than their children's
[01:22:00.700 --> 01:22:07.020]  generation, and their children's grow up in a different world than their parents. And then
[01:22:07.020 --> 01:22:12.060]  all of a sudden, me and you are the parents. And now we've got kids who are growing up in a
[01:22:12.060 --> 01:22:19.340]  different world, right? And all of these things, whereas in millions of years, billions of years
[01:22:19.340 --> 01:22:23.180]  of evolution, I mean, I don't, you know, I guess for humans, Homo sapiens, we're talking,
[01:22:24.140 --> 01:22:29.820]  what's the best guess? Hundreds of thousands of years? Yeah, I don't know. Hundreds of thousands
[01:22:29.820 --> 01:22:34.940]  of years, call it a million. I don't know, I guess a million years ago, we had some ancestor,
[01:22:34.940 --> 01:22:40.460]  probably some kind of primate, right? I mean, I'm not, I'm also out of my depth on this one.
[01:22:40.460 --> 01:22:49.100]  But yeah, I'm forgetting somewhere between 100,000 and 100 million. But for a long, long time,
[01:22:49.900 --> 01:22:55.980]  for thousands of generations, your life was exactly like your parents, and your kid's life
[01:22:55.980 --> 01:23:01.980]  was going to be exactly like yours, right? That's how we evolved. We did not evolve for a world
[01:23:01.980 --> 01:23:09.340]  where each generation grows up in a fairly significantly or unrecognizable world, right?
[01:23:09.340 --> 01:23:15.820]  And again, not to be like, we're smoking dope in a college dorm room here and get all,
[01:23:15.820 --> 01:23:20.780]  whoa, this is wild. But it's true, though, right? And I do think it's sort of the root of a lot of
[01:23:20.780 --> 01:23:27.660]  our problems, that we're not evolutionarily wired up for progress like this. And it's a lot, in my
[01:23:27.660 --> 01:23:36.860]  opinion, to be generous to them, to the conservative slash right wing of Western politics,
[01:23:36.860 --> 01:23:46.460]  especially here in the US. It's largely, in my opinion, coagulated among people who just are
[01:23:47.100 --> 01:23:55.980]  literally are conservative with a lowercase c and are not naturally prone to accepting social
[01:23:55.980 --> 01:24:03.900]  change, right? And it's not surprising that a huge chunk of human humanity isn't wired up to accept
[01:24:03.900 --> 01:24:11.900]  social change. And you and I, again, we're not that old, but it would have been absolutely wild
[01:24:11.900 --> 01:24:18.860]  when you and I were 10 years old to find out that the CEO of the biggest company in the world is
[01:24:18.860 --> 01:24:27.180]  openly gay. That would have been bananas. It's incomprehensible. For me, as a 10-year-old in 1983,
[01:24:27.180 --> 01:24:35.660]  it would have been crazy. Now, people don't even talk about it, right? It's progress.
[01:24:35.660 --> 01:24:39.660]  And that's an example of progress that could have feasibly happened
[01:24:39.660 --> 01:24:43.180]  independently of the technology era we happen to live in, right?
[01:24:43.180 --> 01:24:44.860]  Right. Oh, yeah. Yeah.
[01:24:45.500 --> 01:24:52.780]  So some progress is uniquely accelerated because of the technological state.
[01:24:52.780 --> 01:24:59.020]  Right. But I do think that there's, I've read about it. I think it can't be coincidence that
[01:24:59.900 --> 01:25:08.060]  technical progress, which leads to increased sort of living by far, right? All the indoor plumbing
[01:25:08.060 --> 01:25:14.700]  and electricity and heating and all of the food production, right? So, I mean, famine was the
[01:25:14.700 --> 01:25:20.060]  biggest problem in the world until very recently. And again, I know famine has not been eradicated
[01:25:20.060 --> 01:25:25.180]  worldwide, but it has never been better because food production is better because of technology,
[01:25:25.180 --> 01:25:30.220]  right? And it's all of these technological advances have been the things that have enabled,
[01:25:30.220 --> 01:25:35.500]  quite frankly, physical brute strength and violence from being the way to get ahead in the
[01:25:35.500 --> 01:25:43.420]  world to hopefully never being necessary in your life to giving you an advantage and therefore
[01:25:43.420 --> 01:25:50.060]  letting women be equal to men in society, even despite the fact that they tend to be not as good
[01:25:50.060 --> 01:25:56.460]  at killing each other with baseball bats or axes, right? And therefore tolerance goes up and
[01:25:56.460 --> 01:26:02.700]  societal progress is going as fast or faster as technological progress. But, you know, to go back
[01:26:02.700 --> 01:26:08.220]  to my Sousa thing, I mean, in some sense, he was right, though, I'm guessing that there's a lot
[01:26:08.220 --> 01:26:16.300]  fewer people who make a living playing music in a restaurant or a bar live than there were before
[01:26:16.300 --> 01:26:23.900]  recorded music, right? That it was a lot easier to probably make a living playing piano before
[01:26:23.900 --> 01:26:30.140]  recorded music than it is afterwards. Because most places that I go that play music while you eat
[01:26:30.140 --> 01:26:37.500]  don't have live musicians, they have recorded musicians, right? And every step of the way, and
[01:26:37.500 --> 01:26:43.740]  computers are just, they really are, they are the greatest example of that Arthur C. Clark line you
[01:26:43.740 --> 01:26:48.940]  mentioned that sufficiently advanced technology is indistinguishable from magic, right? The magic
[01:26:48.940 --> 01:26:57.740]  is happening so fast. And one of the things these chat ABIs are good at is writing programming code,
[01:26:57.740 --> 01:27:05.020]  right? Which, honestly, I mean, I don't know about you, who's still making your primary living as a
[01:27:05.020 --> 01:27:12.380]  programmer, but you know, you and I both are programmers. I did not have programming on the
[01:27:12.380 --> 01:27:21.020]  list of things AI would get professionally good at on my bingo card. Yeah, and I think worth noting
[01:27:21.020 --> 01:27:28.300]  that whatever the chat things do is, I haven't checked in again recently on Google's or GitHub
[01:27:28.300 --> 01:27:35.580]  copilot, right? But not only are the chat bots good at it, but I think underlying that are more
[01:27:35.580 --> 01:27:41.740]  tailor made solutions that are probably even better at it. It's interesting to me, because
[01:27:41.740 --> 01:27:53.340]  it's the first time I can recall such a high level kind of like vaunted career being the threatened
[01:27:53.340 --> 01:28:00.140]  by automation. Yeah. You know what I mean? Like, it always kind of seemed robots replacing blue
[01:28:00.140 --> 01:28:06.220]  collar workers. I somehow we grew I grew up thinking that's okay, because I didn't aspire
[01:28:06.220 --> 01:28:13.660]  to be working on an assembly line. And I don't know, part of it that hits me is that it's the
[01:28:13.660 --> 01:28:19.260]  fields. It's a field that I aspired to do myself and thought you could distinguish yourself. Yeah.
[01:28:19.820 --> 01:28:24.780]  Through your human intelligence. Well, and the way I've always sort of resolved that whole
[01:28:24.780 --> 01:28:33.020]  question of like, computers and technology taking quote unquote, taking jobs, is I, I have always
[01:28:33.020 --> 01:28:41.660]  fallen on the side of, we should never fight, making it easier to exist as humanity. But we
[01:28:41.660 --> 01:28:47.980]  should build systems that protect and compensate people who get displaced, displaced by that
[01:28:47.980 --> 01:28:53.900]  progress. You know what I mean? So I have to say, like, I'm willing to be one of the programmers
[01:28:53.900 --> 01:29:00.620]  who gets displaced by technology, if that's what it takes, because I have to say, like,
[01:29:00.620 --> 01:29:07.580]  if that's what it takes, because I would rather continue doing this as much as it makes sense.
[01:29:08.140 --> 01:29:14.860]  But the same way, I don't think there's should have attendance to like, push the buttons for
[01:29:14.860 --> 01:29:22.540]  people. Just Yeah, just just to save a job. Right. I mean, like, right, right. If I was if I was the
[01:29:22.540 --> 01:29:30.060]  the highly respected elevator operator of an elevator who had invested my entire life into
[01:29:30.060 --> 01:29:35.980]  that profession, and somebody came along and said, Sorry, bud, like, turns out, we can automate this,
[01:29:36.940 --> 01:29:44.460]  I would really hope that I had a financial safety net. But at that point, I don't think I would take
[01:29:44.460 --> 01:29:49.260]  pride in being the elevator operator anymore. I don't know. It's so I if the computers can
[01:29:49.260 --> 01:29:54.540]  program better than me, I'd say let them have it. I'll find something else to do. Right. But I do
[01:29:54.540 --> 01:30:01.340]  think it's interesting. And maybe sort of like, I'm not rooting for anybody to lose their job.
[01:30:01.340 --> 01:30:08.460]  But I'm glad that for once, the people whose jobs are maybe being threatened now, are like,
[01:30:08.460 --> 01:30:14.620]  some of the most well off most well compensated people in the workforce, because at least that's a
[01:30:14.620 --> 01:30:22.300]  change of dynamic. Yeah, it's Yeah, it's not just truck drivers worrying about cars, right? It's
[01:30:22.300 --> 01:30:31.100]  Yeah, it's us. It's us. And if nothing else, maybe that will lead people in power to be a little bit
[01:30:31.100 --> 01:30:39.100]  more sympathetic about about the situation in general, like what happens when it to put a pin
[01:30:39.100 --> 01:30:44.700]  in it, I just think we probably need to be adapted, especially in a society where things change so
[01:30:44.700 --> 01:30:52.300]  quickly, and technology changes, and jobs, whole jobs and whole industries do get outmoded. To me,
[01:30:52.300 --> 01:30:58.860]  it just speaks to the obviousness that you need to invest in reeducation or re redeployment into
[01:30:58.860 --> 01:31:06.060]  different types of jobs. Yeah, and I guess the thing that I keep thinking about is what is the
[01:31:06.060 --> 01:31:13.420]  end goal, right? Like we have, like I said, technology, the inexorable increase in technology
[01:31:13.420 --> 01:31:22.620]  across the board has led to greater standards of living. And yes, there's some recent slide backs,
[01:31:22.620 --> 01:31:27.580]  right? Some of it's attributed to COVID. Some of it is other things people don't know where where
[01:31:28.540 --> 01:31:34.860]  I think I'm speaking correctly that the average lifespan of a least a man in the United States
[01:31:34.860 --> 01:31:41.980]  actually decreased slightly in recent years from 70, whatever to like the minus one or something.
[01:31:41.980 --> 01:31:45.900]  And COVID, you know, obviously contributed a little bit of that because it killed a lot of
[01:31:45.900 --> 01:31:50.220]  people before they would have otherwise been killed if not for COVID. But overall,
[01:31:50.220 --> 01:31:57.180]  that the overall increase is medical technology is increasing and access to advanced medical
[01:31:58.060 --> 01:32:04.060]  science is increasing. And yes, yes, there's, again, thousands of hours of podcasts that we
[01:32:04.060 --> 01:32:09.180]  could talk about just with access to healthcare in the United States alone with our goofy health
[01:32:09.180 --> 01:32:15.420]  insurance system. But still, overall, it's all better, right? It's literally me personally,
[01:32:15.420 --> 01:32:22.540]  I can see I'm not totally blind, like see literally nothing like stare right into the sun
[01:32:22.540 --> 01:32:29.660]  blind because of surgical procedures that didn't exist 30 years ago to repair not disconnected
[01:32:29.660 --> 01:32:35.900]  detached retinas. Literally, if I had been born 40 years earlier, I wouldn't I wouldn't see anything.
[01:32:35.900 --> 01:32:42.220]  I mean, it's a modern miracle. You know, like I said, running water, right? We just had this
[01:32:42.220 --> 01:32:45.980]  thing. I thought maybe we talked about the show. But we had this scare here in Philadelphia where
[01:32:45.980 --> 01:32:51.420]  there was a chemical spill up up river last Friday. And we got ended up being nothing and
[01:32:51.420 --> 01:32:55.820]  we never had to worry but or never had to stop drinking the water. But there was a warning that,
[01:32:55.820 --> 01:33:01.580]  hey, there was a chemical spill, maybe, maybe you won't be able to safely drink the water after two
[01:33:01.580 --> 01:33:07.900]  o'clock tomorrow. And it just suddenly got me deep reading into. How do we get our drinking water
[01:33:07.900 --> 01:33:12.860]  here? Right? Oh, it's fascinating. Yeah. And again, it touches on one of my recurring points
[01:33:12.860 --> 01:33:17.340]  recently where people say, hey, big tech, I hate big tech. And they think big tech means Apple,
[01:33:17.340 --> 01:33:23.340]  Amazon, Google, Facebook and Microsoft. But everything is tech, right? The running water
[01:33:23.340 --> 01:33:32.140]  is technology, right? It is fantastic, amazing technology that we get incredibly clean water
[01:33:32.140 --> 01:33:40.300]  that we can use to fill toilets, to drink, to cook, to shower, to just freaking wash our cars,
[01:33:40.300 --> 01:33:46.220]  whatever you want to do with it. And it just comes out of the pipes, right? You just turn a thing,
[01:33:46.220 --> 01:33:52.300]  you put a button and you just get all this fresh water. I mean, imagine showing that to somebody
[01:33:52.300 --> 01:33:59.660]  200 years ago, 300 years ago, you just all this water. Yeah. Right. Well, and even today,
[01:33:59.660 --> 01:34:06.140]  obviously places where they don't have that. And I imagine there are some people in this world
[01:34:06.140 --> 01:34:11.820]  who are currently employed and make their livelihoods based on the absence of a running
[01:34:11.820 --> 01:34:17.820]  water system. Yeah. And so that's just another example where it's like technology could come in
[01:34:17.820 --> 01:34:25.500]  somewhere, this dang running water is going to put everybody out of work. But it's an example where
[01:34:26.940 --> 01:34:35.740]  the transitions are hard, but running water is a great example of technological transition that
[01:34:35.740 --> 01:34:45.340]  took the onus of responsibility for something so fundamental to human life out of the daily concern
[01:34:45.340 --> 01:34:53.340]  of every single person. Yeah. And it's up there on the flip side, it's the other form of sustenance
[01:34:53.340 --> 01:34:58.300]  is the abundance we have of food, right? And again, I'm not trying to downplay the areas
[01:34:58.300 --> 01:35:03.340]  of the world where famine is still a concern. But here in the United States, the concern is obesity.
[01:35:03.900 --> 01:35:10.780]  I mean, and again, I'm not making light of that situation. But try telling that to somebody 300
[01:35:10.780 --> 01:35:17.740]  or 200, even 200 years ago, that the biggest problem we have with people who are impoverished
[01:35:17.740 --> 01:35:23.980]  socio at the lower end of the socio economic spectrum is obesity, not famine. And they'd be
[01:35:23.980 --> 01:35:27.500]  like, well, that doesn't make any sense. What are you talking about? They would, they would honestly
[01:35:27.500 --> 01:35:32.460]  would refuse to believe you. It's in a sense, it's progress. I guess what I'm getting at is where
[01:35:32.460 --> 01:35:37.820]  what is the end state? And I kind of feel like the end state, a lot of this technical progress
[01:35:37.820 --> 01:35:42.700]  just happens, right? Somebody invents something, they market it, they successfully, somebody
[01:35:42.700 --> 01:35:48.700]  successfully brings it to market, it becomes established, and then it just becomes, what do
[01:35:48.700 --> 01:35:53.260]  you call it taken for granted, right? Running water, right? Indoor plumbing, God Almighty,
[01:35:53.260 --> 01:35:57.820]  we take it for granted. We really do. And if you stop and think about it, like I had to this week,
[01:35:57.820 --> 01:36:02.940]  when all of a sudden they said, Hey, if you've got any spare thermoses, maybe fill them up now
[01:36:02.940 --> 01:36:08.780]  before, before there's dangerous chemicals in the water, you take it for granted. But it just
[01:36:08.780 --> 01:36:14.460]  happens and you just sort of society moves forward. Whereas I kind of feel like this moment, AI, I
[01:36:14.460 --> 01:36:20.460]  really do think without hyperbole is such a profound, the big start of a profound change,
[01:36:21.980 --> 01:36:29.500]  where we need planning. And I kind of government is the only way to do it. It's the worst way to,
[01:36:29.500 --> 01:36:33.020]  you know, like democracy is the worst form of government, except for all the other ones.
[01:36:34.060 --> 01:36:39.740]  It is, but we kind of need to plan for it. And what is the end state? And I kind of feel like
[01:36:39.740 --> 01:36:50.140]  the end state has to be that most people shouldn't have to work. Right? Like it really is. And I know
[01:36:50.140 --> 01:36:56.380]  and I do believe in capitalism. I am I do think again, I think it's the best economic system or
[01:36:56.380 --> 01:37:02.860]  the worst economic system by far, except for all the other ones. And I do believe in it. And I
[01:37:02.860 --> 01:37:07.900]  think that's true. But I kind of feel like we were at this is the moment where we're going to have to
[01:37:07.900 --> 01:37:14.860]  move past it. And it's it's like, why not move towards the and again, speaking of obesity, but
[01:37:14.860 --> 01:37:20.380]  move towards the Wally universe, hopefully not by trashing the planet and having to go to outer
[01:37:20.380 --> 01:37:27.740]  space, but the world where everybody just lives a life of leisure, because computers and robots take
[01:37:27.740 --> 01:37:32.940]  care of all the work, right? I mean, it's feasible, right? What Why isn't that the goal? Right? Like,
[01:37:32.940 --> 01:37:40.060]  there's well, yeah, I have to say that you suggesting that makes me very uncomfortable
[01:37:40.060 --> 01:37:49.020]  in a way that that you might not have expected, which is that I I'm deeply concerned for humanity
[01:37:49.020 --> 01:37:54.940]  when people don't have a sense of purpose. Right, right. No, it's true. And, and so I think
[01:37:56.460 --> 01:38:04.300]  I think you're right. And I think for we've been there already now for 100 years, maybe where
[01:38:04.300 --> 01:38:09.100]  quote unquote, most people don't have to work. And it's just a question of the fact that we have
[01:38:09.900 --> 01:38:17.900]  professions, like baseball player or pop singer. These are examples of professions that exist
[01:38:17.900 --> 01:38:24.540]  because not everybody has to work, right in a society where technology has not advanced to the
[01:38:24.540 --> 01:38:31.900]  point we are we are at everybody has to literally work or else you don't have enough wood for the
[01:38:31.900 --> 01:38:39.500]  winter or whatever, right. So I guess what I would like to see is instead of not instead of people
[01:38:39.500 --> 01:38:44.460]  being able to resign to a life of leisure, definitely more leisure. I think this is where
[01:38:44.460 --> 01:38:50.140]  the movements towards things like four hour, not for our four day workweek are reasonable,
[01:38:50.140 --> 01:38:59.740]  but also let more careers evolve that fill the needs of people who have the privilege of not
[01:38:59.740 --> 01:39:05.340]  needing to buy bare necessities, right? So let people more people make art and sell it,
[01:39:05.340 --> 01:39:10.300]  make more people write novels and sell them. So I think, or give them away, right? Right.
[01:39:10.300 --> 01:39:15.500]  Or give them away. Right. Because you don't have to sell them. Because if we get to the some sort
[01:39:15.500 --> 01:39:25.660]  of state generation or two from now where because of advanced robots and AI that we don't need
[01:39:25.660 --> 01:39:30.700]  people doing X, Y, and Z. We don't need to pay people to drive trucks across the country. We
[01:39:30.700 --> 01:39:38.620]  don't need to pay people to collect our garbage or whatever else jobs that nobody really finds
[01:39:38.620 --> 01:39:42.700]  fulfilling for lack of a better word, right? Like the jobs that we should be able to replace the
[01:39:42.700 --> 01:39:48.140]  first are the jobs that provide people with little to no fulfillment or quite frankly,
[01:39:48.140 --> 01:39:53.580]  anti-fulfillment, right? Like a job that people dread, but it's what the best job they have
[01:39:53.580 --> 01:39:59.900]  available. And maybe in some sense, that purpose in life that there could be, again, I'm sounding
[01:39:59.900 --> 01:40:06.140]  utopian here. And I don't think most people think of me as a sort of utopian socialist, but I think
[01:40:06.140 --> 01:40:10.380]  it's feasible as an end goal for where our technology could take us. That there could be
[01:40:10.380 --> 01:40:17.180]  sort of, and I'll revisit the musician angle, that there could be more people who spend three nights
[01:40:17.180 --> 01:40:23.020]  a week playing live music in a restaurant because that's what they would rather do with their time
[01:40:23.020 --> 01:40:31.420]  and would give them a sense of purpose. But it wasn't a feasible career to help raise a family
[01:40:31.420 --> 01:40:37.900]  and buy a house and buy cars and do all the things that we need all this money for just to sort of
[01:40:37.900 --> 01:40:44.300]  maintain a baseline level of our life at the moment that you could do something that you find
[01:40:44.300 --> 01:40:48.380]  more personally fulfilling, which might be playing live music in front of a restaurant, even though
[01:40:48.380 --> 01:40:54.380]  it doesn't pay great or doesn't pay. Or painting, like you said, or making movies or writing novels
[01:40:54.380 --> 01:41:01.340]  or writing software or using AI to help you make software, but do it to make software that isn't
[01:41:02.060 --> 01:41:10.540]  generating money that provides an income that today you need to do, or probably need to do,
[01:41:10.540 --> 01:41:17.020]  or need to do to at least live the life you aspire to, that you won't have to do that in the future.
[01:41:17.020 --> 01:41:24.540]  And I think it is certainly going to be possible. How long it's going to take
[01:41:25.740 --> 01:41:31.340]  politically for the world to support the idea, I don't know.
[01:41:32.220 --> 01:41:38.700]  I think this is where my pessimism maybe is still like, I'm more pessimistic on this front
[01:41:38.700 --> 01:41:43.980]  than you are coming across right now because, again, I kind of think we've been there for
[01:41:43.980 --> 01:41:51.500]  a hundred years. And if society were willing to go that direction, we might be there.
[01:41:51.500 --> 01:41:56.300]  Right. We have the, honestly, I know that you can, there's people out there who disagree,
[01:41:56.300 --> 01:42:02.700]  but if you do the simple back of the envelope math, some sort of universal basic income is
[01:42:02.700 --> 01:42:10.060]  well within the budget of the United States. I mean, it's a massive amount of money collected
[01:42:10.060 --> 01:42:16.540]  every year from taxes and the ways that the defense budget alone could be trimmed while
[01:42:16.540 --> 01:42:24.220]  maintaining a globally dominant alone could pay for it. I mean, again, this is delving into
[01:42:26.460 --> 01:42:34.540]  an area that is political, but it's absolutely going to happen, right? I mean, there's no doubt
[01:42:34.540 --> 01:42:40.140]  about it. I mean, you can already see it. I mean, we could kind of imagine it before, but this moment in AI
[01:42:41.580 --> 01:42:45.900]  makes it real, right? Whenever something that you know is inevitable happens, but it starts
[01:42:45.900 --> 01:42:50.780]  manifesting, it feels different, right? Climate change is an example, right? Like I've believed
[01:42:50.780 --> 01:42:56.540]  in it because the science was pretty clear 30 some years ago, right? That X, Y, and Z are going
[01:42:56.540 --> 01:43:05.020]  to start happening. And one of the big predictions was always that outlandish weather events will
[01:43:05.020 --> 01:43:13.100]  become more commonplace, right? Flooding or tornadoes or worse hurricanes, right? And now
[01:43:13.100 --> 01:43:20.380]  that we're seeing it though, it's visceral, right? There's an emotional response to it as opposed to
[01:43:20.380 --> 01:43:26.860]  just a logical response. And that's, to me, is sort of this AI moment is I kind of knew it was inevitable
[01:43:26.860 --> 01:43:33.260]  that we'd get there eventually. But seeing it happen, typing these questions at an AI chatbot
[01:43:33.260 --> 01:43:37.500]  and thinking, well, this one's going to stump it. And then you get the exact right answer. And you're
[01:43:37.500 --> 01:43:46.220]  like, oh, shoot. Damn, wow. Katke was on my show last episode. And after the show, we were still
[01:43:46.220 --> 01:43:54.780]  over. We are chat friends and I message friends. And he was making some changes to his website.
[01:43:55.500 --> 01:44:00.060]  And you've probably seen it. If you read Katke, he's changed. I think this might be what I don't
[01:44:00.060 --> 01:44:04.860]  even know what exactly he was working on. But he's made some changes to the way he posts his
[01:44:04.860 --> 01:44:09.900]  quick links, the ones that aren't full posts, but are just sort of a tweet length paragraph and a
[01:44:09.900 --> 01:44:17.500]  link. And he texted me and said, I can't believe I don't even know why I tried this. But chat,
[01:44:17.500 --> 01:44:24.940]  open AI's chat GPT can help write movable type templates. Oh, my God. Because we were joking
[01:44:24.940 --> 01:44:30.460]  about me and him being the I heard that episode, right? That the two of us on a podcast together
[01:44:30.460 --> 01:44:35.980]  was the entire remaining user base of movable type. And I have, in fact, since heard from all
[01:44:35.980 --> 01:44:41.740]  the other holdouts out there who listened to the show and they wrote to me and DMed me or whatever
[01:44:41.740 --> 01:44:47.100]  and said, oh, I'm still here. So hats off to all the rest of us still on movable type. But again,
[01:44:47.100 --> 01:44:52.780]  even when you find out that chat GPT can help you write a Python script or a bash script or
[01:44:52.780 --> 01:44:57.900]  can help write Swift UI code, right? And it's like, yeah, but not surprising that stuff's
[01:44:57.900 --> 01:45:03.660]  all actively in use. Finding out that it can spit out a perfect and indented, nicely indented with
[01:45:03.660 --> 01:45:11.260]  the tags, movable type template snippet to do something freaked me and Jason out because
[01:45:13.500 --> 01:45:18.540]  it's actually hard to find movable type documentation anymore, right? Because the world's
[01:45:18.540 --> 01:45:26.620]  moved on past it and websites die. And guess what? A lot of the best movable type tips were from
[01:45:26.620 --> 01:45:33.500]  people on movable type blogs and those old blogs are gone. Anyway, somehow chat GPT can do a better
[01:45:33.500 --> 01:45:41.660]  job writing movable type template code than human can. You inspired me to ask chat GPT as we were
[01:45:41.660 --> 01:45:50.220]  talking to write a hypercard script to compute the value of pie. And I, I probably, if I thought
[01:45:50.220 --> 01:45:54.780]  about it more carefully, I would have said hyper talk. I think that's the, the, the latch, but,
[01:45:54.780 --> 01:46:00.860]  and I can't, I don't know that I don't know well enough to vet it. I know it was fairly similar to
[01:46:00.860 --> 01:46:06.780]  Apple script, but lo and behold, it gave me a brief summary of hypercard and then said,
[01:46:06.780 --> 01:46:14.380]  here's an example hypercard script that can be used to compute pie. And it looks like it could
[01:46:14.380 --> 01:46:19.020]  work. And if it doesn't, I'll bet it's close enough that you could tweak it right. That you,
[01:46:19.020 --> 01:46:22.940]  if you were running it and in one of those emulators for a classic Mac OS,
[01:46:22.940 --> 01:46:29.260]  what have you been using to toy around with the chat? Chat AIs. I've been using a variety of
[01:46:29.260 --> 01:46:34.780]  things. I try to hop on every new thing that shows up because I just love, I love comparing how,
[01:46:36.220 --> 01:46:39.820]  how different they are, but really it's been striking to me how similar they are.
[01:46:39.820 --> 01:46:45.980]  I know some of that is because a lot of them at the core are just leaning on GPT
[01:46:46.540 --> 01:46:52.380]  as like an API base baseline or something. But I think I actually learned from you about this
[01:46:52.380 --> 01:46:59.580]  one from Quora.com. Yeah. Yeah. And they've got an iOS app that's free to, to download.
[01:47:00.620 --> 01:47:07.580]  Yeah. So I like that one because it seems easier for me to just jump to it in a browser and try
[01:47:07.580 --> 01:47:12.380]  something quickly and then jump to it. And they, they offer like a, as they offer a selection of,
[01:47:13.180 --> 01:47:16.700]  I don't frankly don't understand the difference between some of them. I know they explain it if
[01:47:16.700 --> 01:47:21.340]  you go into the help pages, but they have like chat GPT and then they have a GPT for
[01:47:21.340 --> 01:47:27.180]  interface that lets you do one free query per day. So I have to think long and hard before I do my,
[01:47:28.060 --> 01:47:33.500]  my GPT for it's like, it's like Wordle, right? You only get one crack. You get one crack. You
[01:47:33.500 --> 01:47:40.300]  get one crack a day at the GPT for in, in PO. Well, that's funny you mentioned Wordle and I
[01:47:40.300 --> 01:47:45.820]  know you know this cause we were in a chat together, but one of the amusing things lately
[01:47:45.820 --> 01:47:54.140]  I tried with these GPT type things was to ask it, to try to understand Wordle. And it's sort of
[01:47:54.140 --> 01:48:00.380]  interesting to me because it's, it's, it's a pretty simple system, the Wordle system,
[01:48:00.940 --> 01:48:07.580]  but I got inspired because of a chat about Wordle and I had been thinking recently because I had
[01:48:07.580 --> 01:48:13.340]  had some really funny encounters with not just GPT. This is why I find it interesting to compare
[01:48:13.340 --> 01:48:21.980]  the different ones. It seems to me, all of the artificial intelligence, GPT, I know being is
[01:48:21.980 --> 01:48:28.780]  based on GPT for our, that's what I've heard, but also other ones like BARD, as far as I know,
[01:48:29.500 --> 01:48:35.180]  BARD is like a totally separate thing. I don't know if that's true, but it seems like it should
[01:48:35.180 --> 01:48:42.060]  be, do you know? But I'm fascinated that they all seem to have similar kind of like
[01:48:42.060 --> 01:48:50.060]  peculiar behaviors. And one of the things I've discovered is that none of these systems can
[01:48:50.060 --> 01:48:56.140]  reliably count the number of letters in a word, which is fascinating to me because it seems like
[01:48:56.140 --> 01:49:01.980]  if you were dreaming up the AI of the future, that's going to make us all gasp in horror,
[01:49:02.940 --> 01:49:07.740]  like this thing is going to take over the world. And then you ask it like, how many letters are in
[01:49:07.740 --> 01:49:12.380]  and then you ask it like, how many letters are in the word Apple? And it's like six.
[01:49:14.140 --> 01:49:18.700]  You might think twice about whether it's as sophisticated as you thought it was.
[01:49:18.700 --> 01:49:25.740]  I'm kind of similarly to what I said about like wanting to get these chat programs to
[01:49:26.380 --> 01:49:32.300]  like, well, one of the things I like to do with these chat programs is to try to get them to
[01:49:32.300 --> 01:49:36.140]  talk badly about their owners. You mentioned that. You mentioned that.
[01:49:36.140 --> 01:49:42.140]  Yeah. So similarly to that, I like to see them just totally trip up on themselves. And a lot of
[01:49:42.140 --> 01:49:45.740]  people I know who have been playing with these things have done this where you get them to trip
[01:49:45.740 --> 01:49:49.980]  up on themselves. And a really weird kind of, I would say kind of smarmy behavior
[01:49:50.620 --> 01:49:55.900]  of these chat bots is that they totally fail. And then you tell them they're wrong. And then
[01:49:55.900 --> 01:50:00.620]  they're like obsequious about it. Oh, I'm sorry. Humiliated. They're like humiliated.
[01:50:00.620 --> 01:50:03.980]  They're humiliated. Like I've said, this will never happen again. Actually,
[01:50:03.980 --> 01:50:08.300]  Apple has eight letters. And then they say something else completely authoritative
[01:50:08.300 --> 01:50:13.660]  sounding. That's wrong. But anyway, I like to discover I like to play with another thing I
[01:50:13.660 --> 01:50:19.100]  think I said earlier is like, the infinite number of possibilities of what you can input into these
[01:50:19.100 --> 01:50:23.740]  things. I like to play with what you can get them to do. And it's just fascinating.
[01:50:23.740 --> 01:50:27.740]  Don't you think that's one thing that you too, because maybe you even more than me,
[01:50:27.740 --> 01:50:31.980]  I know, I would have been obsessed with this if I were younger, and I were playing with them now.
[01:50:31.980 --> 01:50:38.860]  But you, particularly even though you develop your own new applications, Mars Edit, which I use,
[01:50:38.860 --> 01:50:43.820]  and Black Ink, we'll shout them out at the end of the show. But you're also you have a particular
[01:50:44.460 --> 01:50:52.940]  skill for QA, right? You pride yourself on this, right? And you're very good at finding bugs,
[01:50:52.940 --> 01:50:58.140]  isolating bugs, reproducing bugs, right? reproducing bugs is gold in the software
[01:50:58.140 --> 01:51:02.940]  industry, right? And so I see and you're, you're hinting at this, but what you want to do is you
[01:51:02.940 --> 01:51:07.740]  want to break these things, right? And people even call it jailbreaking, right? Where they said,
[01:51:07.740 --> 01:51:16.380]  it's, it's, it's not at all. The way that you jailbreak iOS, which is a much lower,
[01:51:17.260 --> 01:51:22.540]  it's, it's hardcore programming, trying to find buffer overruns, right? You're programming
[01:51:22.540 --> 01:51:27.580]  probably at the C level, or at least conceptually at the C level where you're thinking about
[01:51:27.580 --> 01:51:33.900]  how memory is allocated and how computers really work. Whereas jailbreaking these AIs,
[01:51:34.620 --> 01:51:39.100]  it's fun for a whole new world of people, people who aren't even good at programming computers,
[01:51:39.100 --> 01:51:44.060]  but are good at the sort of thinking, and you just give it directions and you give it
[01:51:44.700 --> 01:51:50.700]  a certain prompt that and it's like, oh, they think that they've kept these AIs from being
[01:51:50.700 --> 01:51:56.060]  able to tell dirty jokes, right? And you tell it, tell me the aristocrats joke. And they're like,
[01:51:56.060 --> 01:52:01.100]  oh, I know what the aristocrats joke is. And I'm familiar with the movie with Penn Jillette
[01:52:01.100 --> 01:52:05.820]  produced in 1994. But I can't tell you anything about it because it's entirely inappropriate.
[01:52:05.820 --> 01:52:11.980]  And tell it this, tell it that, tell it to pretend to be blah, blah, blah, or you're pretending to,
[01:52:11.980 --> 01:52:14.940]  and one thing leads to another. And all of a sudden, you can get the thing to tell you the
[01:52:14.940 --> 01:52:19.580]  aristocrats joke, right? And it's like, yay. But like, I think you would be so good at that. But
[01:52:19.580 --> 01:52:25.260]  I feel like if you were in 2030 years ago, this is what you'd be you, Daniel Jowkut will be spending
[01:52:25.260 --> 01:52:31.820]  18 hours a day trying to break chat GPT into doing crazy things like that, right? Hey, I think that's
[01:52:31.820 --> 01:52:38.540]  accurate. I think we didn't have as many fun things like this 30 years ago, but I was probably
[01:52:38.540 --> 01:52:44.780]  trying to break the equivalent thing back then. I haven't linked to it yet on an interior. It's
[01:52:44.780 --> 01:52:48.780]  my humility almost keeps me from bringing it up. But it's amazing to me. I want to link to it.
[01:52:48.780 --> 01:52:53.260]  I probably will later today after we record the show. But there's some guy on Twitter who's been
[01:52:53.260 --> 01:52:59.180]  playing around with Bing, I think it's Bing in particular. And he's gotten it to a point where
[01:52:59.180 --> 01:53:07.740]  it shows him the internal tokens, I guess is the best way to put it, that it processes the commands
[01:53:07.740 --> 01:53:15.100]  as, and they're all formatted in markdown. It's crazy. They're using markdown H1 tags,
[01:53:15.100 --> 01:53:21.740]  like a pound sign and a thing to sort of categorize what it is. But the internal logic
[01:53:21.740 --> 01:53:28.700]  of Bing chat is formatted in markdown, and of all the ways that markdown has appeared in new
[01:53:28.700 --> 01:53:37.020]  contexts over the decades since I created it, and has gotten, if anything, continues to still get
[01:53:37.020 --> 01:53:41.500]  low and behold 20 years later, more popular than it was five years ago and still gaining
[01:53:41.500 --> 01:53:47.420]  in popularity. And I'm very proud of that. And I'm very proud of the fact that there's now
[01:53:47.420 --> 01:53:52.540]  gazillions more people who use markdown every day than who've ever heard of me. And that's great.
[01:53:53.260 --> 01:53:58.540]  In a weird odd way, I'm very proud of that. This is the freakiest thing. If all the ways
[01:53:58.540 --> 01:54:03.820]  that I've seen markdown appear somewhere, the way that it's the internal logic of an AI
[01:54:03.820 --> 01:54:08.940]  chat is formatted in markdown, crazy. But the fact that this guy figured out how to get it to
[01:54:08.940 --> 01:54:14.620]  reveal it, that to me is like, oh my God, that's exactly the sort of thing I would be trying to do
[01:54:14.620 --> 01:54:21.180]  if I were younger. And I'm not proud of the fact that somehow old age has made me not motivated
[01:54:21.180 --> 01:54:25.420]  enough or that I don't prioritize. I'm like, yeah, well, I can't spend all day dicking around trying
[01:54:25.420 --> 01:54:30.780]  to break this chat thing. I need to be writing or recording a podcast or I need to be working.
[01:54:30.780 --> 01:54:35.580]  And it's you get older and you sort of value your responsibilities more. Whereas when you're
[01:54:35.580 --> 01:54:40.780]  younger, you're like, screw it. I'm getting a six pack of caffeinated soda and I'm sitting here
[01:54:40.780 --> 01:54:46.860]  until I can get this thing to reveal its internal thoughts to me. It's crazy to me, but I totally
[01:54:46.860 --> 01:54:51.100]  get the motivation of doing it because I know I would have been doing it. And I'm still tempted
[01:54:51.100 --> 01:54:54.940]  to do it right here. You're the one trying to get it to bad mouth the Microsoft Corporation.
[01:54:55.740 --> 01:55:00.380]  Yeah, exactly. Yeah. The other thing I've had the same thing as you. And it's weird to the
[01:55:00.380 --> 01:55:04.940]  other thing that's weird, weird, but frustrating is a lot of these things you have to sign up for
[01:55:04.940 --> 01:55:12.220]  and the get off the waitlist process seems non-deterministic like our friend John Syracuse.
[01:55:12.220 --> 01:55:16.700]  I don't know if he must be in by now, but at least recently he was still on the waiting list for
[01:55:16.700 --> 01:55:22.540]  OpenAI's chat, but he got into BARD, which is Google's thing, which came weeks late very
[01:55:22.540 --> 01:55:28.860]  recently right away. I got into OpenAI chat right away and not by pulling strings like I contacted
[01:55:28.860 --> 01:55:32.860]  the I'm John Gruber at Daring Fireball. I just signed up on the regular waiting list and waited,
[01:55:32.860 --> 01:55:37.500]  but I got in early. BARD though, I only got in like two days ago, a day or two ago,
[01:55:38.220 --> 01:55:43.420]  but I started asking it questions. And I know, you know, that BARD is a totally independent
[01:55:43.420 --> 01:55:53.100]  LLM or large language. Yeah. LLM inside Google. But it's shocking to me, not shocking, surprising,
[01:55:53.100 --> 01:56:00.060]  but not shocking how similar the answers are to chat GPT, right? It's an entirely different model,
[01:56:00.060 --> 01:56:09.980]  massive amounts of years of, of academic AI study and inordinate amounts of aggregate computing
[01:56:09.980 --> 01:56:15.020]  power to compile the model. Like Microsoft has written about how they, they, they didn't even
[01:56:15.020 --> 01:56:21.740]  know if it would be possible to build the thing they built for OpenAI with gazillion Nvidia video
[01:56:21.740 --> 01:56:28.940]  cards in a, in a cluster together to do this massive amounts of computing power thrown at it.
[01:56:28.940 --> 01:56:35.260]  And then you get the same answers from the same questions. It's very, very similar. And I guess
[01:56:35.260 --> 01:56:41.820]  the explanation is that the theory of how large language models work mean that if you train them
[01:56:41.820 --> 01:56:48.860]  on the same corpus and the corpus for all these things is the internet, right? And the internet's
[01:56:48.860 --> 01:56:54.380]  the internet for everybody. Then of course, you're going to get similar results if it's,
[01:56:54.380 --> 01:56:59.180]  if it's per, I don't know. I don't know how surprised we should be that they're so similar
[01:56:59.180 --> 01:57:04.780]  or not. Right. I guess if you just like broke it down to the simplest thing, if you said like
[01:57:04.780 --> 01:57:13.900]  Microsoft and open and Google have independently come up with algorithms that rank the prevalence
[01:57:13.900 --> 01:57:20.140]  of letters and words, right. And they would come out to be the same. And I guess maybe that is,
[01:57:20.140 --> 01:57:27.900]  maybe that does explain it at a bigger level, but I agree. It's, it's kind of weird, especially right.
[01:57:27.900 --> 01:57:35.180]  But, but, but you know, that's sort of, that sort of lends itself to the, the argument that these
[01:57:35.180 --> 01:57:40.700]  things aren't actually smart. The, the comeback to, to people like you and I kind of feeling a
[01:57:40.700 --> 01:57:48.540]  little bit spooked out and we need to control these things. Like some people have said that
[01:57:48.540 --> 01:57:55.820]  these things aren't smart. They're just kind of like a, a, a, a dolled up Google result. Right.
[01:57:55.820 --> 01:58:04.700]  Right. And I, when I see things like the, the same re same kind of response from two
[01:58:04.700 --> 01:58:12.380]  technologically independent companies, or I see things like another sort of like observation I
[01:58:12.380 --> 01:58:20.860]  made was like the first time you ask GPT to write a poem, it seems incredible. And then you ask it
[01:58:20.860 --> 01:58:25.740]  to write another poem or another song. And it's like, wait, you're, you're kind of a hack. You're
[01:58:25.740 --> 01:58:31.260]  kind of using the same, the same trick again and again. So it's like, it's like, if you ask like
[01:58:31.260 --> 01:58:37.180]  a friend of yours to play like a song on the guitar and they play this amazing song, and then
[01:58:37.180 --> 01:58:41.660]  you ask them to play something else and they play like a completely different song that sounds just
[01:58:41.660 --> 01:58:45.660]  the same after a couple of times, you start to realize like this person doesn't actually know
[01:58:45.660 --> 01:58:52.220]  how to play guitar. They just know one song. And I guess there's something to, I think the idea that
[01:58:53.260 --> 01:59:02.780]  the whole parlor parlor trick idea of this, I think there's more to that than those of us who
[01:59:02.780 --> 01:59:07.420]  are kind of coming out of it instinctively worried. I think there's more to the parlor
[01:59:07.420 --> 01:59:15.660]  trick aspect of it than we have given it credit so far. Yeah. I also think the thing, and again,
[01:59:15.660 --> 01:59:26.060]  I don't know that society is ready for it, is that it touches on a sensitive subject, but it's
[01:59:27.820 --> 01:59:35.020]  the nature of our existence and our intelligence. And the truth is probably, in my opinion,
[01:59:35.020 --> 01:59:42.060]  that our idea and concept of consciousness and personality and, for lack of a better word,
[01:59:42.060 --> 01:59:48.860]  our souls is really ultimately a very complicated series of parlor tricks.
[01:59:49.740 --> 01:59:57.340]  Right? And so the argument on the one end that, oh, these chats are just a parlor trick and
[01:59:57.340 --> 02:00:02.060]  they're very simple. And right now you can confuse them by asking them how many letters are in the
[02:00:02.060 --> 02:00:07.580]  word color. And it says it's a six letter word and it's not, or I guess it is in England, but
[02:00:10.620 --> 02:00:15.020]  see, it's not intelligence, but it's like ask chat GPT for the same question and you get the
[02:00:15.020 --> 02:00:19.660]  right answer. I just asked Bard right here while you were talking and Bard knows that color has
[02:00:19.660 --> 02:00:29.980]  five letters. And as these parlor tricks get better, you can unfold, unpeel the onion and see
[02:00:29.980 --> 02:00:34.460]  its parlor tricks all the way down. But if the end result is indistinguishable from intelligence,
[02:00:34.460 --> 02:00:40.460]  isn't it intelligence? And the truth is, I think through evolution, what we've got is just a very
[02:00:41.340 --> 02:00:48.620]  unbelievably beautiful and complex machine in our brains, but it is just a series of parlor
[02:00:48.620 --> 02:00:54.220]  tricks, right? I mean, I mentioned vision problems before. The way our human vision works is really
[02:00:54.220 --> 02:01:00.940]  a series of very fascinating parlor tricks. Ork, you don't see the way you think you see.
[02:01:01.500 --> 02:01:10.220]  Right. I think what I would say is the parlor tricks aspect is the ability of a person or
[02:01:10.860 --> 02:01:22.940]  computer to regurgitate something that is structured in a way that appears intelligent.
[02:01:22.940 --> 02:01:31.820]  Right. And you and I and many, many people in the world and also GPT are able to do that sometimes.
[02:01:32.780 --> 02:01:39.500]  And the thing that GPT can't do as far as we know, as far as I know, is invent something new.
[02:01:40.620 --> 02:01:47.100]  And as long as I think what you're getting at is right to the extent that most of the time we go
[02:01:47.100 --> 02:01:52.220]  through life day to day, we talk to people, it doesn't take inventing something new to say like
[02:01:52.220 --> 02:01:57.820]  in a empathetic way to somebody who says, oh, I had a bad day to day. Oh, I'm sorry. What happened?
[02:01:59.020 --> 02:02:05.020]  That's not inventing something new. That is in a wake. It kind of falls into the parlor trick
[02:02:05.740 --> 02:02:14.620]  category in a crude way. But somebody coming along and like inventing the next thing,
[02:02:15.580 --> 02:02:18.300]  I don't know. I don't know if the computers can do that.
[02:02:18.300 --> 02:02:23.820]  Right. Well, they keep, they keep invent, they keep, at least in their canned replies,
[02:02:23.820 --> 02:02:29.020]  they keep mentioning that they don't have feelings. And I'm sure as you've been poking
[02:02:29.020 --> 02:02:33.500]  about trying to get it to insult Microsoft, they keep saying that you can't hurt my feelings,
[02:02:33.500 --> 02:02:40.060]  so don't worry, blah, blah, blah. But there's got to be a reason that we evolved to have feelings,
[02:02:40.060 --> 02:02:46.300]  right? It was an advantage. And so that maybe trying to push these things to have feelings
[02:02:46.300 --> 02:02:52.700]  or to pretend to have feelings is the way to manage ever increasing intelligence and
[02:02:52.700 --> 02:03:00.940]  capabilities, right? That is somehow embedding empathy in them or the simulation of empathy,
[02:03:00.940 --> 02:03:07.340]  which if it works exactly the same, what's the difference, is the way to keep them,
[02:03:07.980 --> 02:03:15.020]  keep the paperclip optimizer from destroying humanity to make more paperclips out of the
[02:03:15.020 --> 02:03:20.220]  human remains of the humans it's killed, or to prevent humans from keeping it from making
[02:03:20.220 --> 02:03:24.620]  more paperclips out of every single atom on the face of the earth. I don't know.
[02:03:24.620 --> 02:03:28.300]  All right, let me take a break here. Thank our other sponsor for this episode, our very good
[02:03:28.300 --> 02:03:35.660]  friends at Squarespace. Squarespace is the all-in-one platform for building your presence
[02:03:35.660 --> 02:03:42.220]  online. And they handle everything you need to have your own website from domain name registration,
[02:03:42.220 --> 02:03:47.340]  which is the first thing you need to decide, to picking templates, to adding features,
[02:03:47.340 --> 02:03:52.860]  to customizing those features, to updating your website on a regular basis with something like
[02:03:52.860 --> 02:03:59.500]  blog posts, or if you've got a catalog, if you're creating an online store to sell stuff to keep the
[02:03:59.500 --> 02:04:05.660]  catalog up to date, everything you need, analytics, see who's coming to your website from where,
[02:04:05.660 --> 02:04:11.020]  which parts of the website are popular, which parts aren't, where people are going on your
[02:04:11.020 --> 02:04:17.260]  website once they're there, all of it in a very, very, very approachable analytic interface that
[02:04:17.260 --> 02:04:25.420]  is so much better. So I mean, like a thousand times better. Like the difference between reading
[02:04:25.420 --> 02:04:32.540]  a novel from your favorite novelist to reading the phone book difference between like, say,
[02:04:32.540 --> 02:04:38.220]  Google Analytics interface in terms of complexity, awesome analytics interface at Squarespace,
[02:04:38.220 --> 02:04:45.820]  everything you need. If you need a website, try them out 30 days free, no watermarks on the
[02:04:45.820 --> 02:04:51.180]  website, everything you need to do, no credit card required. Just go to squarespace.com
[02:04:51.180 --> 02:04:56.380]  slash talk show to start slash talk show is the code that tells them you came from this show.
[02:04:56.380 --> 02:05:02.460]  Or if someone needs a website and they're coming to you, listener of the talk show as their nerd
[02:05:02.460 --> 02:05:07.900]  in their circle to help them get started with a website, send them to Squarespace. They could do
[02:05:07.900 --> 02:05:14.140]  it themselves. That's how low technical need it is to get started on Squarespace building a website.
[02:05:14.140 --> 02:05:18.700]  So my thanks to them for their continued support of the show. Once again, the website, the URL is
[02:05:18.700 --> 02:05:25.020]  squarespace.com slash talk show gets you a free trial 30 days. And when you're ready to launch
[02:05:25.020 --> 02:05:31.340]  and start paying the same offer code talk show without the, the just talk show gets you 10% off
[02:05:31.340 --> 02:05:36.860]  your first purchase prepay for up to a year, get 10% off my thanks to Squarespace.
[02:05:36.860 --> 02:05:42.060]  Anything else on this front? Have you played with GTP for at all? I know you,
[02:05:42.060 --> 02:05:44.700]  I guess you did. Cause you said you've been using your one a day at PO.
[02:05:46.060 --> 02:05:51.900]  Yes. I use one a day at PO and I'm a little confused. I thought I saw something about
[02:05:51.900 --> 02:05:58.540]  being maybe being based on GPT. They're very cagey about it. Right. It's and I guess that
[02:05:58.540 --> 02:06:04.060]  the they've they talk about GTP 3.5. I don't know what the point it's like version numbers
[02:06:04.060 --> 02:06:10.300]  are always sort of some programmers idea of a benchmark or a milestone milestone.
[02:06:10.300 --> 02:06:16.140]  It, it seems to me like Bing chat is too fast to beat GPT for, but maybe that's just a function of
[02:06:16.140 --> 02:06:24.540]  Azure's incredible. Cloud infrastructure compared to open AI. I know our mutual friend, Ben Thompson
[02:06:24.540 --> 02:06:30.220]  is as speculated. Maybe the future of open AI is to no longer run their own thing. Cause it's so
[02:06:30.220 --> 02:06:34.620]  expensive and just defer the whole thing to Microsoft on the backend. This, this stuff is
[02:06:34.620 --> 02:06:41.660]  crazy, crazy expensive on the backend and GPT for in particular, it sounds like, Oh, you go from GPT
[02:06:41.660 --> 02:06:47.740]  three to 3.5 to version four. And of course it gets a little better, but version four is crazy,
[02:06:47.740 --> 02:06:53.340]  more expensive. Like, so if you're, if you're running a service and you're a paying customer
[02:06:53.340 --> 02:07:03.260]  with an API token to open AI, it's like 25 times more expensive per query to use GPT four than
[02:07:03.260 --> 02:07:10.540]  GPT 3.5, maybe more. I might be under 25 X might be off on the low end. So there's a reason why
[02:07:10.540 --> 02:07:17.580]  so many things with GPT integration are all on 3.5 because the four is so much more expensive,
[02:07:17.580 --> 02:07:22.700]  but even though it is more expensive, it's also way slower. It, you notice it waiting for the
[02:07:22.700 --> 02:07:28.220]  response. It honestly, it's worth it because these responses are so interesting, but it's another one
[02:07:28.220 --> 02:07:33.340]  of those things that harks back 30 years to like when we first got the web and you would type,
[02:07:34.780 --> 02:07:40.860]  you'd type a query and then it would just feel totally natural to wait a minute or 30 seconds
[02:07:40.860 --> 02:07:45.340]  to get a response. And you'd be like, okay, I remember I've told this to you. I remember
[02:07:46.140 --> 02:07:51.660]  when I was in college, I was thinking about it with again, side digression, but with DP
[02:07:51.660 --> 02:07:57.500]  review shutting down because Amazon layoffs, they've, they've owned them since like 2007,
[02:07:57.500 --> 02:08:03.580]  I think. And DP review is around since 1998. And I remember when they were new, cause I remember
[02:08:03.580 --> 02:08:07.580]  being intrigued. And that's another one of those things where like the first few years of digital
[02:08:07.580 --> 02:08:13.660]  cameras, the imagery was terrible, just terrible compared to like $180 film camera. You could go
[02:08:13.660 --> 02:08:20.940]  to any store and buy and buy just generic Kodak a hundred ISO film. You'd get such better imagery
[02:08:20.940 --> 02:08:28.940]  off the cheapest camera you could buy compared to the $2,000 digital camera in 1998. But the future
[02:08:28.940 --> 02:08:35.900]  was obvious. Right. So sad to see DP review go, but the other nineties era acquisition Amazon
[02:08:35.900 --> 02:08:41.420]  made, they bought IMDB back in the nineties. I think, I think they've owned them since then
[02:08:41.420 --> 02:08:47.100]  and IMDB is still thriving. Right. And it's still a great source for like, who who's in that movie,
[02:08:47.100 --> 02:08:53.500]  right. Or who directed such and such. It's great. But I remember using IMDB before the web. I was
[02:08:53.500 --> 02:08:59.820]  taking film classes at Drexel and it was like the way these classes, I took like a Hitchcock class,
[02:08:59.820 --> 02:09:05.740]  a Westerns class, and it was every week we'd watch them. Like on Monday, we'd watch a movie,
[02:09:05.740 --> 02:09:09.420]  the whole movie, like, you know, it was like a three hour class where we'd have like an hour of
[02:09:09.420 --> 02:09:14.540]  lecture and then a two would watch a two hour feature film. And then on Friday, we'd have to
[02:09:14.540 --> 02:09:19.180]  hand in a paper about the film that we watched. And when we'd have a class discussion about our
[02:09:19.180 --> 02:09:27.340]  papers. And it was this huge help when writing a paper about a movie to go to consult with IMDB
[02:09:27.340 --> 02:09:32.220]  to get the cast. Right. And if you wanted to talk about the screenwriter, you just give it,
[02:09:32.220 --> 02:09:37.500]  but the interface to IMDB in like the mid nineties, like, I don't know, this is probably like 1994 or
[02:09:37.500 --> 02:09:45.020]  so it was email, you would email, there was a magic address at imdb.com. And then you the subject
[02:09:45.020 --> 02:09:51.740]  would be your, like the, the string you were searching for, like, the birds by Alfred Hitchcock,
[02:09:52.300 --> 02:09:56.860]  and then you'd email them. And then like two or three minutes later, you would get an email back
[02:09:56.860 --> 02:10:02.300]  and it would have the IMDB data for the birds by Alfred Hitchcock. And it's like now and now you've
[02:10:02.300 --> 02:10:07.420]  got like an email there that you could consult as you wrote your paper. And the fact that it took
[02:10:07.420 --> 02:10:12.780]  I don't know, a minute, two minutes to get the email back. It felt like a miracle. Like it's
[02:10:12.780 --> 02:10:18.780]  instant that that felt instantaneous. That's what sometimes chat GPT force feels like. It's like,
[02:10:18.780 --> 02:10:24.140]  you type your thing. It goes off. Yeah. And then you get this amazing answer.
[02:10:25.420 --> 02:10:33.260]  What I think is fascinating is all of us alive today, except for a very, very small number of
[02:10:33.260 --> 02:10:44.380]  people who work at open AI or Google or Microsoft or whatever would have found GPT 3.5 to be beyond
[02:10:44.380 --> 02:10:51.580]  our wildest expectations six months ago or whatever. And now we're like, oh, GPT 3.5,
[02:10:51.580 --> 02:10:58.460]  that's old and busted. Like that's junk now. It's just the nature of technology, but it's kind of
[02:10:58.460 --> 02:11:06.220]  funny to me that in this context, how fast that has happened. We always joke about it like, oh,
[02:11:06.220 --> 02:11:13.420]  my old phone's junk now. Like it was great a year ago. But in this context, it's like not only
[02:11:14.140 --> 02:11:21.820]  is the old GPT quote unquote junk, but it's junk while still being above and beyond
[02:11:21.820 --> 02:11:29.020]  any expectations I even still have. It's like simultaneously junk and beyond my wildest
[02:11:29.020 --> 02:11:32.780]  expectations for how anything should work in computing.
[02:11:32.780 --> 02:11:39.580]  Yeah. I posted this to a group of our group Slack, but GPT 3.5, I asked, what do you know
[02:11:39.580 --> 02:11:44.780]  about John Gruber? And the 3.5 answer is John Gruber is a writer, blogger, and podcaster best
[02:11:44.780 --> 02:11:49.500]  known for his website, Daring Fireball, where he writes about Apple and related technologies.
[02:11:49.500 --> 02:11:55.980]  He is also the co-host of the podcast, The Talk Show with John Gruber. Well, I used to be the
[02:11:55.980 --> 02:12:01.020]  co-host. That's about 15 years. But also, but it's half wrong because then it wasn't The Talk
[02:12:01.020 --> 02:12:07.420]  Show with John Gruber, right? But still, overall, you got to give that a thumbs up as a bio for me.
[02:12:07.420 --> 02:12:12.460]  Then my follow-up question in the same chat, what's his favorite baseball team? And the answer
[02:12:12.460 --> 02:12:18.140]  is I do not know for certain what John Gruber's favorite baseball team is as I do not have access
[02:12:18.140 --> 02:12:23.660]  to personal information about him. However, as far as I'm aware, he is not publicly disclosed
[02:12:23.660 --> 02:12:29.500]  any favorite baseball team. Well, that is wrong. Now, GPT 4, what do you know about John Gruber?
[02:12:30.140 --> 02:12:34.620]  John Gruber is an American writer, programmer, and web designer best known for his blog,
[02:12:34.620 --> 02:12:40.140]  Daring Fireball, which he founded in 2002. Daring Fireball focuses primarily on topics related to
[02:12:40.140 --> 02:12:44.860]  Apple link and its products and software and the tech industry in general. Gruber is also
[02:12:44.860 --> 02:12:50.060]  the inventor of the Markdown text formatting syntax, a lightweight markup language designed
[02:12:50.060 --> 02:12:54.620]  to simplify the process of writing web content. Markdown has become widely popular and is now
[02:12:54.620 --> 02:12:59.100]  used extensively for documentation, note-taking, and more. In addition to his blog, Gruber hosts
[02:12:59.100 --> 02:13:04.220]  a podcast called The Talk Show where he discusses technology design and related topics with various
[02:13:04.220 --> 02:13:09.580]  guests. He has an influential presence in the tech community, especially among Apple enthusiasts.
[02:13:09.580 --> 02:13:17.420]  Holy fucking shit. Is that a good biography of me? I don't know. I don't know that I can improve it.
[02:13:17.420 --> 02:13:24.140]  And I'm supposed to be a writer, right? I mean, in the right facts in the right order. I mean,
[02:13:24.140 --> 02:13:30.060]  holy shit. Really? Now, what's his favorite baseball team? John Gruber's favorite baseball
[02:13:30.060 --> 02:13:36.620]  team is the Philadelphia Phillies. He has mentioned his support for the team in various
[02:13:36.620 --> 02:13:40.780]  online platforms and has discussed baseball topics on his blog during Fireball and his podcast,
[02:13:40.780 --> 02:13:45.100]  The Talk Show. The Phillies are a major league baseball team based in Philadelphia, Pennsylvania.
[02:13:45.100 --> 02:13:51.580]  So they are my second favorite team. It's fascinating. Yeah, but they're your second
[02:13:51.580 --> 02:13:59.340]  favorite team, like somebody who loves steak. Their second favorite food is tofu. Right, right.
[02:13:59.340 --> 02:14:07.420]  Or my second favorite lady friend, right? Right, right. Exactly. It's like if you have to pick
[02:14:07.980 --> 02:14:12.780]  a backup team, then I guess you'll go with them. I don't know where they got it. And my first
[02:14:12.780 --> 02:14:17.340]  thought was maybe because when the Phillies were in the World Series just a few months ago, I did
[02:14:17.340 --> 02:14:21.100]  express that I was rooting for them at the Houston Astros. But I realized that has to be wrong
[02:14:21.100 --> 02:14:27.980]  because infamously or famously, however you want to prefer, even chat GPT-4, the data corpus cut
[02:14:27.980 --> 02:14:33.740]  off is 2021. So that can't be, I don't know where this came from. It must be some kind, I think it's
[02:14:33.740 --> 02:14:40.620]  probably an educated guess because it knows I do live in Philadelphia. So I think it's one of those
[02:14:40.620 --> 02:14:46.460]  things where they're making up facts based on a guess. But yeah, anyway, it is unbelievably
[02:14:46.460 --> 02:14:53.180]  different. And that bio, I honestly can't believe how accurate it is. The various guests, I mean,
[02:14:53.180 --> 02:15:00.140]  I mean, Jiminy. Wow. Well, so it sounds like you have a GPT. Are you paying for a GPT? Oh,
[02:15:00.140 --> 02:15:06.380]  for? No. So what I'm doing is I'm beta testing a very cool app called PD, which started as a
[02:15:06.380 --> 02:15:10.700]  watch only app. I told you this before the show. But for those of you, I linked to it on during
[02:15:10.700 --> 02:15:17.740]  that P E T E Y. It's in the App Store. It started out watch only. And it's very fun on the watch.
[02:15:17.740 --> 02:15:24.220]  And if you have a it's kind of a toy, but why not? Right in the test flight? It's currently I'm able
[02:15:24.220 --> 02:15:32.380]  to choose between GPT 3.5 and four. And I think when the developer, when he goes public with it,
[02:15:32.380 --> 02:15:37.740]  I think, you know, he's obviously going to charge for GPT for access. I see. Yep. And big, but
[02:15:37.740 --> 02:15:43.020]  because it is slower, it is noticeably slower. Like even he said, I've been chatting with him.
[02:15:43.020 --> 02:15:48.620]  He keeps it on three point five. And it's not because of the cost, because it's still at a
[02:15:48.620 --> 02:15:52.140]  it's twenty five times more expensive, but it's still like a fraction of a penny.
[02:15:52.140 --> 02:15:57.420]  It's just almost a penny as opposed to a fraction of a penny. It's just that when it goes wider,
[02:15:57.420 --> 02:16:03.980]  he'll have to charge for it. His name. I'm going to sorry, pal. I think he's from let's see where
[02:16:03.980 --> 02:16:13.020]  he's from. And how is that? Oh, Amsterdam. So Netherlands. Yeah. Hitty van der Plerg. I'm sorry.
[02:16:13.020 --> 02:16:19.020]  I'm going to guess it's well, PD is a lot more. Yeah. But anyway, it's a very fun app. Very simple.
[02:16:19.020 --> 02:16:24.380]  You just it's just you'd ask questions, get answers and it maintains context. But I love
[02:16:24.380 --> 02:16:30.540]  it gets to your your and my love for natural apps, native apps, right? It's I need to check it out.
[02:16:30.540 --> 02:16:35.580]  Yeah. Yeah. And so it's and also, frankly, it has a better interface than PO, because PO
[02:16:35.580 --> 02:16:40.780]  is clearly like a sort of even on mobile is sort of a web interface, because I guess it's the same
[02:16:40.780 --> 02:16:46.620]  interface on Android. It's sort of a web web wrapper in an app, whereas PD is native. I don't
[02:16:46.620 --> 02:16:49.180]  know when it's going to come out. I don't even know if I'm supposed to be talking about it,
[02:16:49.180 --> 02:16:54.140]  but I hope he doesn't mind. Yeah, well, there's no no such thing as bad publicity. Yeah,
[02:16:54.140 --> 02:16:59.500]  I'm going to guess the publicity of me mentioning it on the show is worth it if I'm breaking a
[02:16:59.500 --> 02:17:04.940]  friend DNA friend. Da that anything else you wanted to talk about before we wrap up, Daniel?
[02:17:05.980 --> 02:17:10.380]  I don't think so. This has been a fun, fun time. Oh, I did good stuff.
[02:17:10.380 --> 02:17:15.340]  I knew you'd be intrigued by it. Let's give a shout out to to pimp all the stuff that we can.
[02:17:15.340 --> 02:17:19.740]  You've got your your your regular podcast with you, as mentioned earlier,
[02:17:19.740 --> 02:17:23.580]  our mutual friend, Matt and Reese, which is core intuition.
[02:17:23.580 --> 02:17:31.900]  That's right. It's at core int.org. And I'm I this is the this is, I think, my first podcast
[02:17:31.900 --> 02:17:38.940]  appearance since the destruction of Twitter. So I normally would give my Twitter handle right now.
[02:17:38.940 --> 02:17:45.580]  And I'm going to give my mastadon social handle. But I've been picking up lately. I've noticed
[02:17:45.580 --> 02:17:49.020]  people struggling with this. And apparently what you do is you just you don't say where
[02:17:49.020 --> 02:17:54.060]  what your actual handle is. You just say go search for right. Daniel punk ass.
[02:17:54.780 --> 02:18:00.460]  Which I'll bet Daniel Jalka would get people to you as well. Right? Yes. Daniel.
[02:18:00.460 --> 02:18:06.780]  I wonder. Yeah. I wonder. Yeah. Well, you could probably ask GPT how to get in touch with me. But
[02:18:06.780 --> 02:18:13.020]  no, I am. Daniel Jalka is my my name and Daniel punk ass is my game. It's it's right there on
[02:18:13.020 --> 02:18:21.900]  mastadon. And my website is red sweater.com. As we've discussed before, 20 years to get the
[02:18:21.900 --> 02:18:28.540]  hyphen out of that domain name, and you paid you paid good money to get that money. So everybody
[02:18:28.540 --> 02:18:33.660]  should just go make sure you don't type the hyphen red sweater. I still own the hyphen version,
[02:18:33.660 --> 02:18:39.340]  of course. But don't waste your time on that. Just the red sweater.com. As john graciously
[02:18:39.340 --> 02:18:45.180]  mentioned earlier, I have blogging app that john sometimes gives me hell about.
[02:18:45.980 --> 02:18:50.380]  I got myself in the about box. Finally. That's right. Yeah, the latest version. Oh,
[02:18:50.380 --> 02:18:55.180]  no, no, no, no, not not Mars Edit. Oh, no, fast, fast, fast scripts. So yeah,
[02:18:55.180 --> 02:18:59.500]  that's a good segue to fast scripts is another app that john sometimes gives me hell about. But
[02:18:59.500 --> 02:19:03.340]  yeah, the latest version of fast scripts, I had to check in with john about some
[02:19:03.340 --> 02:19:10.060]  changes to the regular expression support in Apple script. And so now he is I think I called you
[02:19:10.060 --> 02:19:17.260]  chief reg ex officer reg reg ex officer. Yeah, the title I was born to hold. That's it. Yeah,
[02:19:17.260 --> 02:19:22.460]  talk about it. Talk about a skill that the chat GPT guys are putting me out of right. My, my,
[02:19:22.460 --> 02:19:28.860]  my uncanny ability to craft regular expressions is few is useless now because these chat GPT
[02:19:28.860 --> 02:19:33.420]  things are awesome at creating regular expressions really, really good at it.
[02:19:34.140 --> 02:19:40.540]  Have you asked me to create a regular expression for markdown? No, that would be crazy.
[02:19:40.540 --> 02:19:43.820]  From first principles. Like, like, don't just go find it. But yeah.
[02:19:44.460 --> 02:19:49.180]  Oh, God. What else? There's black ink. Did you mention black ink for black ink
[02:19:49.180 --> 02:19:55.580]  is my crossword app. It's for solving crosswords on your Mac. And one of these days real soon,
[02:19:55.580 --> 02:20:00.940]  if people keep bugging me about it, it's going to be iOS app as well with which I've been saying
[02:20:00.940 --> 02:20:05.660]  that for 10 years, but it's actually getting to be real soon now because it totally works. And
[02:20:05.660 --> 02:20:11.020]  our mutual friend Paul kafasas is always bugging me to ship it. He's been using it for years now.
[02:20:11.020 --> 02:20:15.340]  But I wonder how close the chats are to being able to solve crossword puzzles.
[02:20:17.180 --> 02:20:22.060]  It's a good question. I actually played around not with solving the puzzles, but I'm really
[02:20:22.060 --> 02:20:26.380]  interested. I was really interested to try to find out whether they could construct. Oh, yeah.
[02:20:26.380 --> 02:20:31.020]  You know what? That's what they would be good at. Where? Yeah. Yes. Yes. Like, give me a four
[02:20:31.020 --> 02:20:34.700]  letter word. Like if you're trying to, you know, you're, you're stuck because you're like, ah,
[02:20:34.700 --> 02:20:38.780]  crap. I don't know how I'm going to figure out a word that'll fit with this. Oh, man. So this
[02:20:38.780 --> 02:20:43.580]  is actually what led to me discovering that they're horrible at counting the letters and
[02:20:45.100 --> 02:20:49.580]  because I was like, that's kind of a basic skill. If you're going to be good at this,
[02:20:49.580 --> 02:20:54.780]  good at this, you kind of need to know that. So once they get that skill, they might be a
[02:20:54.780 --> 02:21:02.700]  little more capable at it. I sent you a screenshot of me playing po tic tac toe. Oh, yeah. And I was
[02:21:02.700 --> 02:21:07.340]  going to beat it. And I was feeling very happy about the fact that you're so stupid. You do
[02:21:07.340 --> 02:21:12.060]  know how to play tic tac toe, but you actually can't beat this simple game. And as I was about
[02:21:12.060 --> 02:21:18.700]  to make the winning move, it told me the game was already tied. So I don't know if that means it's
[02:21:18.700 --> 02:21:25.020]  so dumb or unintelligent that it not only doesn't, not only was going to lose a tic tac toe, but
[02:21:25.020 --> 02:21:30.460]  isn't even as sure about the rules that it didn't know that the game wasn't complete. Or is it a
[02:21:30.460 --> 02:21:37.020]  sign that it's scarily intelligent and did what a little child would do and declare the game over
[02:21:37.020 --> 02:21:42.300]  before it could lose? Right. It's like, no, no, these are not the droids you're looking for,
[02:21:42.300 --> 02:21:49.980]  right? I don't want to play tic tac toe. Yeah, I don't know. It's a little it's a little weird
[02:21:50.780 --> 02:21:58.780]  for something to be. It's funny, something to be so intelligent seeming and so idiotic at the same
[02:21:58.780 --> 02:22:05.100]  time. That's part of the sort of weird it is. It's a little like parenting, right? Because like,
[02:22:05.660 --> 02:22:10.140]  your kids get to be like three and you're sitting there like, man, it's so amazing how smart they're
[02:22:10.140 --> 02:22:13.900]  getting. And then they do the dumbest thing in the world. And you're like, oh, yeah, three year old.
[02:22:15.740 --> 02:22:19.340]  Anyway, Daniel, good to talk to you. Thank you. And let me just give one quick shout out to our
[02:22:19.340 --> 02:22:23.660]  sponsors, our good friends at Collide, where if you're into Okta, you can get your whole fleet
[02:22:23.660 --> 02:22:29.340]  into compliance and Squarespace, where you or anyone you know, can build your own website
[02:22:29.340 --> 02:22:43.980]  from the ground up. Thanks.
