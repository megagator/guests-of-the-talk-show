[00:00.000 --> 00:02.440]  I still have lots of WWDC stuff to talk about.
[00:02.440 --> 00:04.820]  And I was thinking one thing I didn't talk about
[00:04.820 --> 00:09.620]  was what was the plan B if it had rained in Cupertino?
[00:09.620 --> 00:11.760]  And I actually asked some people from Apple,
[00:11.760 --> 00:14.280]  I got to speak to a couple of people
[00:14.280 --> 00:17.800]  who were involved in setting up the big day
[00:17.800 --> 00:19.680]  or whatever, the special day,
[00:19.680 --> 00:23.120]  and what if it rained?
[00:23.120 --> 00:25.420]  The plan was more or less a shrug.
[00:26.840 --> 00:29.440]  It was, let's hope it doesn't rain.
[00:29.440 --> 00:32.240]  And apparently it did rain just a couple of days
[00:32.240 --> 00:34.200]  before the keynote, so.
[00:34.200 --> 00:36.200]  So there's a running joke that whenever I come
[00:36.200 --> 00:38.000]  to Cupertino, it rains, and they always say
[00:38.000 --> 00:39.860]  it's because of the Canadian low pressure system
[00:39.860 --> 00:41.320]  that I'm bringing with me.
[00:41.320 --> 00:44.040]  But I woke up in Cupertino and it was raining that morning.
[00:44.040 --> 00:46.080]  It had been raining at night.
[00:46.080 --> 00:47.440]  Yeah, what day did you get there?
[00:47.440 --> 00:48.600]  Was it Saturday?
[00:48.600 --> 00:49.880]  Yeah, I got there Saturday.
[00:49.880 --> 00:52.120]  See, I got there Saturday but very, very late,
[00:52.120 --> 00:53.760]  so I didn't see the remnants of it.
[00:53.760 --> 00:55.780]  But yeah, that's what I heard, that it was,
[00:55.780 --> 00:58.280]  within 48 hours, it would have been,
[00:58.280 --> 00:59.180]  I don't know if that would have been,
[00:59.180 --> 01:00.360]  I guess that would have been worse,
[01:00.360 --> 01:01.720]  but they would have made me leave.
[01:01.720 --> 01:02.560]  That's what would have happened.
[01:02.560 --> 01:04.340]  I did see you there, and it was good to see you,
[01:04.340 --> 01:05.880]  and I'll say the same thing I said to Jason.
[01:05.880 --> 01:07.440]  My God, it was good to see you.
[01:07.440 --> 01:09.960]  Now, with you, it is a little bit of a cheat move
[01:09.960 --> 01:12.880]  because I did see you just a couple of months ago
[01:12.880 --> 01:14.640]  in New York, briefly.
[01:14.640 --> 01:16.560]  We had like, what, a cup of coffee or something like that?
[01:16.560 --> 01:17.680]  Yeah, a cocktail or two.
[01:17.680 --> 01:19.520]  But still, good to see you.
[01:19.520 --> 01:21.240]  Did you have a seat in the sun for that,
[01:21.240 --> 01:23.300]  for the big show, for the keynote?
[01:23.300 --> 01:26.960]  No, so I'm usually very, again, Canadian, follow orders,
[01:26.960 --> 01:29.520]  but I looked, and I said, I just asked one of our
[01:29.520 --> 01:31.920]  mutual friends, can I go sit in the back in the shade?
[01:31.920 --> 01:33.960]  They said yes, and then I started moving,
[01:33.960 --> 01:35.760]  and then props to him, Neelay was like,
[01:35.760 --> 01:38.320]  that's the pro move, and just like went right behind me
[01:38.320 --> 01:39.840]  into the shade.
[01:39.840 --> 01:42.400]  We sat in the shade, Panzarino and Ben Thompson and I,
[01:42.400 --> 01:45.760]  just because we were hot, and it was because we weren't late
[01:45.760 --> 01:49.640]  but we had to walk from, we took the visitor center tour,
[01:49.640 --> 01:52.520]  and it was across the street, and it's, you know,
[01:52.520 --> 01:54.200]  just a bit of walking in the sun,
[01:54.200 --> 01:58.920]  and then we got some coffee and water and stuff,
[01:58.920 --> 02:01.200]  and so we had just been walking for 15 minutes
[02:01.200 --> 02:02.720]  in the hot sun, so we were a little hot,
[02:02.720 --> 02:04.480]  and we're like, eh, let's sit in the shade, cool off,
[02:04.480 --> 02:08.840]  and then we realized what a stroke of inadvertent genius
[02:08.840 --> 02:11.040]  it was, because it really wasn't about the temperature,
[02:11.040 --> 02:14.080]  it was people, people actually took a bad sunburst.
[02:14.080 --> 02:16.200]  Sam, one of the YouTubers who was there,
[02:16.200 --> 02:19.480]  very, very fair-skinned, looked fluorescent product red
[02:19.480 --> 02:21.220]  at the end of that session.
[02:21.220 --> 02:24.780]  It was a nice day, I don't know what they could do about it,
[02:24.780 --> 02:27.900]  and it was unusually, the sun was unusually fierce
[02:27.900 --> 02:30.620]  for Cupertino, I mean, it's, as everybody knows,
[02:30.620 --> 02:34.900]  it is a running joke that as much as Apple's weather app
[02:34.900 --> 02:38.060]  has always been interesting and overall very good,
[02:38.060 --> 02:40.580]  anything weather-related from Apple tends to have
[02:40.580 --> 02:45.580]  a severe Silicon Valley bias where the assumption is
[02:45.760 --> 02:48.580]  it's probably, no matter what day of the calendar,
[02:48.580 --> 02:51.580]  speaking of matching temperatures, it's probably
[02:51.580 --> 02:54.660]  around 71 degrees and partly sunny.
[02:54.660 --> 02:56.580]  Yeah, with a, with a very.
[02:56.580 --> 02:58.340]  To their credit, though, they did put sunscreen
[02:58.340 --> 02:59.860]  and a hat in every bag.
[02:59.860 --> 03:00.680]  Kiehl's.
[03:00.680 --> 03:01.520]  So as not to use them.
[03:01.520 --> 03:04.620]  It was Kiehl's, which is very expensive premium sunscreen,
[03:04.620 --> 03:08.940]  and I was only packing a carry-on, and it was big,
[03:08.940 --> 03:11.260]  it was like, I don't know, like eight ounces,
[03:11.260 --> 03:12.980]  so way too big to take in a carry-on,
[03:12.980 --> 03:15.460]  so I had to give it away to a, actually forget
[03:15.460 --> 03:17.500]  who I gave it to, but I found a fair-skinned friend
[03:17.500 --> 03:21.220]  who was like, yes, I will take it, and I was like, enjoy.
[03:22.400 --> 03:23.940]  Yeah, absolutely, but at least they thought of it,
[03:23.940 --> 03:25.060]  I thought that was nice.
[03:25.060 --> 03:28.860]  Yeah, anyway, we got lots to talk about still from WWDC,
[03:28.860 --> 03:29.700]  but I thought.
[03:29.700 --> 03:30.660]  The weirdest thing for me, if I could just interject,
[03:30.660 --> 03:32.660]  the weirdest thing for me is that they said
[03:32.660 --> 03:34.300]  that you had to, you had to, you had to test negative
[03:34.300 --> 03:35.980]  every day to get in. Oh, yeah.
[03:35.980 --> 03:37.460]  But they didn't, like, I couldn't find
[03:37.460 --> 03:39.500]  an established protocol, so I ended up just taking
[03:39.500 --> 03:41.600]  a selfie of me with my negative test result
[03:41.600 --> 03:45.160]  and then sending them all the information every morning,
[03:45.160 --> 03:49.020]  so some poor jerk at Apple or Health had to look
[03:49.020 --> 03:51.380]  at a dumb picture of me every morning with my test.
[03:51.380 --> 03:53.460]  Didn't you have the, for the media pass,
[03:53.460 --> 03:55.900]  didn't you have, like, it was like a portal
[03:55.900 --> 03:57.420]  you had to go through. Yes, yeah.
[03:57.420 --> 04:00.140]  Yeah, but that wasn't. I just uploaded that.
[04:00.140 --> 04:02.940]  Yeah, it was flaky, though, it didn't work for me
[04:02.940 --> 04:06.460]  on Sunday night, which worried me,
[04:06.460 --> 04:08.060]  because the keynote was Monday morning,
[04:08.060 --> 04:11.300]  and so what I did, I think I told you this
[04:11.300 --> 04:14.940]  when we got into each other, I wasn't sure what to do.
[04:14.940 --> 04:19.300]  I, it kept saying my picture, it was like you'd,
[04:19.300 --> 04:21.100]  Apple gave us in the media, like, here,
[04:21.100 --> 04:24.420]  click this link and submit a photo of your test,
[04:24.420 --> 04:27.340]  and I think the, it's a little bit more
[04:27.340 --> 04:29.260]  than the honor system, I guess.
[04:29.260 --> 04:31.540]  I actually don't know how this portal works,
[04:31.540 --> 04:35.180]  but if you ever take a COVID test, you'll notice,
[04:35.180 --> 04:38.140]  I get, you know, just a consumer, buy the drug store,
[04:38.140 --> 04:41.060]  take it at home COVID test, they all have little QR codes
[04:41.060 --> 04:44.180]  on them, and so each test has a unique identifier,
[04:44.180 --> 04:47.940]  so I, you know, I think it's some kind of checking,
[04:47.940 --> 04:50.400]  like, hey, is this a test that has ever been entered
[04:50.400 --> 04:53.940]  into one of these systems before to prevent somebody
[04:53.940 --> 04:58.300]  from just loading a rando negative test from the internet
[04:58.300 --> 05:01.680]  or sharing it or reusing the same test from the day before
[05:01.680 --> 05:05.100]  and uploading it, presumably they're actually looking
[05:05.100 --> 05:08.120]  at the QR code and checking that it's a unique test
[05:08.120 --> 05:10.440]  that's never been submitted as negative before,
[05:10.440 --> 05:14.260]  I don't know, but whatever, mine was a unique test
[05:14.260 --> 05:16.740]  that no one had ever used, it was a legit negative test
[05:16.740 --> 05:19.380]  that I had a photo of, and the system just kept saying
[05:19.380 --> 05:23.620]  it's, the error message could not have been less helpful,
[05:23.620 --> 05:26.700]  it was, it didn't say what was wrong with it,
[05:26.700 --> 05:28.740]  it just said this can't be accepted,
[05:28.740 --> 05:31.300]  so what I did on Monday morning to go to the keynote,
[05:31.300 --> 05:34.340]  it actually, I tried it from my phone instead of my Mac,
[05:34.340 --> 05:37.040]  number one, Monday morning, and it did go through,
[05:37.040 --> 05:40.540]  but I didn't get a green check mark, I got a yellow one
[05:40.540 --> 05:43.420]  that said, okay, we accepted your photo,
[05:43.420 --> 05:47.180]  but it didn't mean that they put me in the green lights,
[05:47.180 --> 05:49.700]  so what I did is I just took my test with me,
[05:49.700 --> 05:53.620]  I put it in my laptop bag, and it's the, what's the brand,
[05:53.620 --> 05:55.740]  Binax, I don't know, do you guys have the same brand
[05:55.740 --> 05:56.860]  in Canada?
[05:56.860 --> 05:59.140]  No, I don't know what mine is.
[05:59.140 --> 06:02.860]  It's the one from Abbott's, people in the US know it,
[06:02.860 --> 06:04.060]  it's like a purple box.
[06:04.060 --> 06:05.200]  Yeah, mine's the same.
[06:05.200 --> 06:08.140]  But the way that they work, and I bet a lot of people
[06:08.140 --> 06:09.940]  listening have taken these, because, I mean,
[06:09.940 --> 06:13.580]  it seems like that's part of our 2022 shared experience,
[06:13.580 --> 06:15.960]  is taking a lot of these COVID tests.
[06:15.960 --> 06:20.140]  With the Binax one, you have this long swab
[06:20.140 --> 06:22.420]  that you run around inside your nostrils,
[06:22.420 --> 06:23.960]  all right, gross, right?
[06:23.960 --> 06:27.260]  But then you put it in the cardboard test,
[06:27.260 --> 06:29.300]  fold it shut, and there's a bit of adhesive,
[06:29.300 --> 06:31.520]  so the gross part that's been up your nose
[06:31.520 --> 06:35.220]  is at least sort of locked into a little cardboard thing.
[06:35.220 --> 06:38.980]  So I didn't feel too gross about carrying around
[06:38.980 --> 06:41.260]  a cotton swab that had been up both my nostrils
[06:41.260 --> 06:44.820]  for 30 seconds, but it ended up not needing it.
[06:44.820 --> 06:46.340]  My only issue is that you had to wait
[06:46.340 --> 06:47.940]  till after 24 hours each time,
[06:47.940 --> 06:51.120]  so it got later in the day every day that it was there.
[06:51.120 --> 06:54.180]  Yeah, and it sounds to me, again,
[06:54.180 --> 06:57.460]  I don't blame Apple at all for requiring a negative test
[06:57.460 --> 07:00.180]  and for partnering with this, you know, somebody to do it.
[07:00.180 --> 07:03.420]  I think overall, everything they did
[07:03.420 --> 07:07.580]  to make it as COVID safe as they could,
[07:07.580 --> 07:10.540]  you know, with as much stuff being outside as possible,
[07:10.540 --> 07:13.200]  the whole cafeteria having the doors open,
[07:13.200 --> 07:15.060]  and the mask requirement.
[07:15.060 --> 07:17.740]  I mean, I roll my eyes at the mask requirement, frankly,
[07:17.740 --> 07:19.020]  but I realize, though,
[07:19.020 --> 07:20.580]  if they're gonna err on one side or the other,
[07:20.580 --> 07:22.660]  I can't blame Apple for doing it,
[07:22.660 --> 07:24.500]  especially outside with masks.
[07:24.500 --> 07:28.020]  It's like, I'm like, what's the point?
[07:28.020 --> 07:29.180]  But anyway, I don't blame them.
[07:29.180 --> 07:30.500]  But I know from talking to people
[07:30.500 --> 07:32.980]  that Apple was not happy with the functionality of the site,
[07:32.980 --> 07:35.120]  and the 24-hour thing was definitely weird.
[07:35.120 --> 07:37.060]  I get it that you wanna take it once a day,
[07:37.060 --> 07:40.540]  but if you submitted it at 8 a.m. the day before,
[07:40.540 --> 07:44.460]  submitting it at 7.59 a.m. the next day should work.
[07:45.700 --> 07:46.780]  I mean, like, to their credit,
[07:46.780 --> 07:50.020]  I didn't see anyone get sick at WWDC,
[07:50.020 --> 07:52.020]  and I saw everyone get sick at VidCon,
[07:52.020 --> 07:53.660]  so Apple did something right.
[07:53.660 --> 07:54.980]  Yeah, that's interesting.
[07:54.980 --> 07:56.420]  What I've noticed, and again,
[07:56.420 --> 07:59.220]  I just think it is such a weird disease,
[07:59.220 --> 08:01.900]  and I know that these current variants are super contagious,
[08:01.900 --> 08:04.700]  and luckily, everybody who's fully vaxxed and boosted
[08:04.700 --> 08:09.700]  seems to just suffer like a low-to-mid cold symptoms
[08:11.980 --> 08:14.740]  for a couple of days, but it doesn't, I don't know.
[08:14.740 --> 08:16.620]  I know a lot of people who went to WWDC.
[08:16.620 --> 08:20.300]  I don't know anybody who got COVID there, so that's great.
[08:20.300 --> 08:22.100]  But I do know lots of people,
[08:22.100 --> 08:24.300]  especially people over Twitter and stuff,
[08:24.300 --> 08:28.460]  who've gotten COVID in the last 10 to 12 weeks,
[08:28.460 --> 08:30.740]  and all the people I know who got it
[08:30.740 --> 08:33.980]  didn't do anything risky.
[08:33.980 --> 08:36.260]  It wasn't like, oh, I went to Vegas,
[08:36.260 --> 08:41.020]  or I went to, I don't know, a convention
[08:41.020 --> 08:41.860]  or something like that.
[08:41.860 --> 08:42.940]  It's all people who were like,
[08:42.940 --> 08:45.300]  yeah, I was kind of mostly just staying home.
[08:45.300 --> 08:46.940]  Somehow I got it.
[08:46.940 --> 08:48.100]  So for me, it's the people who went
[08:48.100 --> 08:50.820]  to the YouTuber boxing event in Florida, or now VidCon,
[08:50.820 --> 08:54.340]  so it's like, just avoid groups of wild YouTubers
[08:54.340 --> 08:55.180]  and you'll probably be fine.
[08:55.180 --> 08:58.420]  Yeah, boxing, anything where people are screaming inside
[08:58.420 --> 09:01.380]  seems like a bad, from what I know about how it spreads.
[09:01.380 --> 09:05.140]  I would say screaming or yelling indoors probably stay away.
[09:05.140 --> 09:06.500]  Yeah, but anyway, I don't know.
[09:06.500 --> 09:07.700]  Somehow I've avoided it.
[09:07.700 --> 09:09.820]  I don't attribute this to anything
[09:09.820 --> 09:12.640]  other than really dumb luck
[09:12.640 --> 09:15.420]  and whatever advantage being fully vaxxed gets.
[09:15.420 --> 09:16.940]  I'll say, but except the funny thing for me
[09:16.940 --> 09:19.020]  was that at VidCon, nobody wore a mask
[09:19.020 --> 09:19.940]  except for the Canadians.
[09:19.940 --> 09:21.220]  Whenever I was wearing one,
[09:21.220 --> 09:24.100]  because Transport Canada still mandates them,
[09:24.100 --> 09:25.320]  so the minute you get into the airport,
[09:25.320 --> 09:26.160]  you're wearing a mask,
[09:26.160 --> 09:28.380]  and then I just continued it through the indoor activities
[09:28.380 --> 09:31.020]  and I saw someone else like, yep, Canadian, ha!
[09:31.020 --> 09:32.820]  And we all left fine.
[09:32.820 --> 09:33.660]  Yeah.
[09:35.660 --> 09:37.860]  What kind of a, VidCon's huge, right?
[09:37.860 --> 09:39.300]  I mean, so it's like-
[09:39.300 --> 09:40.520]  The Anaheim Convention Center.
[09:40.520 --> 09:44.780]  And that's just like a typical big, super big cap.
[09:44.780 --> 09:45.700]  Like a Moscone event.
[09:45.700 --> 09:47.060]  Yeah, like a Moscone type thing.
[09:47.060 --> 09:50.440]  Yeah, I mean, and there's, I'm sure,
[09:50.440 --> 09:51.280]  and again, I don't want to-
[09:51.280 --> 09:52.500]  But with fans, like it's a fan event,
[09:52.500 --> 09:55.380]  so maybe more like Macworld back in the day.
[09:55.380 --> 09:56.900]  Yeah, that's probably a good analogy,
[09:56.900 --> 09:58.940]  but not surprised that people got it,
[09:58.940 --> 10:01.940]  but it's gonna happen, but yeah.
[10:01.940 --> 10:02.940]  I'm gonna go down swinging though, Jon.
[10:02.940 --> 10:04.300]  I'm gonna go down swinging.
[10:04.300 --> 10:08.100]  I would wear a mask in a convention center.
[10:08.100 --> 10:10.960]  Yeah, but yeah, well, it was just jam-packed.
[10:10.960 --> 10:13.780]  I was just, whatever, I'm happy, I'm safe,
[10:13.780 --> 10:16.060]  and I'll just keep trying to say it that way.
[10:16.060 --> 10:18.060]  All right, let me just take a break
[10:18.060 --> 10:19.100]  before we get into the show,
[10:19.100 --> 10:20.960]  and I'll knock one of the sponsors out of the way,
[10:20.960 --> 10:23.560]  and that's our good friends at Squarespace.
[10:23.560 --> 10:26.120]  Oh man, the all-in-one platform
[10:26.120 --> 10:29.620]  for building your brand and website online.
[10:29.620 --> 10:32.460]  You can stand out with your own beautiful website,
[10:32.460 --> 10:35.780]  customized to look just the way you like it with your brand,
[10:35.780 --> 10:39.360]  engage with your audience, and sell anything.
[10:39.360 --> 10:42.020]  Your products, the content you create, even your time.
[10:42.020 --> 10:44.900]  Squarespace truly has all of these things.
[10:44.900 --> 10:48.740]  They have great built-in analytics, absolutely great,
[10:48.740 --> 10:51.340]  just super clear, all the information you could need
[10:51.340 --> 10:54.460]  to know about who's coming to your website from where,
[10:54.460 --> 10:55.460]  what are they looking at,
[10:55.460 --> 10:58.560]  what parts of your website are high traffic,
[10:58.560 --> 10:59.680]  which parts are low traffic,
[10:59.680 --> 11:02.180]  which things are getting clicked, which things aren't.
[11:02.180 --> 11:04.060]  You can see it all in our analytics.
[11:04.060 --> 11:06.500]  Online stores, you need an online store.
[11:06.500 --> 11:08.320]  Squarespace has it all built in.
[11:08.320 --> 11:09.720]  You can sell your products.
[11:09.720 --> 11:12.460]  Whether you sell physical goods or digital products,
[11:12.460 --> 11:16.460]  Squarespace has everything you need to start selling online
[11:16.460 --> 11:18.780]  right up to the point where it takes the credit cards
[11:18.780 --> 11:20.640]  and stuff like that, everything.
[11:20.640 --> 11:22.400]  One more thing, email campaigns.
[11:22.400 --> 11:24.880]  You can stand out in any inbox
[11:24.880 --> 11:27.440]  with Squarespace's email campaigns.
[11:27.440 --> 11:29.140]  You can collect email subscribers,
[11:29.140 --> 11:30.680]  convert them into loyal customers.
[11:30.680 --> 11:32.180]  You start with an email template,
[11:32.180 --> 11:35.340]  customize it by applying your brand,
[11:35.340 --> 11:37.180]  site colors, logo, stuff like that,
[11:37.180 --> 11:38.960]  and you get analytics for that too
[11:38.960 --> 11:42.060]  to measure the impact of every email you send.
[11:42.060 --> 11:45.300]  Head to squarespace.com slash talk show,
[11:45.300 --> 11:46.940]  just slash talk show.
[11:46.940 --> 11:49.620]  You get a free trial at that URL, 30 days,
[11:49.620 --> 11:51.740]  no questions asked, no limits,
[11:51.740 --> 11:54.300]  no watermark on the site you set up.
[11:54.300 --> 11:56.460]  It's the real deal for 30 days,
[11:56.460 --> 11:58.180]  and then when you're ready to launch,
[11:58.180 --> 12:01.980]  use that same offer code, talk show, T-I-L-K-S-H-O-W,
[12:01.980 --> 12:04.380]  and by doing that, you will save 10%
[12:04.380 --> 12:05.460]  off your first purchase.
[12:05.460 --> 12:06.420]  My thanks to Squarespace.
[12:06.420 --> 12:08.980]  Go to squarespace.com slash talk show.
[12:08.980 --> 12:10.740]  Before we go back to WWDC,
[12:10.740 --> 12:13.340]  I've been writing about more or less prompted
[12:13.340 --> 12:15.700]  by last week's news here in the United States
[12:15.700 --> 12:17.900]  that the Supreme Court overturned Roe v. Wade,
[12:17.900 --> 12:21.740]  and as all of a sudden, in up to 26 states here,
[12:21.740 --> 12:25.940]  abortion is criminalized or severely limited,
[12:25.940 --> 12:29.660]  and a lot of women and other people
[12:29.660 --> 12:33.180]  are deeply concerned about the sort of tracking
[12:33.180 --> 12:35.340]  that can happen with their phones
[12:35.340 --> 12:38.100]  with regard to reproductive healthcare and stuff like that,
[12:38.100 --> 12:41.460]  and it's just general, something good to know in general,
[12:41.460 --> 12:42.700]  so I posted the other day.
[12:42.700 --> 12:44.700]  I reiterated something I've posted before,
[12:44.700 --> 12:48.220]  but never quite as the main thrust of a post to sort of,
[12:49.340 --> 12:52.300]  I mean, honestly, I hate, I know SEO can be a dirty word,
[12:52.300 --> 12:54.980]  but it's sort of my attempt at SEO at Daring Fireball.
[12:54.980 --> 12:58.180]  Like, let me write the post with the headline
[12:58.180 --> 13:01.140]  that people can, if they're searching for it
[13:01.140 --> 13:03.580]  on Google or DuckDuckGo or whatever else
[13:03.580 --> 13:04.940]  they search the web for,
[13:04.940 --> 13:06.100]  we'll hopefully lead them there.
[13:06.100 --> 13:10.220]  How to hard lock your iOS device,
[13:10.220 --> 13:14.820]  meaning how to put your,
[13:14.820 --> 13:16.260]  that's just, I'll just say iPhone,
[13:16.260 --> 13:18.260]  but everything here definitely applies to iPad
[13:18.260 --> 13:22.580]  and iPod touches, but that's a past tense product.
[13:22.580 --> 13:24.700]  But how do you put your iPhone into a state
[13:24.700 --> 13:28.900]  where Face ID or Touch ID no longer lets you into the phone
[13:28.900 --> 13:32.140]  and you have to put your passcode in?
[13:32.140 --> 13:34.340]  And one way everybody knows to do this
[13:34.340 --> 13:36.740]  is if your device restarts, like you turn it
[13:36.740 --> 13:38.780]  all the way off, turn it back on,
[13:38.780 --> 13:41.620]  or a lot of people run into it,
[13:41.620 --> 13:43.980]  maybe only run into it when they do a software update,
[13:43.980 --> 13:45.860]  which of course necessitates a restart.
[13:45.860 --> 13:48.740]  And everybody knows when you restart your phone,
[13:48.740 --> 13:50.420]  you have to enter the code.
[13:50.420 --> 13:53.260]  You can't just log in with Touch ID or Face ID.
[13:53.260 --> 13:55.900]  Why would you wanna do this to secure your device?
[13:55.900 --> 14:00.900]  Because, and bizarrely, coincidental with me writing this
[14:00.900 --> 14:05.100]  in the light of people's concerns about privacy,
[14:05.100 --> 14:08.700]  device privacy, health data privacy
[14:08.700 --> 14:11.780]  regarding the abortion ruling here in the US.
[14:11.780 --> 14:16.500]  In an unrelated thing, one of Trump's lawyers
[14:16.500 --> 14:18.820]  was out to eat one of these inserts,
[14:18.820 --> 14:22.660]  this John Eastman who promoted a bunch of, I think,
[14:22.660 --> 14:26.300]  which is why he was, why the FBI greeted him after dinner,
[14:26.300 --> 14:29.620]  a lot of illegal, legal advice to Trump
[14:29.620 --> 14:32.500]  about what he could do about the 2020 election.
[14:32.500 --> 14:37.340]  And they took his phone from his belt holster
[14:37.340 --> 14:40.580]  and required him, I guess, I think it was,
[14:40.580 --> 14:42.100]  they said it was an iPhone 12,
[14:42.100 --> 14:44.500]  so it would have been Face ID.
[14:44.500 --> 14:46.660]  And then they used Face ID to get into his phone
[14:46.660 --> 14:47.660]  and access his email.
[14:47.660 --> 14:51.860]  For whatever reason, this is legal in the United States,
[14:51.860 --> 14:54.900]  that a law enforcement with a warrant or whatever,
[14:54.900 --> 14:56.580]  I don't wanna get into the legal weeds on it,
[14:56.580 --> 14:59.220]  but legally, law enforcement, if they legally
[14:59.220 --> 15:01.180]  have the right to take your phone,
[15:01.180 --> 15:04.740]  can legally require you to use Face ID
[15:04.740 --> 15:06.340]  to unlock it biometrically.
[15:06.340 --> 15:10.300]  And they can, if it's a Touch ID device,
[15:10.300 --> 15:13.340]  can require you to provide your fingerprint.
[15:13.340 --> 15:14.180]  But-
[15:14.180 --> 15:15.340]  Even if they don't, like if you're incapacitated
[15:15.340 --> 15:17.380]  or you're, people on airplanes have fallen asleep
[15:17.380 --> 15:18.660]  and other people have unlocked their phones
[15:18.660 --> 15:19.500]  with their fingers,
[15:19.500 --> 15:21.100]  so it's always something to be careful about.
[15:21.100 --> 15:22.980]  Yeah, I've never, I've seen some stories about,
[15:22.980 --> 15:25.620]  what I've seen is one of those things,
[15:25.620 --> 15:27.220]  like when they first announced Face ID
[15:27.220 --> 15:29.780]  and people were like, well, what if blank, blank?
[15:29.780 --> 15:32.540]  And one thing that, right as soon as they announced it,
[15:32.540 --> 15:34.620]  where people were like, what if you fall asleep
[15:34.620 --> 15:36.740]  and your kids know you have Face ID
[15:36.740 --> 15:38.700]  and they come up and try to unlock your phone
[15:38.700 --> 15:39.740]  while you're sleeping?
[15:39.740 --> 15:40.580]  And I've heard-
[15:40.580 --> 15:41.500]  It's harder because you need your eyes,
[15:41.500 --> 15:43.380]  but Touch ID works every time.
[15:43.380 --> 15:47.020]  Yeah, Touch ID works, but if you turn off attention,
[15:47.020 --> 15:49.940]  I think it might work with your eyes closed.
[15:49.940 --> 15:52.820]  So it's one reason that if,
[15:52.820 --> 15:55.020]  unless you have some sort of accessibility need
[15:55.020 --> 15:56.560]  to turn off the attention feature,
[15:56.560 --> 15:57.400]  you probably-
[15:57.400 --> 15:59.140]  Or those damn kids stick googly eyes on you
[15:59.140 --> 15:59.980]  before they do it.
[15:59.980 --> 16:01.420]  I used to turn off the attention feature
[16:01.420 --> 16:03.900]  because for a while I had sunglasses that didn't work
[16:03.900 --> 16:06.520]  with Face ID and turning off the attention feature
[16:06.520 --> 16:09.420]  would do it and I only have one kid and I trust him.
[16:09.420 --> 16:12.980]  But anyway, again, it is an interesting legal distinction,
[16:12.980 --> 16:17.700]  but also though, they are not allowed to compel you
[16:17.700 --> 16:20.940]  to provide a password or passcode or passphrase,
[16:20.940 --> 16:22.260]  whatever you want to do,
[16:22.260 --> 16:24.260]  which would be tricky anyway, right?
[16:24.260 --> 16:25.540]  Because if you imagine it,
[16:25.540 --> 16:28.940]  let's say they take your phone and you have Face ID
[16:28.940 --> 16:31.280]  and you don't want to let them in,
[16:31.280 --> 16:32.600]  you can sit there and struggle,
[16:32.600 --> 16:35.180]  but they can physically hold you down
[16:35.180 --> 16:36.700]  and hold the phone in front of you.
[16:36.700 --> 16:38.000]  You can keep your eyes closed,
[16:38.000 --> 16:40.020]  but eventually you're going to open them.
[16:40.020 --> 16:41.420]  It's whereas-
[16:41.420 --> 16:42.620]  Hashtag not legal advice,
[16:42.620 --> 16:44.740]  but they have physical rights to you.
[16:44.740 --> 16:46.700]  They can fingerprint you, they can photograph you,
[16:46.700 --> 16:49.460]  but you have the right not, again, self-incrimination,
[16:49.460 --> 16:50.900]  so they can't force you to say anything.
[16:50.900 --> 16:54.640]  Right, stuff that's in your brain you have the right to keep
[16:54.640 --> 16:56.860]  and things on your physical body you don't.
[16:56.860 --> 17:00.060]  I think that's, as a legal argument or a moral,
[17:00.060 --> 17:01.060]  what should the law be?
[17:01.060 --> 17:02.680]  I think that's a whole separate discussion,
[17:02.680 --> 17:06.300]  and I'm such a civil libertarian
[17:06.300 --> 17:08.820]  that I would come down on the side of
[17:08.820 --> 17:10.980]  that your biometrics should be
[17:10.980 --> 17:13.060]  as protected as your passwords.
[17:13.060 --> 17:14.900]  I believe your phone should have the same protection
[17:14.900 --> 17:17.040]  as the spouse because they contain everything about you
[17:17.040 --> 17:17.880]  at this point.
[17:17.880 --> 17:19.980]  They should have full spousal non-incrimination protection,
[17:19.980 --> 17:21.780]  but that's how I roll, John.
[17:21.780 --> 17:26.100]  But regardless, that is the way the law is here in the US,
[17:26.100 --> 17:27.360]  so it's worth knowing.
[17:27.360 --> 17:30.300]  And even if the law were different,
[17:30.300 --> 17:34.140]  you could certainly argue that if you thought your phone
[17:34.140 --> 17:36.060]  were going to be in the hands of someone
[17:36.060 --> 17:37.840]  you don't want it to be in,
[17:37.840 --> 17:38.780]  you still might want it,
[17:38.780 --> 17:40.900]  even if the law protected your biometrics,
[17:40.900 --> 17:44.740]  you still might want to put it in a hard lock situation
[17:44.740 --> 17:47.960]  just to prevent somebody who, an adversary,
[17:47.960 --> 17:50.220]  whether it's a police officer or whether it's-
[17:50.220 --> 17:51.240]  You're crossing a border.
[17:51.240 --> 17:54.980]  Right, or it could be a thief, could be anybody.
[17:54.980 --> 17:57.740]  But in theory, I think it makes sense
[17:57.740 --> 18:00.540]  that an iPhone that is in a state
[18:00.540 --> 18:04.160]  that requires your passkey is more secure
[18:04.160 --> 18:08.740]  than one that requires your passkey or face ID or touch ID.
[18:08.740 --> 18:09.660]  So how do you do it?
[18:09.660 --> 18:12.380]  It's really easy on a modern iPhone.
[18:12.380 --> 18:14.780]  You just squeeze the power button,
[18:14.780 --> 18:16.120]  they call it the side button,
[18:16.120 --> 18:18.860]  with either of the volume buttons,
[18:18.860 --> 18:21.400]  or I guess both if you squeeze all three.
[18:21.400 --> 18:22.240]  I actually didn't try that.
[18:22.240 --> 18:23.580]  Yep, that'll work too.
[18:25.320 --> 18:27.580]  So just remember that.
[18:27.580 --> 18:31.100]  Squeeze the phone, side button, either volume button,
[18:31.100 --> 18:33.260]  hold it, it takes, I said two seconds,
[18:33.260 --> 18:35.160]  I actually think it's a little less than two seconds,
[18:35.160 --> 18:36.860]  but call it two seconds.
[18:36.860 --> 18:39.260]  You get a little bit of haptic feedback.
[18:39.260 --> 18:41.720]  If, this is a weird thing, I didn't note this,
[18:41.720 --> 18:42.760]  I have a follow-up to write,
[18:42.760 --> 18:45.620]  but you don't get, a couple of people have said,
[18:45.620 --> 18:49.380]  I don't get any haptic feedback, do you know about this?
[18:49.380 --> 18:51.220]  No, I get it all the time, I hadn't even thought about it.
[18:51.220 --> 18:55.380]  So the haptic feedback for when you hard-lock your phone
[18:55.380 --> 19:00.380]  only occurs if you have, in settings,
[19:01.920 --> 19:06.100]  sound in haptics, here is where I'm going through this,
[19:06.100 --> 19:10.320]  sound in haptics, vibrate on ring.
[19:11.780 --> 19:14.520]  Not, the vibrate on silent doesn't,
[19:14.520 --> 19:17.420]  the switch doesn't matter, but it's the top thing.
[19:17.420 --> 19:21.100]  I would have never guessed that vibrate on ring
[19:21.100 --> 19:24.380]  would affect the haptic feedback for locking your phone.
[19:24.380 --> 19:26.820]  Not in a million years, but I tracked it down
[19:26.820 --> 19:29.900]  with a couple of readers who were like,
[19:29.900 --> 19:31.540]  I don't know why I'm getting haptics.
[19:31.540 --> 19:32.820]  That seems very strange to me,
[19:32.820 --> 19:34.860]  I think you should always get haptic feedback
[19:34.860 --> 19:37.140]  for this, did this thing.
[19:37.140 --> 19:39.700]  One of the things that is really neat about this feature,
[19:39.700 --> 19:43.220]  and I know for a fact it is deliberately designed this way,
[19:43.220 --> 19:46.580]  is that you can do it while your phone is in your pocket
[19:46.580 --> 19:48.780]  without looking at it, or your purse.
[19:48.780 --> 19:50.340]  And any of those things.
[19:50.340 --> 19:53.660]  Any of those things, if it's just in your hand
[19:53.660 --> 19:56.100]  down by your leg, you can do it
[19:56.100 --> 19:57.540]  without looking at the screen,
[19:57.540 --> 19:59.020]  so you don't draw attention to it.
[19:59.020 --> 20:02.960]  And any, let's say, let's just say cop or a TSA agent
[20:02.960 --> 20:05.320]  who might consider it suspicious
[20:05.320 --> 20:07.360]  that you're doing something at your phone
[20:07.360 --> 20:09.340]  right before you're supposed to hand it over
[20:09.340 --> 20:12.640]  or something like that, you can do it very surreptitiously.
[20:12.640 --> 20:14.900]  And then at that point, it is in a state
[20:14.900 --> 20:16.500]  where it requires your passcode.
[20:16.500 --> 20:19.120]  For the nerdy reference, most of us exist in Springboard,
[20:19.120 --> 20:21.120]  where it just has all of our normal applications.
[20:21.120 --> 20:23.500]  And when you do that, when you squeeze it like that,
[20:23.500 --> 20:26.220]  it basically throws away the hardware encryption keys
[20:26.220 --> 20:27.860]  and puts you back into pre-board.
[20:27.860 --> 20:29.740]  And there is literally, that's why phone numbers
[20:29.740 --> 20:31.460]  won't show where the person calling is.
[20:31.460 --> 20:33.460]  It has no access to any of your information
[20:33.460 --> 20:35.820]  until you put in the passcode or password,
[20:35.820 --> 20:38.140]  which gets you out of pre-board
[20:38.140 --> 20:40.620]  and back into the springboard, the unlocked springboard world.
[20:40.620 --> 20:44.060]  Yeah, and I know you had a video like two years ago,
[20:44.060 --> 20:46.580]  something like that, where you went through this.
[20:46.580 --> 20:48.620]  So that's one of the reasons I'm so happy
[20:48.620 --> 20:49.460]  to have you on the show,
[20:49.460 --> 20:51.900]  because I know you know more about this than I do, I think.
[20:51.900 --> 20:53.280]  But every time I post about this,
[20:53.280 --> 20:54.520]  I get lots and lots of comments like,
[20:54.520 --> 20:56.200]  hey, I didn't know this, good to know.
[20:56.200 --> 21:00.880]  And it's exactly why I've posted this multiple times,
[21:00.880 --> 21:02.340]  why I'm talking about it now.
[21:02.340 --> 21:05.620]  It is, like I wrote, I think I wrote the other day,
[21:05.620 --> 21:07.120]  even if you're the sort of person
[21:07.120 --> 21:11.480]  who doesn't typically share iPhone tips and tricks
[21:11.480 --> 21:13.020]  with your extended friends and family.
[21:13.020 --> 21:16.460]  This is one you should make an exception for.
[21:16.460 --> 21:18.580]  And share it with your friends and family
[21:18.580 --> 21:20.140]  as wide as you can.
[21:20.140 --> 21:23.760]  A, the police can force you to use Face ID and Touch ID,
[21:23.760 --> 21:25.980]  but cannot force you to reveal your passcode.
[21:25.980 --> 21:27.600]  And B, if you know this tip,
[21:27.600 --> 21:32.220]  you can very quickly surreptitiously put your iPhone
[21:32.220 --> 21:34.080]  into a requires a passcode state.
[21:34.080 --> 21:36.020]  And even if it's not the police, if it's a bad actor
[21:36.020 --> 21:37.260]  and you know they're gonna take your phone
[21:37.260 --> 21:38.860]  and try to get into your accounts or something,
[21:38.860 --> 21:40.400]  you can still lock it, it's great.
[21:40.400 --> 21:42.720]  Or you're going through a market, a public market
[21:42.720 --> 21:44.140]  where you know there's a lot of pickpockets
[21:44.140 --> 21:46.900]  and things around, it protects you everywhere.
[21:46.900 --> 21:48.540]  And I've heard stories about that too,
[21:48.540 --> 21:50.460]  like where a pickpocket will take a phone
[21:50.460 --> 21:51.940]  and then it's like you're in a moment,
[21:51.940 --> 21:53.420]  you don't even know it happened,
[21:53.420 --> 21:55.660]  and they hold it up to your face or something like that.
[21:55.660 --> 21:57.060]  I've heard about that.
[21:57.060 --> 22:00.900]  And it does seem, and I don't know how apocryphal
[22:00.900 --> 22:02.140]  some of those stories might be,
[22:02.140 --> 22:05.620]  but it passes the sniff test, right?
[22:05.620 --> 22:07.180]  It seems like something that could happen.
[22:07.180 --> 22:08.900]  And it would be easier than getting you
[22:08.900 --> 22:12.100]  to somehow getting your index finger on a Touch ID button.
[22:12.100 --> 22:13.700]  So it's good to know.
[22:13.700 --> 22:15.620]  It's way easier than getting a long passcode
[22:15.620 --> 22:17.740]  or alphanumeric password put into the device.
[22:17.740 --> 22:20.180]  Yeah, if you have a Touch ID iPhone,
[22:20.180 --> 22:22.600]  and I think it's all a little different,
[22:22.600 --> 22:25.900]  I think that you can, I think with an iPhone SE,
[22:25.900 --> 22:28.480]  the current SE, you could just hold the side button
[22:28.480 --> 22:30.500]  for a couple seconds, that's all you have to do.
[22:30.500 --> 22:34.620]  Anything that brings up the slide to power down screen
[22:34.620 --> 22:38.580]  will do it, however you get to there on your iPhone,
[22:38.580 --> 22:40.100]  I believe, will do it.
[22:40.100 --> 22:43.860]  There's also an older shortcut where you quickly
[22:43.860 --> 22:46.540]  tap the side button five times.
[22:46.540 --> 22:47.380]  Yes.
[22:47.380 --> 22:49.460]  Which used to immediately,
[22:49.460 --> 22:54.420]  they've, I know the word quietly gets overused,
[22:54.420 --> 22:55.800]  not just in stories about Apple,
[22:55.800 --> 22:58.340]  but in the media overall, right?
[22:58.340 --> 23:00.680]  Anything that doesn't come with a press relief
[23:00.680 --> 23:05.680]  can somehow get described as this company quietly did mic.
[23:05.900 --> 23:08.240]  But it seems like Apple quietly changed
[23:08.240 --> 23:09.860]  the way that feature worked,
[23:09.860 --> 23:13.340]  where years ago, at some point,
[23:13.340 --> 23:17.580]  if you did the, quickly pressed the power button,
[23:17.580 --> 23:20.060]  side button, whatever you wanna call it, five times,
[23:20.060 --> 23:22.740]  it would immediately start playing like a,
[23:22.740 --> 23:23.680]  for lack of a better term,
[23:23.680 --> 23:27.180]  like a klaxon or a siren from your iPhone.
[23:27.180 --> 23:28.500]  Yeah, it goes into SOS mode.
[23:28.500 --> 23:31.100]  And within like three or four seconds,
[23:31.100 --> 23:34.300]  would place an emergency SOS phone call,
[23:34.300 --> 23:36.180]  which in the U.S. is 911.
[23:36.180 --> 23:37.580]  What do you guys have in Canada?
[23:37.580 --> 23:38.420]  Same thing.
[23:38.420 --> 23:39.240]  911, okay.
[23:39.240 --> 23:41.000]  But I know around the world, the numbers are different,
[23:41.000 --> 23:43.020]  but it'll, wherever you are,
[23:43.020 --> 23:45.340]  whatever locale you're in, it will immediately do that.
[23:45.340 --> 23:47.020]  It doesn't seem to do that anymore.
[23:47.020 --> 23:48.860]  Like when I press mine five times,
[23:48.860 --> 23:53.800]  it just does the same thing that holding the side button
[23:53.800 --> 23:55.820]  and the volume button for a few seconds does.
[23:55.820 --> 23:57.980]  It just puts me into slide to power off.
[23:57.980 --> 23:58.900]  Now you just squeeze,
[23:58.900 --> 24:00.260]  if you keep holding the squeeze down
[24:00.260 --> 24:01.220]  for an extended period of time,
[24:01.220 --> 24:02.980]  it'll do the alarm and then the 911.
[24:02.980 --> 24:06.200]  Yeah, and it doesn't place the phone call automatically.
[24:06.200 --> 24:08.020]  And I just did it right here.
[24:08.020 --> 24:10.700]  And as you can tell from my audio,
[24:10.700 --> 24:12.480]  it does not play a siren sound.
[24:12.480 --> 24:15.300]  I feel like maybe that was deemed too,
[24:15.300 --> 24:16.620]  people did it accidentally.
[24:16.620 --> 24:18.560]  And all of a sudden, all of a sudden their phone
[24:18.560 --> 24:20.820]  is making a terrible siren noise
[24:20.820 --> 24:23.300]  and they're talking to someone on 911
[24:23.300 --> 24:25.880]  and they didn't mean to do it.
[24:25.880 --> 24:26.720]  I think this is-
[24:26.720 --> 24:27.540]  I heard about people in cars,
[24:27.540 --> 24:29.460]  they had it in the car holder and it would slip down
[24:29.460 --> 24:30.740]  and it'd suddenly be pressing,
[24:30.740 --> 24:31.580]  it was in there in the car
[24:31.580 --> 24:32.460]  and it scares the hell out of them.
[24:32.460 --> 24:35.140]  Yeah, and if it jiggles five times,
[24:35.140 --> 24:37.380]  if it's slightly loose enough to push the button.
[24:37.380 --> 24:40.140]  But anyway, it is easy to make the emergency SOS.
[24:40.140 --> 24:41.860]  The other good thing about knowing this tip,
[24:41.860 --> 24:42.740]  I didn't write about this,
[24:42.740 --> 24:45.060]  but the other good thing about knowing this tip
[24:45.060 --> 24:47.040]  of how to put your phone into this,
[24:48.740 --> 24:50.820]  there's four options, slide to power off,
[24:50.820 --> 24:54.540]  medical ID, emergency SOS, and then cancel.
[24:54.540 --> 24:57.660]  But it's also good to know if you ever encounter somebody
[24:57.660 --> 25:00.900]  who passes out or takes a bad fall, needs help,
[25:00.900 --> 25:05.580]  and they have an iPhone, do the same thing on their iPhone
[25:05.580 --> 25:06.940]  and you will get to the screen
[25:06.940 --> 25:10.700]  and you can slide the medical ID thing over
[25:10.700 --> 25:14.160]  and if they've set up a medical ID,
[25:14.160 --> 25:16.420]  here's what mine says, it says I'm 49 years old,
[25:16.420 --> 25:19.220]  I'm an organ donor, I have some drug allergies,
[25:19.220 --> 25:20.840]  my blood type, my height and weight,
[25:20.840 --> 25:23.620]  I don't know why they would need that, but it's there.
[25:23.620 --> 25:26.620]  And it has also, but it also has
[25:26.620 --> 25:28.900]  the emergency contact of my wife.
[25:28.900 --> 25:30.100]  So it's a good thing to know
[25:30.100 --> 25:32.580]  if you ever encounter a complete stranger
[25:32.580 --> 25:33.840]  or somebody who's not a stranger,
[25:33.840 --> 25:37.540]  a friend or a family member and they're incapacitated,
[25:37.540 --> 25:39.960]  they have an iPhone, do the same thing.
[25:39.960 --> 25:41.980]  Press and hold the side button and a volume button
[25:41.980 --> 25:46.200]  on their iPhone and you'll get access to their medical ID.
[25:46.200 --> 25:48.260]  Everybody who works in emergency services,
[25:48.260 --> 25:51.100]  of course, knows this, so like paramedics
[25:51.100 --> 25:53.940]  who come from an ambulance will know to do this,
[25:53.940 --> 25:56.640]  but before they get there, you could do it yourself.
[25:56.640 --> 25:57.480]  So it's good to know.
[25:57.480 --> 25:58.520]  All right, so far, so good.
[25:58.520 --> 26:01.780]  Here's some questions that have come up since I wrote this.
[26:01.780 --> 26:03.540]  Here's the first one and it's a big one.
[26:03.540 --> 26:05.540]  What's the equivalent for a MacBook?
[26:07.020 --> 26:08.160]  Yeah, you just hold down the power button
[26:08.160 --> 26:10.060]  for five seconds and kill it and then it has to,
[26:10.060 --> 26:14.020]  it goes into its equivalent of safe mode or private mode.
[26:14.020 --> 26:15.700]  Right, that's the short answer
[26:15.700 --> 26:17.260]  and the best thing I can come up with
[26:17.260 --> 26:19.020]  is if you have a touch ID,
[26:19.020 --> 26:22.200]  if your MacBook doesn't have touch ID, you're good
[26:22.200 --> 26:24.660]  because then as long as the lid is closed,
[26:24.660 --> 26:27.780]  it requires a password, so you're in a,
[26:27.780 --> 26:29.980]  requires a password state.
[26:29.980 --> 26:33.900]  But if your MacBook has touch ID, which at this point,
[26:33.900 --> 26:35.360]  just about anybody who bought one
[26:35.360 --> 26:38.660]  in the last couple of years does,
[26:38.660 --> 26:40.500]  how do you put your Mac into a state
[26:40.500 --> 26:43.700]  where it requires the password, passphrase,
[26:43.700 --> 26:48.700]  whatever you wanna call it, and doesn't unlock with touch ID?
[26:48.700 --> 26:52.540]  If you press and hold that, the touch ID slash power button
[26:52.540 --> 26:56.540]  for what, I guess it is longer, it's cause it's so.
[26:56.540 --> 26:57.780]  I think it's five seconds.
[26:57.780 --> 27:00.940]  Yeah, but it's so detrimental to the overall experience.
[27:00.940 --> 27:02.820]  You definitely don't want it to happen by accident,
[27:02.820 --> 27:06.660]  but it affects, it shuts down the whole system.
[27:06.660 --> 27:07.500]  Yeah.
[27:08.360 --> 27:10.880]  Which is very disruptive, right?
[27:10.880 --> 27:13.520]  So like part of my advice is that you should do this
[27:13.520 --> 27:16.940]  with your phone, like just going through airport security
[27:16.940 --> 27:21.940]  or like here in the U.S. now, I know Major League Baseball
[27:22.820 --> 27:25.180]  requires it, I think football does too,
[27:25.180 --> 27:27.220]  but lots of sporting events and concerts
[27:27.220 --> 27:30.780]  require you to go through a magnetometer
[27:30.780 --> 27:32.560]  and you have to take your keys
[27:32.560 --> 27:34.420]  and anything metal out of your pockets,
[27:34.420 --> 27:35.500]  you have to take your phone out.
[27:35.500 --> 27:37.380]  When you get separated from your phone,
[27:37.380 --> 27:40.700]  even if it's just the rent-a-cops at a stadium,
[27:40.700 --> 27:43.800]  I do this with my phone, you should do it with your phone.
[27:43.800 --> 27:44.640]  Same.
[27:44.640 --> 27:46.180]  But if you're going through the airport
[27:46.180 --> 27:49.300]  and you want to do this with all of your devices
[27:49.300 --> 27:53.300]  you've packed, shutting down your Mac completely
[27:53.300 --> 27:54.660]  is a pain in the ass.
[27:54.660 --> 27:58.700]  And as fast as even Apple Silicon MacBooks are now,
[27:58.700 --> 28:01.860]  and they really are fast and they do boot up quicker,
[28:01.860 --> 28:04.620]  the part that isn't quick and can't be made quicker
[28:04.620 --> 28:08.180]  is restoring all of the state, right?
[28:08.180 --> 28:10.100]  I've got tons of browser tabs open,
[28:10.100 --> 28:14.100]  I've got easily a dozen apps open at a time.
[28:14.100 --> 28:16.260]  And anybody who's listening to the show
[28:16.260 --> 28:17.140]  knows what I'm talking about.
[28:17.140 --> 28:18.900]  If you have to restart your Mac
[28:18.900 --> 28:23.580]  to install a software update or whatever reason, it's great.
[28:23.580 --> 28:26.940]  It is one of the great additions to macOS
[28:26.940 --> 28:30.420]  over the last decade, the way that state restoration
[28:30.420 --> 28:33.260]  is so seamless system-wide and it's,
[28:34.180 --> 28:36.420]  because I remember when it wasn't like this
[28:36.420 --> 28:40.140]  and it was such a, you had to manually restore everything.
[28:40.140 --> 28:43.380]  If you wanted to shut down your Mac, you had to,
[28:43.380 --> 28:44.540]  nothing came back.
[28:44.540 --> 28:46.460]  Every time you came, you logged back in,
[28:46.460 --> 28:48.700]  you had nothing open, none of your documents,
[28:48.700 --> 28:51.140]  none of your browser tabs, all of this stuff for me,
[28:51.140 --> 28:52.900]  every app I use on a regular basis
[28:52.900 --> 28:54.940]  just comes back to where it was,
[28:54.940 --> 28:58.060]  even with untitled, unsaved changes to documents,
[28:58.060 --> 29:00.060]  which is great, right?
[29:00.060 --> 29:03.500]  Like the old way of doing it 10 or more years ago
[29:03.500 --> 29:05.500]  was you'd go to restart and be like,
[29:05.500 --> 29:07.300]  oh, do you wanna save the changes to this?
[29:07.300 --> 29:09.860]  Save, don't save, whatever, and it's like, ah.
[29:09.860 --> 29:12.100]  And then the other, every single app you have open,
[29:12.100 --> 29:13.700]  anything unsaved is asking you,
[29:13.700 --> 29:15.660]  what do you wanna do with these unsaved changes?
[29:15.660 --> 29:17.420]  Took minutes.
[29:17.420 --> 29:18.460]  Going through the airport,
[29:18.460 --> 29:22.580]  I don't want to go through security with my Mac shut down,
[29:22.580 --> 29:25.100]  and then if I have half an hour to burn
[29:25.100 --> 29:27.140]  before boarding the flight
[29:27.140 --> 29:28.820]  and I wanna do something on my Mac,
[29:28.820 --> 29:31.300]  I don't wanna wait five minutes for my Mac
[29:31.300 --> 29:34.060]  to fully come back.
[29:34.060 --> 29:36.020]  But I don't know what the better answer is.
[29:36.020 --> 29:38.420]  So I do it anyway, I put it in that state anyway,
[29:38.420 --> 29:41.420]  and with Apple Silicon Macs, it comes back way faster.
[29:41.420 --> 29:43.500]  I'm not like mutual friend Alex Lindsay level.
[29:43.500 --> 29:44.700]  I don't do the kind of work he does.
[29:44.700 --> 29:48.060]  So if his devices leave his physical eyeline,
[29:48.060 --> 29:50.860]  he just abandons them and figures they're compromised.
[29:50.860 --> 29:52.580]  Like someone has taken them to do something to them
[29:52.580 --> 29:54.060]  and he can never touch them again.
[29:54.060 --> 29:55.140]  I'm not that extreme.
[29:55.140 --> 29:57.900]  I don't have the same clients, so I don't have to be.
[29:57.900 --> 29:59.460]  But I...
[29:59.460 --> 30:00.620]  That is hardcore,
[30:00.620 --> 30:02.820]  but I don't necessarily blame them, right?
[30:02.820 --> 30:03.660]  Yes.
[30:04.860 --> 30:06.580]  Yes, well, because you don't know what they put on it
[30:06.580 --> 30:07.420]  or what they've done to it
[30:07.420 --> 30:09.020]  or if they've pulled everything off the phone at that point,
[30:09.020 --> 30:11.700]  and you can't ever verify that it's safe again, so.
[30:11.700 --> 30:13.900]  I get it, I'm just like in the middle of paranoia.
[30:13.900 --> 30:16.300]  But what did, so what does he, I mean,
[30:16.300 --> 30:19.060]  but to what degree does he mean lose sight?
[30:19.060 --> 30:22.420]  Like the stuff does have to pass through the X-ray machine.
[30:22.420 --> 30:24.940]  Oh, if someone, if like, if a state agent picks it up
[30:24.940 --> 30:26.180]  and walks out of the room with it.
[30:26.180 --> 30:28.700]  Yeah, well, that's never happened to me,
[30:28.700 --> 30:30.820]  but I would find it very suspicious if they did.
[30:30.820 --> 30:33.060]  I do keep an eye on the conveyor belt
[30:33.060 --> 30:34.740]  coming out of the X-ray machine.
[30:34.740 --> 30:37.780]  And I'm not worried about the TSA agent so much
[30:37.780 --> 30:39.620]  as somebody just in a, you know,
[30:39.620 --> 30:41.580]  either a thief or somebody accidentally
[30:41.580 --> 30:43.180]  just taking the wrong tray or whatever.
[30:43.180 --> 30:47.260]  It always, it's always very, even with TSA pre-check,
[30:47.260 --> 30:50.500]  it is so chaotic and I'm always super stressed.
[30:50.500 --> 30:51.540]  Like, what do I need?
[30:51.540 --> 30:52.540]  What do I need?
[30:52.540 --> 30:53.380]  My belt?
[30:53.380 --> 30:54.380]  What the hell's missing?
[30:54.380 --> 30:55.220]  What am I missing?
[30:55.220 --> 30:56.580]  I find it very stressful, but.
[30:56.580 --> 30:58.060]  And like, we're not the kind of people
[30:58.060 --> 30:59.300]  that a state agent would burn
[30:59.300 --> 31:01.980]  a million dollar undisclosed zero-day vulnerability on,
[31:01.980 --> 31:03.260]  so I don't worry that much.
[31:03.260 --> 31:06.220]  The other thing I can think of for a MacBook
[31:06.220 --> 31:11.220]  is, with Touch ID, is if you use the wrong finger,
[31:12.860 --> 31:14.740]  I don't know, I'm not quite sure how many times,
[31:14.740 --> 31:16.380]  I think it's like three or four.
[31:16.380 --> 31:19.460]  If you do three or four fingerprints that don't take,
[31:19.460 --> 31:21.380]  it puts it in that state.
[31:21.380 --> 31:22.220]  Yeah.
[31:22.220 --> 31:25.060]  And I think that's, I have never done this,
[31:25.060 --> 31:27.420]  but I think that's what I'm going to do henceforth.
[31:27.420 --> 31:31.820]  But that's the sort of thing that, unlike the phone,
[31:31.820 --> 31:34.340]  you can't do it at the last second, right?
[31:34.340 --> 31:37.500]  Like, you're in the TSA security line,
[31:37.500 --> 31:41.660]  you're snaking through it, you've got a laptop bag
[31:41.660 --> 31:43.460]  on your shoulder or your backpack,
[31:43.460 --> 31:46.820]  you've got a carry-on, you're hauling behind you,
[31:46.820 --> 31:49.580]  you've got your wallet and keys
[31:49.580 --> 31:51.100]  and your phone in your pocket,
[31:51.100 --> 31:53.100]  and you've got to do something with them.
[31:53.100 --> 31:54.740]  And for me, it's always like,
[31:54.740 --> 31:56.180]  as I'm snaking through the line, it's like,
[31:56.180 --> 31:57.700]  well, I don't want to put all this stuff
[31:57.700 --> 32:00.020]  into one of their little trays.
[32:00.020 --> 32:03.580]  I take my wallet and stuff out of my pockets,
[32:03.580 --> 32:06.700]  put them in my laptop bag into little pockets
[32:06.700 --> 32:08.580]  inside the bag so that all I have to do
[32:08.580 --> 32:10.380]  is remember to take the bag
[32:10.380 --> 32:12.180]  and all my pocket stuff is in the bag.
[32:12.180 --> 32:14.740]  That's a good time to squeeze your phone,
[32:14.740 --> 32:16.180]  to put it in a locked state,
[32:16.180 --> 32:18.100]  put the phone somewhere where, you know,
[32:18.100 --> 32:19.900]  I like to put it in my bag again
[32:19.900 --> 32:21.660]  so that it's not just sitting out in a tray.
[32:21.660 --> 32:22.860]  Yeah, I do the same thing.
[32:22.860 --> 32:25.260]  One last thing that I have to remember is a separate,
[32:25.260 --> 32:27.420]  pick it up on the other side of the X-ray machine.
[32:27.420 --> 32:30.500]  But doing it with your Mac seems inconvenient, right?
[32:30.500 --> 32:31.340]  What are you going to do?
[32:31.340 --> 32:33.900]  Make your entire MacBook out of your bag,
[32:33.900 --> 32:38.340]  in line, open it, and start squeezing the power button?
[32:38.340 --> 32:39.180]  I mean, you could do it.
[32:39.180 --> 32:41.020]  It's not that, it just-
[32:41.020 --> 32:41.860]  Just before I leave,
[32:41.860 --> 32:42.700]  because I'm not going to use,
[32:42.700 --> 32:45.060]  from the time I leave my home to the time I go through security,
[32:45.060 --> 32:45.980]  I'm not going to use it.
[32:45.980 --> 32:49.300]  And also ever since 2016, when they got the T2 chip,
[32:49.300 --> 32:51.660]  which was just an S, an Apple, sorry, the T1 chip,
[32:51.660 --> 32:53.500]  that's an Apple S2 watch chip.
[32:53.500 --> 32:57.300]  And then the later version, the T2 chip was an A10,
[32:57.300 --> 32:58.820]  you know, and now they have whatever they like,
[32:58.820 --> 33:00.540]  the M1 is the equivalent to the Secure Enclave
[33:00.540 --> 33:02.900]  on the A14, and M2 is the one on the A15.
[33:02.900 --> 33:05.540]  They work the same way as iOS devices.
[33:05.540 --> 33:06.420]  So as long as you can get them
[33:06.420 --> 33:07.740]  to throw out those hardware keys,
[33:07.740 --> 33:08.660]  like your idea is even better.
[33:08.660 --> 33:09.980]  Just like do the touch ID wrong.
[33:09.980 --> 33:11.620]  It's much less destructive.
[33:11.620 --> 33:12.460]  You're safe.
[33:12.460 --> 33:13.380]  Like there's no way to get into that.
[33:13.380 --> 33:15.060]  Right, that's what I think I'm going to do.
[33:15.060 --> 33:18.980]  But what you said is what I'm going to add it to my,
[33:18.980 --> 33:20.860]  before I leave home, checklist.
[33:20.860 --> 33:23.580]  You know, I keep, I just call it my pack list
[33:23.580 --> 33:27.540]  in Apple Notes with everything I want to pack for any trip.
[33:27.540 --> 33:31.940]  And then at the bottom is like my things to remember
[33:31.940 --> 33:33.660]  right as I go out the door, you know.
[33:33.660 --> 33:35.380]  And I'm going to add that to it,
[33:35.380 --> 33:38.620]  which is to put my MacBook into this state.
[33:38.620 --> 33:40.820]  And I think that's what I'm going to start doing
[33:40.820 --> 33:44.540]  until further notice is just use like a pinky finger
[33:44.540 --> 33:45.380]  that I know-
[33:45.380 --> 33:46.220]  Just give it the finger, the right finger.
[33:46.220 --> 33:47.540]  Yeah, give it the finger.
[33:47.540 --> 33:49.180]  Like I think it's three or four times,
[33:49.180 --> 33:51.380]  and then it requires a passcode.
[33:51.380 --> 33:53.700]  Shut the lid, put it in my backpack,
[33:53.700 --> 33:56.460]  and then don't enter the passcode
[33:56.460 --> 33:59.420]  until I'm through security and back to that state.
[33:59.420 --> 34:02.140]  But then I don't have to worry about rebooting
[34:02.140 --> 34:04.100]  the entire machine and restoring the whole state.
[34:04.100 --> 34:05.060]  Yeah, yeah, that's way better.
[34:05.060 --> 34:07.540]  But it does, unlike the phone, it is something
[34:07.540 --> 34:10.380]  I think is best done before you leave for the airport
[34:10.380 --> 34:13.860]  rather than doing it right as you approach the magnetometer
[34:13.860 --> 34:14.900]  in the security line.
[34:14.900 --> 34:17.780]  I do think Apple should consider this and come,
[34:17.780 --> 34:19.460]  I don't know what the solution would be
[34:19.460 --> 34:24.380]  because I know that it seems like that touch ID
[34:24.380 --> 34:27.860]  slash power button is really the only button
[34:27.860 --> 34:28.700]  that could be used.
[34:28.700 --> 34:29.940]  Yeah, there's no external button that you,
[34:29.940 --> 34:31.220]  like once it's shut, you can't do anything.
[34:31.220 --> 34:35.940]  Right, and I get it that the just press and hold it
[34:35.940 --> 34:39.660]  for five seconds and it will, I forget what happens.
[34:39.660 --> 34:41.980]  Does it restart automatically or just shut down?
[34:41.980 --> 34:42.820]  I think it just shuts down.
[34:42.820 --> 34:43.660]  It just shuts down.
[34:43.660 --> 34:44.480]  Right, and then you-
[34:44.480 --> 34:45.320]  Maybe when you do it on your iPhone,
[34:45.320 --> 34:47.780]  they should propagate that to all your iCloud devices.
[34:47.780 --> 34:49.500]  That's an idea that people have had,
[34:49.500 --> 34:52.020]  and that sounds like a great idea.
[34:52.020 --> 34:57.020]  I suspect that if you start war gaming it,
[34:57.460 --> 35:00.500]  there might be some either technical issues
[35:00.500 --> 35:04.560]  that would keep it from working or downsides,
[35:04.560 --> 35:06.860]  but as a general spitball idea,
[35:06.860 --> 35:09.740]  it's worth considering, right?
[35:09.740 --> 35:13.420]  Should, when you do this to one of your devices,
[35:13.420 --> 35:16.420]  should all of the devices on the same iCloud account
[35:16.420 --> 35:20.460]  go to the same state where touch ID and face ID are disabled
[35:20.460 --> 35:23.340]  and every device needs a passcode?
[35:23.340 --> 35:24.160]  Maybe?
[35:24.160 --> 35:25.000]  Or at least maybe it's just proximity.
[35:25.000 --> 35:26.860]  Maybe it's, like continuity is really good.
[35:26.860 --> 35:28.380]  One of the ways that you use Bluetooth
[35:28.380 --> 35:30.000]  is to determine how close you are to your stuff,
[35:30.000 --> 35:31.740]  so maybe it's just the stuff in your vicinity.
[35:31.740 --> 35:36.060]  Right, but would a closed MacBook that's on your person
[35:36.060 --> 35:38.940]  but in a bag, does continuity work?
[35:38.940 --> 35:40.820]  I don't know that it does, right?
[35:40.820 --> 35:42.240]  Yeah, you'd probably have to leave a signal
[35:42.240 --> 35:45.220]  that says when it's open, when you do,
[35:45.220 --> 35:46.180]  remember the power nap thing
[35:46.180 --> 35:48.140]  where it starts doing something when it's closed?
[35:48.140 --> 35:49.380]  And that's what I mean about
[35:49.380 --> 35:51.580]  that this is surely more complicated
[35:51.580 --> 35:54.260]  than the spitball idea sounds at first thought,
[35:54.260 --> 35:56.840]  but it still, it seems pretty,
[35:56.840 --> 35:57.680]  seems like some-
[35:57.680 --> 35:58.500]  And that's the rule, right?
[35:58.500 --> 35:59.340]  Like anything you're not personally in charge
[35:59.340 --> 36:01.020]  of implementing, shipping, and supporting
[36:01.020 --> 36:03.100]  is trivial by nature, by definition.
[36:03.100 --> 36:04.820]  Oh, the other, I wanted to mention this,
[36:04.820 --> 36:07.820]  the other downside that I can imagine
[36:07.820 --> 36:12.820]  while I was playing around pretending to do this,
[36:14.180 --> 36:18.300]  the other problem with just using
[36:18.300 --> 36:19.940]  on your MacBook a couple of times
[36:19.940 --> 36:23.300]  is you have to be conscious of,
[36:23.300 --> 36:24.980]  are you wearing your Apple Watch
[36:24.980 --> 36:28.640]  and do you have the unlock login
[36:28.640 --> 36:31.280]  with Apple Watch feature on your MacBook?
[36:31.280 --> 36:34.260]  And I do, it's one of my very favorite Apple Watch features,
[36:34.260 --> 36:36.300]  but that could defeat the point too
[36:36.300 --> 36:38.780]  where you think, okay, I've put the wrong finger
[36:38.780 --> 36:40.900]  in four times, it's asking me for a passcode,
[36:40.900 --> 36:43.340]  I close the lid, you go to the airport
[36:43.340 --> 36:45.080]  and the TSA guy opens it
[36:45.080 --> 36:47.020]  and your Apple Watch is right there,
[36:47.020 --> 36:48.660]  and then your Apple Watch taps
[36:48.660 --> 36:51.540]  and says, unlocked your MacBook with your Apple Watch,
[36:51.540 --> 36:53.140]  and you're like, oh, well, shit,
[36:53.140 --> 36:53.980]  I didn't mean that to happen.
[36:53.980 --> 36:55.860]  You have to put your Apple Watch into secure mode too,
[36:55.860 --> 36:57.360]  like the whole thing just keeps tumbling.
[36:57.360 --> 37:00.340]  Yeah, and that's one that I think definitely
[37:00.340 --> 37:02.340]  should work with continuity, right?
[37:02.340 --> 37:06.220]  Where if you squeeze your phone to put it in lock mode,
[37:06.220 --> 37:08.380]  I think your watch should go into it,
[37:08.380 --> 37:09.900]  if you're wearing your Apple Watch,
[37:09.900 --> 37:12.300]  it should go in the same mode at the same time.
[37:13.660 --> 37:15.860]  So I'm gonna do a follow-up article,
[37:15.860 --> 37:17.280]  but I wanted to talk it through with you.
[37:17.280 --> 37:19.620]  That's one of my ideas, that I think your watch
[37:19.620 --> 37:21.980]  should automatically go into that mode too,
[37:21.980 --> 37:25.820]  because it's, I don't really, I laugh.
[37:25.820 --> 37:28.020]  I'm sure some people use it and it's very useful.
[37:28.020 --> 37:31.380]  Every once in a while, I'll get an email notification
[37:31.380 --> 37:34.180]  on my watch and I will tap it and look at it
[37:34.180 --> 37:36.500]  right on the watch because I'm at dinner or something.
[37:36.500 --> 37:38.620]  But for the most part, I don't read my email
[37:38.620 --> 37:42.160]  on my Apple Watch, but your email is there,
[37:42.160 --> 37:44.660]  for most of us, your messages are there,
[37:44.660 --> 37:47.420]  so your watch is another device to think about.
[37:47.420 --> 37:49.740]  So it's another thing to put on the checklist.
[37:49.740 --> 37:51.940]  And just depending again on what your threat model is,
[37:51.940 --> 37:54.540]  a lot of your phone and your watch also sometimes have
[37:54.540 --> 37:57.220]  your location history from a running app
[37:57.220 --> 38:00.580]  or from a game or something, and someone can very quickly
[38:00.580 --> 38:02.660]  look and see where you were for the last two hours.
[38:02.660 --> 38:04.740]  Yeah, yeah, so something to think about.
[38:04.740 --> 38:08.180]  I mean, one thing I'm, and I don't wear my Apple Watch
[38:08.180 --> 38:10.260]  all the time, I have actual mechanical watches
[38:10.260 --> 38:12.460]  and I'm thinking, should I make up my policy
[38:12.460 --> 38:15.940]  that every time I fly, I'm not wearing my Apple Watch
[38:15.940 --> 38:18.800]  just to take it out of the, make sure my Apple Watch
[38:18.800 --> 38:21.260]  is packed rather than on my wrist for travel
[38:21.260 --> 38:22.540]  for that reason, maybe?
[38:22.540 --> 38:24.740]  Because it's so convenient, like you go in,
[38:24.740 --> 38:26.420]  I remember when the Apple Watch first came out,
[38:26.420 --> 38:28.420]  it took about a year, and then I went through the airport
[38:28.420 --> 38:31.260]  without anything else, I tapped for my coffee,
[38:31.260 --> 38:34.180]  for my ticket, for everything with my Apple Watch,
[38:34.180 --> 38:36.820]  including the hotel door, and it was just wild.
[38:36.820 --> 38:38.460]  But then you realize that the convenience
[38:38.460 --> 38:39.840]  comes with a security penalty.
[38:39.840 --> 38:41.900]  Right, that's one of the reasons I often,
[38:41.900 --> 38:43.900]  even though I do mix and match my watches a lot,
[38:43.900 --> 38:46.940]  I generally travel, do the actual traveling part
[38:46.940 --> 38:49.300]  with the Apple Watch on because it is convenient, right?
[38:49.300 --> 38:51.040]  You can put your boarding pass on it or,
[38:51.040 --> 38:52.540]  but again, something to think about.
[38:52.540 --> 38:55.220]  It's, once you start peeling the onion,
[38:55.220 --> 38:58.220]  it's like you get past the basic idea of,
[38:58.220 --> 39:00.120]  are you aware that you squeeze these two buttons
[39:00.120 --> 39:01.660]  on your phone to hard lock it?
[39:01.660 --> 39:03.180]  And everybody's like, yes, this is great.
[39:03.180 --> 39:05.100]  And now you're 85% of the way there,
[39:05.100 --> 39:09.220]  and the other 15% of the story is a lot of nuance, right?
[39:09.220 --> 39:10.900]  Apple Watch, something to think about.
[39:10.900 --> 39:12.980]  If it wasn't for humans, all this stuff would be fantastic,
[39:12.980 --> 39:15.740]  but humans keep making, ruining all the good internet fun.
[39:15.740 --> 39:17.220]  Your Touch ID MacBook.
[39:17.220 --> 39:20.820]  I think that the no questions asked solution
[39:20.820 --> 39:24.160]  is to shut it down before you go through a checkpoint.
[39:24.160 --> 39:26.940]  In an emergency situation,
[39:26.940 --> 39:30.580]  if you're unexpectedly pulled over or something like that,
[39:30.580 --> 39:32.980]  and you have your MacBook and you can access it,
[39:33.900 --> 39:35.500]  you can still do that, right?
[39:35.500 --> 39:38.080]  The safest thing to do is just squeeze the button
[39:38.080 --> 39:40.180]  for five seconds and just shut it down.
[39:40.180 --> 39:43.460]  I wouldn't want to, if I felt like time was of the essence
[39:43.460 --> 39:45.620]  and I was under duress,
[39:45.620 --> 39:49.460]  jerking around with my pinky finger a couple of times
[39:49.460 --> 39:50.680]  doesn't seem like a good idea.
[39:50.680 --> 39:51.520]  Just squeeze the button.
[39:51.520 --> 39:53.220]  And the watch can't unlock it in that state anyway.
[39:53.220 --> 39:55.020]  Right, so just squeeze the button
[39:55.020 --> 39:56.940]  until the screen goes off,
[39:56.940 --> 39:59.580]  and that's the best thing to remember.
[39:59.580 --> 40:00.980]  Just turn it off.
[40:00.980 --> 40:03.620]  Oh, and then the other topic on this thing,
[40:03.620 --> 40:04.540]  I'm so glad I remember it.
[40:04.540 --> 40:06.340]  I'm so glad I have notes.
[40:06.340 --> 40:07.160]  Yeah.
[40:07.160 --> 40:08.740]  Using Siri to do it.
[40:08.740 --> 40:09.860]  Yep.
[40:09.860 --> 40:14.140]  So you can tell Siri,
[40:14.140 --> 40:17.500]  and I hope that, I'll switch to calling it dingus soon
[40:17.500 --> 40:18.820]  without using the hey word,
[40:18.820 --> 40:21.580]  but to avoid triggering people's actual devices,
[40:21.580 --> 40:26.020]  but you can use Siri to lock your iPhone.
[40:26.020 --> 40:29.320]  Like if you say, hey, dingus, lock this device,
[40:29.320 --> 40:31.540]  it does turn the screen off and lock it,
[40:31.540 --> 40:33.260]  but it's not a hard lock.
[40:33.260 --> 40:34.180]  It's a soft lock.
[40:34.180 --> 40:36.520]  So I'm not quite sure why Siri even lets you do it.
[40:36.520 --> 40:41.340]  So touch ID and face ID still work in that state.
[40:41.340 --> 40:43.260]  And as far as I was aware,
[40:43.260 --> 40:45.660]  there's no way to say like,
[40:45.660 --> 40:47.440]  I call this hard locking.
[40:47.440 --> 40:49.160]  That's not an Apple term.
[40:49.160 --> 40:51.220]  They don't really talk about that,
[40:51.220 --> 40:53.260]  but I feel like it's useful to have a term
[40:53.260 --> 40:57.500]  and hard locking describes it as well.
[40:57.500 --> 40:58.340]  I can't think of a better term.
[40:58.340 --> 41:00.380]  Putting it into pre-board makes no sense to anybody.
[41:00.380 --> 41:01.920]  Right, right.
[41:01.920 --> 41:02.820]  Well, it makes sense.
[41:02.820 --> 41:04.880]  It makes a lot of sense to some people.
[41:04.880 --> 41:06.020]  Yes.
[41:06.020 --> 41:09.240]  And no sense at all to 99.99% of people.
[41:09.240 --> 41:12.340]  But for one out of every,
[41:12.340 --> 41:15.620]  one out of the nerds and certainly some engineers
[41:15.620 --> 41:17.180]  who work in the Cupertino area,
[41:17.180 --> 41:18.560]  it makes a lot of sense.
[41:18.560 --> 41:20.780]  But yeah, putting it into pre-board
[41:20.780 --> 41:22.580]  is definitely not going to do it.
[41:22.580 --> 41:25.400]  But anyway, what I did find via a tweet
[41:25.400 --> 41:30.360]  is if you say, hey, dingus, who do you belong to?
[41:30.360 --> 41:34.140]  Or hey, dingus, who does this device belong to?
[41:34.140 --> 41:36.500]  Or who does this iPhone belong to?
[41:36.500 --> 41:38.820]  And thankfully, unsurprisingly,
[41:38.820 --> 41:40.660]  this seems like something Siri can handle.
[41:40.660 --> 41:45.020]  You could use the more grammatically pedantic formulation
[41:45.020 --> 41:48.540]  of to whom do you belong?
[41:48.540 --> 41:52.180]  But if you more or less ask any sort of plain English,
[41:52.180 --> 41:57.180]  who or whom do you or does this device belong to?
[41:57.780 --> 42:01.020]  Siri will answer, I believe I belong to John,
[42:01.020 --> 42:03.380]  or I believe I belong to Renee.
[42:03.380 --> 42:07.900]  And they just, Siri just uses your first name,
[42:07.900 --> 42:09.740]  because there's, if you think about it,
[42:09.740 --> 42:12.180]  there's a lot of privacy angles on this.
[42:12.180 --> 42:13.820]  It's just your first name.
[42:13.820 --> 42:17.740]  But at that point, the device is hard-locked.
[42:17.740 --> 42:22.340]  Once you've asked Siri who the device
[42:22.340 --> 42:25.060]  Siri is running on belongs to,
[42:25.060 --> 42:28.140]  you need to enter your passcode to unlock the phone.
[42:28.140 --> 42:31.980]  Now, this only works if the phone is already locked.
[42:31.980 --> 42:35.860]  So if you're playing at home and you're paused this podcast
[42:35.860 --> 42:38.300]  and you look at your phone and your phone is unlocked
[42:38.300 --> 42:40.980]  and you ask Siri that question,
[42:40.980 --> 42:43.060]  it will not lock the phone.
[42:43.060 --> 42:45.420]  And you'll be like, oh, the group is wrong.
[42:45.420 --> 42:48.700]  No, no, lock the phone and ask Siri
[42:48.700 --> 42:51.740]  while the phone is locked who the device belongs to.
[42:51.740 --> 42:53.860]  It will hard-lock the phone.
[42:53.860 --> 42:56.180]  I would have never figured this out in a million years.
[42:56.180 --> 42:58.900]  I just would not have guessed that that would happen.
[42:58.900 --> 43:00.900]  It's a nice feature, it's good to know,
[43:00.900 --> 43:02.660]  and that's why I'm sharing it.
[43:02.660 --> 43:05.540]  But I feel like there should be a better way
[43:05.540 --> 43:07.460]  to do it with Siri, right?
[43:07.460 --> 43:11.020]  And it's truly one of those examples
[43:11.020 --> 43:13.740]  of accessibility helping everybody, right?
[43:16.180 --> 43:17.940]  Literally going back to the example
[43:17.940 --> 43:20.260]  of Trump's criminal lawyer, John Eastman,
[43:20.260 --> 43:23.020]  who had his phone cap taken over the weekend,
[43:23.020 --> 43:26.460]  I guess it was his wife, somebody who was with for dinner,
[43:26.460 --> 43:29.380]  videotaped a lot of, or at least the tail end
[43:29.380 --> 43:32.340]  of his confrontation with the FBI agents.
[43:32.340 --> 43:33.740]  And they had told him,
[43:35.580 --> 43:37.580]  it starts with him with his hands on his head.
[43:37.580 --> 43:39.140]  I presume his hands are on his head
[43:39.140 --> 43:42.860]  because the FBI agents who were confronting him
[43:42.860 --> 43:46.100]  in the parking lot told him to put his hands on the head,
[43:46.100 --> 43:49.260]  although perhaps he did it preemptively, I don't know.
[43:49.260 --> 43:52.100]  But if somebody tells you to put your hands on your head
[43:52.100 --> 43:55.140]  and they're in a position of authority and you do it
[43:55.140 --> 43:57.460]  and they take your phone out of your pocket,
[43:57.460 --> 44:00.260]  if you could direct a Siri command to the phone,
[44:00.260 --> 44:02.100]  that would be helpful.
[44:02.100 --> 44:06.620]  So it is good to know that you could do that.
[44:06.620 --> 44:10.140]  And if you're in a position where your hands are on your head
[44:10.140 --> 44:13.100]  and somebody takes your phone out of your pocket,
[44:13.100 --> 44:15.060]  you might be able to get it to work by saying,
[44:15.060 --> 44:19.160]  hey, dingus, who do you belong to?
[44:19.160 --> 44:22.580]  And that should lock, hard lock your phone.
[44:22.580 --> 44:26.660]  Again, that is such a weird formulation.
[44:26.660 --> 44:29.380]  I get it, why asking that hard locks the phone,
[44:29.380 --> 44:31.380]  but if your goal is to hard lock the phone,
[44:31.380 --> 44:34.120]  I don't think anybody would come up with that, right?
[44:35.060 --> 44:38.700]  I feel like there should be a way to do it with Siri.
[44:38.700 --> 44:41.480]  And also, to go back to what we were talking about
[44:41.480 --> 44:44.580]  five minutes ago, to lock all of your devices,
[44:44.580 --> 44:46.240]  wouldn't it be a neat feature if you could say,
[44:46.240 --> 44:49.780]  hey, dingus, lock all my devices?
[44:49.780 --> 44:52.420]  And just saying lock all my devices
[44:52.420 --> 44:55.620]  would send a signal to iCloud that any device,
[44:55.620 --> 44:58.660]  Mac, iPad, watch, whatever,
[44:58.660 --> 45:01.260]  would go into a require the passcode state
[45:01.260 --> 45:03.460]  as soon as it connects to iCloud again.
[45:04.460 --> 45:06.260]  It's funny because I've set up HomeKit
[45:06.260 --> 45:07.920]  to where if I say crash the West Wing,
[45:07.920 --> 45:09.140]  it just turns everything off,
[45:09.140 --> 45:10.900]  and I would love that similar feature.
[45:10.900 --> 45:13.900]  So anyway, I would love to have a better solution,
[45:13.900 --> 45:15.720]  but there is a way to do it with Siri.
[45:15.720 --> 45:19.220]  It is nonintuitive, and I feel like Apple
[45:19.220 --> 45:23.340]  has the potential here to improve that greatly, right?
[45:23.340 --> 45:24.960]  And then you don't have to,
[45:24.960 --> 45:26.820]  if you could do that, and I do,
[45:26.820 --> 45:31.820]  I think that if you just ask Siri to lock your device,
[45:32.260 --> 45:33.740]  it should lock all of them,
[45:33.740 --> 45:35.380]  because it's not really locking you out.
[45:35.380 --> 45:37.580]  All it does is it's just making you enter your password
[45:37.580 --> 45:38.520]  on all of them.
[45:38.520 --> 45:41.420]  Just say lock my device or devices,
[45:41.420 --> 45:44.220]  and it should somehow trigger a thing
[45:44.220 --> 45:46.300]  through continuity and or iCloud
[45:46.300 --> 45:48.260]  to lock as many of them as it can,
[45:48.260 --> 45:51.320]  or lock them when they next wake up.
[45:51.320 --> 45:54.520]  Yeah, no, I think that's brilliant.
[45:54.520 --> 45:56.320]  That's fantastic.
[45:56.320 --> 45:59.400]  I think just like for operational security point of view,
[45:59.400 --> 46:03.100]  I turn off, I disable Siri whenever I'm in anywhere
[46:03.100 --> 46:04.760]  approaching an uncontrolled environment
[46:04.760 --> 46:07.180]  just because Siri can leak a ton of information.
[46:07.180 --> 46:09.440]  So I never want them asking Siri about contact questions
[46:09.440 --> 46:11.360]  or call people or stuff like that,
[46:11.360 --> 46:13.200]  which would make this harder, but I think that's-
[46:13.200 --> 46:14.840]  How do you do that?
[46:14.840 --> 46:16.000]  I just go in and turn off,
[46:16.000 --> 46:17.880]  I turn off access to everything on the lock screen,
[46:17.880 --> 46:20.480]  notifications, Siri, everything that's,
[46:20.480 --> 46:22.220]  like I'm gonna say leaks for the lock screen.
[46:22.220 --> 46:23.340]  Usually it's really convenient,
[46:23.340 --> 46:26.000]  but again, when I'm in an uncontrolled environment,
[46:26.000 --> 46:28.120]  I don't want any of that stuff on,
[46:28.120 --> 46:29.160]  like I don't want them to pick up my phone
[46:29.160 --> 46:31.440]  and just see a bunch of notifications, anything like that.
[46:31.440 --> 46:34.480]  So you go to settings and-
[46:34.480 --> 46:36.240]  Yeah, there's a lock screen section of settings
[46:36.240 --> 46:38.440]  and there's a Siri section of the settings.
[46:38.440 --> 46:40.560]  So you have to go to both?
[46:40.560 --> 46:41.400]  Yeah, to get everything,
[46:41.400 --> 46:42.760]  to turn everything off on the lock screen.
[46:42.760 --> 46:44.360]  I believe you still have to go to both.
[46:44.360 --> 46:46.600]  I wonder if you could do it with a shortcut.
[46:47.480 --> 46:49.120]  Probably.
[46:49.120 --> 46:50.760]  I need to get rid of the lock screen section.
[46:50.760 --> 46:51.600]  Maybe it's just all-
[46:51.600 --> 46:55.800]  Yeah, I'm looking, I'm on iPad OS 16 on this iPad
[46:55.800 --> 46:59.800]  and I'm looking and I don't see the lock screen thing.
[46:59.800 --> 47:00.960]  Yeah, no, me too.
[47:00.960 --> 47:02.740]  But I turn off like the hey dingus thing
[47:02.740 --> 47:04.560]  and I turn off like all of that kind of stuff.
[47:04.560 --> 47:06.240]  I just have like a little routine.
[47:06.240 --> 47:07.920]  Yeah, well, it's worth thinking about.
[47:07.920 --> 47:09.880]  Yeah, so you could turn off listen for hey dingus
[47:09.880 --> 47:11.760]  and you can turn off allow dingus when locked.
[47:11.760 --> 47:14.560]  Right, so that's sort of-
[47:15.880 --> 47:17.720]  Again, it depends on what your threat model is
[47:17.720 --> 47:19.040]  and what your paranoia level is.
[47:19.040 --> 47:19.880]  Right.
[47:19.880 --> 47:22.240]  On the one hand, I'm thinking about ways
[47:22.240 --> 47:24.780]  that Siri could help you in this situation,
[47:24.780 --> 47:26.800]  but on the other hand, I don't disagree with you
[47:26.800 --> 47:29.560]  that maybe the best thing is to just turn it off completely.
[47:29.560 --> 47:34.040]  And it's funny too, because I don't really know how,
[47:34.040 --> 47:36.740]  because again, I'm doing this all
[47:36.740 --> 47:38.160]  from a hypothetical situation
[47:38.160 --> 47:41.000]  where I don't recall ever having personally been
[47:41.000 --> 47:43.280]  in a situation where I needed this to happen,
[47:43.280 --> 47:47.080]  but you want to be prepared for it before you do, obviously.
[47:47.080 --> 47:51.400]  I don't know how well Siri actually is these days
[47:51.400 --> 47:54.100]  at like knowing my voice from others.
[47:54.100 --> 47:57.120]  Like, you know, what's the likelihood
[47:57.120 --> 48:01.680]  that a random police officer or security agent at the TSA
[48:01.680 --> 48:04.720]  could talk to my phone and get information out of it?
[48:04.720 --> 48:07.240]  I don't even know, you know?
[48:07.240 --> 48:10.720]  Yeah, it's probably a degree of differentiation
[48:10.720 --> 48:12.100]  in the voice is that it wants to,
[48:12.100 --> 48:12.940]  it needs to be able to like,
[48:12.940 --> 48:15.160]  if your voice is slightly coffee or if your voice is
[48:15.160 --> 48:16.080]  in the morning, come here tonight,
[48:16.080 --> 48:17.360]  it probably has to cover a range.
[48:17.360 --> 48:19.160]  Right, but anyway, things to think about,
[48:19.160 --> 48:20.920]  and I do think in general,
[48:20.920 --> 48:24.160]  regardless of what anybody listening's personal concerns
[48:24.160 --> 48:25.320]  are, it's all good to know.
[48:25.320 --> 48:27.880]  And at the very least that squeeze your phone
[48:27.880 --> 48:30.160]  to lock the passcode is something
[48:30.160 --> 48:31.280]  you should share with others.
[48:31.280 --> 48:34.460]  Anything else you want to add before we move on?
[48:34.460 --> 48:35.780]  No, just to your point,
[48:35.780 --> 48:38.000]  make yourself a little like personal safety list.
[48:38.000 --> 48:40.040]  And you may never need it, but if you do travel,
[48:40.040 --> 48:41.640]  if you do go to protests or anything like that,
[48:41.640 --> 48:42.800]  just make sure you go through it.
[48:42.800 --> 48:44.480]  Right, I forget his name,
[48:44.480 --> 48:46.920]  but he's a medical doctor at the New Yorker,
[48:46.920 --> 48:48.980]  but he's even, I think he even has a book,
[48:48.980 --> 48:50.660]  The Checklist Manifesto.
[48:52.200 --> 48:54.480]  And that making a checklist and having a check,
[48:54.480 --> 48:56.840]  you know, like speaking of airlines, pilots go through this.
[48:56.840 --> 48:58.680]  They have an actual checklist that they go through
[48:58.680 --> 49:01.120]  every time they get into cockpit
[49:01.120 --> 49:02.840]  and they go through every single item
[49:02.840 --> 49:04.240]  on the checklist every time.
[49:04.240 --> 49:06.280]  And it doesn't matter if they do it twice a day,
[49:06.280 --> 49:08.560]  every day, five days a week for 20 years,
[49:08.560 --> 49:10.540]  they still use the actual checklist
[49:10.540 --> 49:12.800]  and as opposed to just doing it from memory
[49:12.800 --> 49:17.080]  because it actually helps to avoid ever forgetting a step.
[49:17.080 --> 49:20.840]  And the, the anecdotal data about it,
[49:20.840 --> 49:23.600]  we're not, not even anecdotal, like actual data
[49:23.600 --> 49:26.720]  is that in surgical situations in hospitals,
[49:26.720 --> 49:29.200]  having an actual checklist to go through
[49:29.200 --> 49:32.920]  for every single procedure actually reduces mistakes.
[49:32.920 --> 49:36.080]  It, to a statistically significant degree.
[49:36.080 --> 49:38.120]  I'll try to find that and put it in the show notes.
[49:38.120 --> 49:40.340]  But anyway, you should have one for travel.
[49:40.340 --> 49:41.920]  I recommend it wholeheartedly
[49:41.920 --> 49:43.600]  because the only things you forget
[49:43.600 --> 49:44.680]  are the things you forget.
[49:44.680 --> 49:46.780]  And if you have a checklist, you won't forget them.
[49:46.780 --> 49:47.620]  Absolutely.
[49:47.620 --> 49:49.520]  Let me thank our next sponsor.
[49:49.520 --> 49:50.900]  Oh, I love this company.
[49:50.900 --> 49:52.440]  This is great, Collide.
[49:52.440 --> 49:55.640]  Collide is the new take on endpoint management.
[49:55.640 --> 49:57.160]  And they ask a simple question.
[49:57.160 --> 50:00.760]  How can we get end users more involved?
[50:00.760 --> 50:04.640]  Collide, the whole system is in direct contrast
[50:04.640 --> 50:07.760]  to old school device management tools like MDM.
[50:07.760 --> 50:11.560]  MDM typically locks down your employees' devices
[50:11.560 --> 50:13.600]  without considering their needs
[50:13.600 --> 50:15.760]  or even attempting to educate them
[50:15.760 --> 50:18.580]  about the security issues on their laptops.
[50:18.580 --> 50:21.320]  Why, you know, what are the reasons why
[50:21.320 --> 50:23.480]  you're installing some kind of security management
[50:23.480 --> 50:24.400]  on their laptops?
[50:24.400 --> 50:25.820]  MDM doesn't even consider that.
[50:25.820 --> 50:29.040]  Collide is built by like-minded security practitioners
[50:29.040 --> 50:32.880]  who saw all the flaws in traditional MDM structures
[50:32.880 --> 50:34.940]  and how disruptive it was to end users
[50:34.940 --> 50:37.300]  and often how it frustrated them so badly
[50:37.300 --> 50:39.220]  that they would just take their work laptops,
[50:39.220 --> 50:41.840]  keep them aside, and just do their work on their personal
[50:41.840 --> 50:45.160]  laptops with no security software on it whatsoever,
[50:45.160 --> 50:46.260]  which defeats the whole point.
[50:46.260 --> 50:47.640]  But they did it just to get work done
[50:47.640 --> 50:51.180]  because the tools that were being used were so disruptive
[50:51.180 --> 50:53.840]  to actually just, you know, doing their work.
[50:53.840 --> 50:55.460]  In that scenario, everyone loses.
[50:55.460 --> 50:59.400]  Collide, K-O-L-I-D-E, Collide with a K,
[50:59.400 --> 51:00.640]  on the other hand, is different.
[51:00.640 --> 51:01.880]  Instead of locking down a device,
[51:01.880 --> 51:03.480]  they take a user-focused approach
[51:03.480 --> 51:05.960]  that communicates security recommendations
[51:05.960 --> 51:08.740]  directly to employees on Slack.
[51:08.740 --> 51:10.300]  If you're already on Slack,
[51:10.300 --> 51:13.480]  you're not even asking them to turn on something else
[51:13.480 --> 51:16.520]  to get notifications and messages about new changes
[51:16.520 --> 51:20.000]  or recommendations or notices about something
[51:20.000 --> 51:22.040]  that's misconfigured on their system.
[51:22.040 --> 51:24.480]  They get it all, they do it all directly through Slack,
[51:24.480 --> 51:25.880]  which you're already using.
[51:25.880 --> 51:27.800]  You're not even asking your employees
[51:27.800 --> 51:30.720]  to use another communication tool.
[51:30.720 --> 51:32.720]  It's Slack, which you're already using.
[51:32.720 --> 51:35.800]  It'll educate your employees about the company's policies
[51:35.800 --> 51:38.100]  and give them recommendations and direct steps
[51:38.100 --> 51:40.360]  to keep their devices secure
[51:40.360 --> 51:44.160]  using real tangible examples, not theoretical scenarios.
[51:44.160 --> 51:47.700]  That's Collide, cross-platform endpoint management
[51:47.700 --> 51:50.240]  for Linux, Mac, and Windows devices
[51:50.240 --> 51:53.240]  that puts end users first for teams that Slack.
[51:53.240 --> 51:58.240]  Visit Collide.com, K-O-L-I-D-E, Collide.com slash
[51:58.840 --> 52:01.640]  the talk show to learn more.
[52:01.640 --> 52:04.960]  You get, with that URL, you get a free 14-day trial.
[52:04.960 --> 52:06.360]  Enter your email when prompted,
[52:06.360 --> 52:08.920]  and you'll receive a free Collide gift bundle
[52:08.920 --> 52:12.760]  after your trial activation with no credit card required.
[52:12.760 --> 52:17.520]  Once again, that's Collide.com slash the talk show.
[52:17.520 --> 52:20.420]  Speaking of security, Rene, I was,
[52:20.420 --> 52:22.120]  have you heard of Instagram?
[52:23.400 --> 52:26.800]  It's, it's, it used to be like a photo sharing app.
[52:26.800 --> 52:29.160]  It was pretty cool and really simple,
[52:29.160 --> 52:30.000]  and they've added-
[52:30.000 --> 52:31.200]  So like phones, right?
[52:31.200 --> 52:33.120]  Like that's all you wanted to post was like your phone stuff.
[52:33.120 --> 52:36.760]  Yeah, they've added some features in recent years.
[52:37.600 --> 52:40.480]  I was, I was flipping through the Instagram yesterday,
[52:40.480 --> 52:44.680]  and I came across a reel from a guy named Rene Ritchie
[52:44.680 --> 52:48.640]  explaining the new PassKey feature in iOS,
[52:48.640 --> 52:52.160]  or all of Apple's, it was talked about at WWDC,
[52:52.160 --> 52:53.920]  coming to all their platforms.
[52:53.920 --> 52:55.880]  Yeah, I, this is,
[52:58.920 --> 53:00.880]  let's explain PassKey.
[53:00.880 --> 53:02.600]  Okay, so like in a traditional world,
[53:02.600 --> 53:05.240]  you would go to create an account for like Megacorp,
[53:05.240 --> 53:07.400]  and you'd give them an email address,
[53:07.400 --> 53:08.800]  and you'd create a password,
[53:08.800 --> 53:10.920]  and the password would be stored on your machine,
[53:10.920 --> 53:13.040]  and then a hashed and salted version of it
[53:13.040 --> 53:14.480]  would be stored on the server.
[53:14.480 --> 53:16.640]  And when you go to log in, they would compare the two,
[53:16.640 --> 53:18.480]  and if they matched, they would let you in.
[53:18.480 --> 53:20.440]  But the downside to that is that
[53:20.440 --> 53:22.280]  because you have a copy of your password
[53:22.280 --> 53:23.480]  and you probably know it,
[53:23.480 --> 53:25.320]  and, or at least, you know, in the early days,
[53:25.320 --> 53:26.400]  everybody knew their passwords
[53:26.400 --> 53:28.120]  and often used the same passwords,
[53:28.120 --> 53:29.360]  and the server had a copy,
[53:29.360 --> 53:31.120]  even though it was hashed and salted,
[53:31.120 --> 53:33.240]  that made two targets that people could get to.
[53:33.240 --> 53:35.280]  They could hack them, they could crack them,
[53:35.280 --> 53:37.360]  they could shoulder surf you typing it in,
[53:37.360 --> 53:39.280]  they could call up and pretend to be tech support
[53:39.280 --> 53:41.000]  and ask you for the password.
[53:41.000 --> 53:43.600]  There's just so many ways that it could be exploited.
[53:43.600 --> 53:45.760]  So what, the way PassKeys work is,
[53:45.760 --> 53:48.320]  there's no more passwords in that sense.
[53:48.320 --> 53:49.920]  When you create an account,
[53:49.920 --> 53:52.360]  it creates a public and a private key,
[53:52.360 --> 53:54.240]  and people will know this formula from SSH.
[53:54.240 --> 53:56.680]  It's not a new formula, it's just a new implementation.
[53:56.680 --> 54:00.120]  And then the private key is stored on your device,
[54:00.120 --> 54:03.400]  only in your device, locked down behind biometrics
[54:03.400 --> 54:05.440]  or a password or a passcode,
[54:05.440 --> 54:07.920]  and the public key is stored on the server.
[54:07.920 --> 54:10.360]  And because a public key has no special information,
[54:10.360 --> 54:12.240]  it's no better than a username,
[54:12.240 --> 54:14.320]  if somebody goes and steals it, it doesn't matter.
[54:14.320 --> 54:16.200]  Again, it's public information.
[54:16.200 --> 54:18.960]  And because your private key is only on your device,
[54:18.960 --> 54:19.880]  you don't know what it is,
[54:19.880 --> 54:22.600]  so it's much harder to socially engineer it from you.
[54:22.600 --> 54:26.760]  When you go to log in, the server issues a challenge,
[54:26.760 --> 54:28.800]  and it sends that to your device.
[54:28.800 --> 54:32.240]  And then your private key is used to generate a response,
[54:32.240 --> 54:35.520]  a solution to that challenge, in the form of a signature,
[54:35.520 --> 54:37.920]  and then that signature is sent back to the server.
[54:37.920 --> 54:41.800]  So at no point does your private key ever leave your device.
[54:41.800 --> 54:44.560]  It's never in transit, and it's never on the other server.
[54:44.560 --> 54:46.520]  And that makes it really, really hard
[54:46.520 --> 54:48.440]  for any of the traditional, you know,
[54:48.440 --> 54:52.720]  phaser-set-to-kill-style attacks to work on you.
[54:52.720 --> 54:54.040]  So there's a lot to unpack there.
[54:54.040 --> 54:55.200]  That was wonderful, though,
[54:55.200 --> 54:57.520]  especially since I sort of sprung it on you.
[54:57.520 --> 54:59.640]  That was a wonderful synopsis.
[54:59.640 --> 55:01.240]  There's a couple of things about it
[55:01.240 --> 55:05.560]  that I think is hard to get one's head wrapped around.
[55:05.560 --> 55:09.160]  So part of it is that,
[55:09.160 --> 55:12.680]  and let's go back to the way passwords traditionally work.
[55:14.360 --> 55:17.400]  If you go back far enough, and this was,
[55:17.400 --> 55:20.520]  actually was, I would say it was never a best practice,
[55:20.520 --> 55:23.240]  but it was just the way things always were done,
[55:23.240 --> 55:25.600]  is that passwords were just stored
[55:25.600 --> 55:29.000]  as plain text passwords on the system
[55:29.000 --> 55:30.200]  that you were logging into.
[55:30.200 --> 55:32.480]  You'd have to go back a long time before, you know,
[55:32.480 --> 55:33.400]  but we're talking, you know,
[55:33.400 --> 55:34.880]  people have been using passwords
[55:34.880 --> 55:37.560]  in network situations for decades.
[55:37.560 --> 55:40.720]  You know, you go back to the 80s or 90s,
[55:40.720 --> 55:43.240]  this was common, and people just didn't, you know,
[55:43.240 --> 55:46.760]  the advice was basically pick a good password
[55:46.760 --> 55:51.400]  and a unique password for every account you create.
[55:51.400 --> 55:55.480]  So never reuse a password and pick a good one,
[55:55.480 --> 55:58.880]  you know, with lots of letters and punctuation, whatever.
[56:00.040 --> 56:02.680]  And if the server or the service,
[56:02.680 --> 56:06.080]  whatever you wanna call it, just stored it as plain text,
[56:06.080 --> 56:09.640]  so your password is password, right?
[56:09.640 --> 56:12.160]  It's just the dumbest password of all time.
[56:12.160 --> 56:15.120]  And it would just be in a file,
[56:15.120 --> 56:18.200]  and it would say the account name is Gruber,
[56:18.200 --> 56:20.000]  and the password is password.
[56:20.000 --> 56:21.800]  And then you would log in and say,
[56:21.800 --> 56:25.560]  my account name is Gruber, and my password is password.
[56:25.560 --> 56:27.200]  And then it would check those two things
[56:27.200 --> 56:29.000]  and say, okay, you're in.
[56:29.000 --> 56:31.360]  Now, the security downside of this is obvious,
[56:31.360 --> 56:34.600]  which is that anybody with access to the server
[56:34.600 --> 56:36.600]  could just see everybody's passwords.
[56:36.600 --> 56:41.360]  And whether that's a rogue employee
[56:41.360 --> 56:46.320]  or just a hack, right?
[56:46.320 --> 56:48.840]  Somebody who breaks into the server
[56:48.840 --> 56:53.240]  and steals the password file, it's all just there.
[56:53.240 --> 56:54.760]  Obviously, bad news.
[56:54.760 --> 56:56.840]  Everybody's known for quite a long time.
[56:56.840 --> 56:59.360]  This is a terrible practice.
[56:59.360 --> 57:02.280]  It seems like we're at a point in 2022
[57:02.280 --> 57:05.640]  where I don't see very many stories anymore
[57:05.640 --> 57:08.360]  about such and such service got hacked
[57:08.360 --> 57:10.800]  and they stored the passwords as plain text,
[57:10.800 --> 57:13.160]  like, you know, it's one of those things
[57:13.160 --> 57:14.680]  where it takes years and years
[57:14.680 --> 57:19.680]  to shake out all the legacy systems
[57:19.720 --> 57:21.440]  that were never updated.
[57:21.440 --> 57:23.920]  But that doesn't seem to happen anymore.
[57:23.920 --> 57:27.320]  What you described as salted passwords
[57:27.320 --> 57:29.080]  is typically the way it works.
[57:29.080 --> 57:33.320]  And without turning this into a programming podcast,
[57:33.320 --> 57:37.000]  the way that typically works is
[57:37.000 --> 57:41.120]  at some point you have to send the service your password,
[57:41.120 --> 57:42.640]  right, and you send it.
[57:42.640 --> 57:47.240]  But what the service would do is they have a secret string.
[57:47.240 --> 57:48.480]  That's the salt.
[57:48.480 --> 57:49.680]  And you don't know it.
[57:49.680 --> 57:53.840]  Like, you're logging into example.com.
[57:53.840 --> 57:55.760]  Example.com has a salt,
[57:55.760 --> 58:00.760]  and the salt might be just a 32-character string
[58:01.680 --> 58:05.080]  of random letters, totally random.
[58:05.080 --> 58:09.120]  Every single password, it takes your password,
[58:09.120 --> 58:11.680]  password, combines it with the salt,
[58:11.680 --> 58:14.200]  which is a secret to the server,
[58:14.200 --> 58:16.880]  and then gives it to a function
[58:16.880 --> 58:20.760]  that creates a hash key out of it.
[58:20.760 --> 58:22.960]  And without going into those functions,
[58:22.960 --> 58:25.080]  there's lots of them, you know,
[58:25.080 --> 58:30.080]  but it's like SSH, S-H-A is one of them.
[58:31.480 --> 58:34.600]  MD5 is an old one that is more or less random.
[58:34.600 --> 58:36.080]  More or less frowned upon at this point
[58:36.080 --> 58:38.720]  because it's not quite as cryptographically secure.
[58:38.720 --> 58:40.320]  But basically these are functions
[58:40.320 --> 58:43.360]  that you can give any string of characters to,
[58:43.360 --> 58:44.520]  give it to the function,
[58:44.520 --> 58:48.440]  and it'll create a new string that looks totally random.
[58:48.440 --> 58:51.400]  And you can't take backwards
[58:51.400 --> 58:56.120]  to produce the password and the salt.
[58:56.120 --> 58:57.600]  You take two strings.
[58:57.600 --> 59:00.120]  It could just be Renee and Richie,
[59:00.120 --> 59:02.040]  and you give them to this function.
[59:02.040 --> 59:05.240]  And if you give it to the same function on any system,
[59:05.240 --> 59:07.560]  you'll get the same random string
[59:07.560 --> 59:10.160]  of let's say 64 characters out.
[59:10.160 --> 59:12.200]  That's what gets stored.
[59:12.200 --> 59:15.240]  And then every time you log in, you give it your password.
[59:15.240 --> 59:18.120]  They combine it with the secret salt,
[59:18.120 --> 59:19.480]  pass it to the function,
[59:19.480 --> 59:22.600]  and then see if that product of that
[59:22.600 --> 59:24.680]  matches what's stored.
[59:24.680 --> 59:26.720]  Far more secure.
[59:26.720 --> 59:29.920]  But one of the reasons it's more secure
[59:29.920 --> 59:34.040]  is anybody who captures the list on the server
[59:34.040 --> 59:36.320]  just has those hash values.
[59:36.320 --> 59:37.680]  And those can't be,
[59:37.680 --> 59:40.120]  literally cannot be backwards engineered
[59:40.120 --> 59:41.720]  to get the passwords out of them.
[59:42.920 --> 59:47.000]  But one of the weaknesses is that the salt somewhere
[59:47.000 --> 59:49.360]  has to be readable, right?
[59:49.360 --> 59:51.680]  And so if somebody gets the source code
[59:52.880 --> 59:54.560]  that contains the salt,
[59:56.000 --> 59:59.720]  you still can't get the passwords knowing only half.
[59:59.720 --> 01:00:01.240]  You still don't know the passwords.
[01:00:01.240 --> 01:00:02.880]  It's the password plus the salt
[01:00:02.880 --> 01:00:05.920]  plus the cryptographic function equals the hash key.
[01:00:06.800 --> 01:00:09.760]  But at that point, you can start brute forcing
[01:00:09.760 --> 01:00:13.720]  with a common list of passwords, right?
[01:00:13.720 --> 01:00:16.240]  And there are lots and lots of these lists everywhere,
[01:00:16.240 --> 01:00:19.880]  where it's like, and with how fast computers are today,
[01:00:19.880 --> 01:00:23.400]  it doesn't have to be like the top 100 common passwords.
[01:00:23.400 --> 01:00:28.400]  It could be the top 50,000 common passwords that people use.
[01:00:28.400 --> 01:00:31.880]  And you can mow through those relatively quickly,
[01:00:31.880 --> 01:00:34.000]  and all of a sudden you're getting,
[01:00:34.000 --> 01:00:36.200]  then you can start getting passwords out of it
[01:00:36.200 --> 01:00:40.920]  by guessing 50,000 common passwords against the known salt
[01:00:40.920 --> 01:00:45.440]  that you've stolen from the source code
[01:00:45.440 --> 01:00:47.680]  and knowing which cryptographic function they use.
[01:00:47.680 --> 01:00:50.000]  Now, all of a sudden, your password is out in the open.
[01:00:50.000 --> 01:00:51.600]  And that has happened.
[01:00:51.600 --> 01:00:52.440]  That does not-
[01:00:52.440 --> 01:00:54.280]  You'll see like the Home Depot was compromised
[01:00:54.280 --> 01:00:56.480]  and suddenly the dark web has like a list of passwords
[01:00:56.480 --> 01:00:57.320]  and user accounts.
[01:00:57.320 --> 01:00:58.560]  Right.
[01:00:58.560 --> 01:01:00.480]  That is not a hypothetical scenario.
[01:01:00.480 --> 01:01:02.280]  That is an actual weakness.
[01:01:02.280 --> 01:01:06.120]  The other weakness is just passwords themselves, right?
[01:01:06.120 --> 01:01:10.280]  The fact that human beings typically just reuse passwords
[01:01:10.280 --> 01:01:12.800]  as opposed to creating unique ones for each account.
[01:01:12.800 --> 01:01:15.200]  They often have picked,
[01:01:15.200 --> 01:01:17.240]  especially for accounts that have been around
[01:01:17.240 --> 01:01:21.040]  for a longer time before more modern systems
[01:01:21.040 --> 01:01:24.360]  have started like Safari, et cetera,
[01:01:24.360 --> 01:01:27.080]  offering very secure passwords for you
[01:01:27.080 --> 01:01:29.920]  that they will store for you in your key chain.
[01:01:29.920 --> 01:01:32.880]  Or if you use one password, it will do the same thing,
[01:01:32.880 --> 01:01:36.440]  suggest a unique, secure, strong password.
[01:01:36.440 --> 01:01:37.720]  And if you accept it,
[01:01:37.720 --> 01:01:42.320]  immediately store it in your password manager
[01:01:42.320 --> 01:01:45.080]  so that you don't have to take a manual step afterwards
[01:01:45.080 --> 01:01:47.480]  of writing it down, storing it somewhere.
[01:01:47.480 --> 01:01:50.200]  It still isn't great because anybody who gets-
[01:01:50.200 --> 01:01:53.600]  You have passwords that can be stolen.
[01:01:53.600 --> 01:01:56.640]  And that's an intractable problem
[01:01:56.640 --> 01:01:57.640]  and cannot be solved.
[01:01:57.640 --> 01:02:01.360]  Whereas with these pass keys, there is no password.
[01:02:01.360 --> 01:02:03.280]  And I know that's hard to understand,
[01:02:03.280 --> 01:02:06.200]  but basically it's because these keys
[01:02:06.200 --> 01:02:07.720]  never leave your device.
[01:02:07.720 --> 01:02:10.480]  So they're never, for lack of a better way of putting it,
[01:02:10.480 --> 01:02:12.080]  they're never in your brain.
[01:02:12.080 --> 01:02:13.720]  There is nothing for you to memorize.
[01:02:13.720 --> 01:02:15.120]  Even if you could,
[01:02:15.120 --> 01:02:18.000]  if you're so good at memorizing random strings
[01:02:18.000 --> 01:02:19.120]  of characters and letters,
[01:02:19.120 --> 01:02:22.200]  that you could memorize a 32 character
[01:02:22.200 --> 01:02:26.680]  or 64 character string of random letters and numbers.
[01:02:26.680 --> 01:02:27.880]  It's a random blob.
[01:02:27.880 --> 01:02:30.320]  Right. You never see it.
[01:02:30.320 --> 01:02:31.200]  You can't see it.
[01:02:31.200 --> 01:02:33.560]  It never leaves, in Apple's world,
[01:02:33.560 --> 01:02:36.840]  never leaves the secure enclave of your devices, I believe,
[01:02:36.840 --> 01:02:37.680]  or-
[01:02:37.680 --> 01:02:39.320]  You can airdrop it to a loved one
[01:02:39.320 --> 01:02:40.760]  and conceivably somebody could trick you
[01:02:40.760 --> 01:02:41.960]  into airdropping it to them,
[01:02:41.960 --> 01:02:44.600]  but that's an incredibly specific use case.
[01:02:44.600 --> 01:02:45.640]  Right. So yeah,
[01:02:45.640 --> 01:02:48.600]  you can obviously get them out of the secure storage
[01:02:48.600 --> 01:02:51.640]  because they have to be used, but it's, you know-
[01:02:51.640 --> 01:02:52.680]  They never leave the secure storage.
[01:02:52.680 --> 01:02:55.080]  But like, for example, if Amy Jane was like,
[01:02:55.080 --> 01:02:57.160]  she's on a different device, but like a different account,
[01:02:57.160 --> 01:02:58.680]  but you want to use the same, not a different device.
[01:02:58.680 --> 01:03:00.760]  She's on a different device and wants to use the same service
[01:03:00.760 --> 01:03:01.720]  that you subscribe to.
[01:03:01.720 --> 01:03:04.680]  You can airdrop her the key for it.
[01:03:04.680 --> 01:03:06.840]  Right. Like something, you know, which is, you know,
[01:03:06.840 --> 01:03:08.280]  not even wink, wink, nudge, nudge.
[01:03:08.280 --> 01:03:10.600]  You're allowed to do like a shared Netflix account
[01:03:10.600 --> 01:03:12.120]  for a family or something like that.
[01:03:12.120 --> 01:03:13.480]  The other thing that's really cool,
[01:03:13.480 --> 01:03:14.760]  just in general about this,
[01:03:14.760 --> 01:03:16.360]  is it removes the need for two factor,
[01:03:16.360 --> 01:03:18.520]  which we've seen can also be a weakness,
[01:03:18.520 --> 01:03:20.840]  not just because people can do a SIM swap attack.
[01:03:20.840 --> 01:03:23.240]  Like they can call up your cell company and say,
[01:03:23.240 --> 01:03:24.440]  oh, I've lost my phone.
[01:03:24.440 --> 01:03:26.280]  This is my new number, my new SIM card.
[01:03:26.280 --> 01:03:27.800]  Can you assign the number to that SIM card?
[01:03:27.800 --> 01:03:29.720]  And then they get your token that,
[01:03:29.720 --> 01:03:31.880]  because so many companies still either use
[01:03:31.880 --> 01:03:35.400]  or fall back on SMS as a way to deliver tokens.
[01:03:35.400 --> 01:03:36.920]  This, you don't need the token.
[01:03:36.920 --> 01:03:38.520]  So this totally eliminates that.
[01:03:38.520 --> 01:03:39.400]  Yeah. Good point.
[01:03:39.400 --> 01:03:42.280]  I guess it, and the other thing to remember is
[01:03:42.280 --> 01:03:45.480]  this is not a Apple invention.
[01:03:45.480 --> 01:03:48.760]  This is a industry-wide consortium.
[01:03:48.760 --> 01:03:51.720]  And as far, I can't think of a big company
[01:03:51.720 --> 01:03:54.440]  who you'd wish would be involved who isn't, right?
[01:03:54.440 --> 01:03:57.560]  Google and Microsoft and, you know,
[01:03:57.560 --> 01:03:58.760]  because this is one of those areas
[01:03:58.760 --> 01:04:01.880]  where there's really no, there's,
[01:04:01.880 --> 01:04:03.960]  you're not even depending on the goodwill
[01:04:03.960 --> 01:04:04.680]  of these companies.
[01:04:04.680 --> 01:04:07.560]  There is no downside to them collaborating
[01:04:07.560 --> 01:04:09.400]  to make this work, right?
[01:04:09.400 --> 01:04:12.040]  None of these major companies have any interest
[01:04:12.040 --> 01:04:15.480]  whatsoever in anybody, I mean,
[01:04:15.480 --> 01:04:19.080]  I guess the hypothetical scenario would be,
[01:04:19.080 --> 01:04:24.360]  if let's say company A, let's say Apple and Google,
[01:04:24.360 --> 01:04:27.000]  and Apple thinks they've invented this thing
[01:04:27.000 --> 01:04:28.040]  that's more secure.
[01:04:28.040 --> 01:04:30.280]  Would they like to be able to use it
[01:04:30.280 --> 01:04:32.920]  as a competitive advantage in marketing
[01:04:32.920 --> 01:04:35.080]  against Google and Android?
[01:04:35.960 --> 01:04:39.400]  That's, you know, you can construe a scenario
[01:04:39.400 --> 01:04:41.080]  where they'd want to do it on their own,
[01:04:41.080 --> 01:04:43.720]  but working together is clearly in the interest
[01:04:43.720 --> 01:04:45.560]  of all these companies.
[01:04:45.560 --> 01:04:48.040]  And what it means is that you can use
[01:04:48.040 --> 01:04:50.280]  this passkey system that Apple's announced
[01:04:50.280 --> 01:04:51.880]  is gonna be built into all their systems.
[01:04:52.520 --> 01:04:58.200]  It's not just for stuff within the Apple ecosystem
[01:04:58.200 --> 01:05:03.000]  of apps on your iOS and Macs that come through
[01:05:03.000 --> 01:05:05.480]  the App Store and use Apple's APIs.
[01:05:06.200 --> 01:05:09.400]  It will be able to be used for stuff over the web
[01:05:10.040 --> 01:05:12.600]  that has nothing to do with Apple in particular.
[01:05:12.600 --> 01:05:14.920]  Yeah, and the exciting thing is, like,
[01:05:14.920 --> 01:05:16.600]  1Password, LastPass, Dashlane,
[01:05:16.600 --> 01:05:18.680]  they've all announced support for it as well.
[01:05:18.680 --> 01:05:20.920]  So theoretically, if you don't want to tie your keys
[01:05:20.920 --> 01:05:23.080]  to an iPhone or to a Windows machine
[01:05:23.080 --> 01:05:25.960]  or to an Android phone, you'd be able to tie them
[01:05:25.960 --> 01:05:29.320]  to a 1Password or LastPass account.
[01:05:29.320 --> 01:05:32.200]  And then you can choose horizontal or vertical lock-in.
[01:05:32.200 --> 01:05:33.560]  You don't have to be just one or the other.
[01:05:33.560 --> 01:05:35.960]  Right, which is a great, it's key.
[01:05:35.960 --> 01:05:37.640]  I know that, you know, there are people
[01:05:37.640 --> 01:05:40.840]  who are, you know, listen to my show
[01:05:40.840 --> 01:05:42.280]  and watch your YouTube channel
[01:05:42.280 --> 01:05:43.640]  and love Apple stuff the best,
[01:05:43.640 --> 01:05:47.160]  but who live in a cross-platform world professionally
[01:05:47.160 --> 01:05:49.800]  or by choice, you know, because they have a gaming PC
[01:05:49.800 --> 01:05:52.760]  or whatever, whether it's for work or for gaming
[01:05:52.760 --> 01:05:56.760]  or for whatever, they need cross-platform solutions
[01:05:56.760 --> 01:05:59.960]  to something like this, and PassKey absolutely
[01:05:59.960 --> 01:06:02.040]  is as cross-platform as it could be.
[01:06:02.040 --> 01:06:06.840]  You know, it's not like, to name the,
[01:06:07.800 --> 01:06:09.160]  just one example from mystery,
[01:06:09.160 --> 01:06:10.760]  it's not like Steve Jobs has promised
[01:06:10.760 --> 01:06:12.520]  that FaceTime would become an open standard.
[01:06:13.160 --> 01:06:14.760]  No, this really is an open standard,
[01:06:14.760 --> 01:06:16.360]  and that's where this work has come from.
[01:06:16.920 --> 01:06:18.040]  There is a bit of a downside.
[01:06:18.040 --> 01:06:20.360]  Like, some people will say they like simple passwords
[01:06:20.360 --> 01:06:22.120]  they can remember because if anything ever happens,
[01:06:22.120 --> 01:06:23.400]  they can go to their friend's computer
[01:06:23.400 --> 01:06:26.120]  and just, you know, type in the word password and log in.
[01:06:26.120 --> 01:06:27.480]  And as scary as that is for us,
[01:06:27.480 --> 01:06:29.080]  it's like comforting to them.
[01:06:29.080 --> 01:06:31.080]  And this does remove that because you don't know
[01:06:31.080 --> 01:06:33.240]  what the PassKey is, so if you lose your phone,
[01:06:33.240 --> 01:06:34.840]  you can't just go to your friend's computer
[01:06:34.840 --> 01:06:35.800]  and type it in.
[01:06:35.800 --> 01:06:37.720]  It's basically like if you lost your phone
[01:06:37.720 --> 01:06:39.960]  that has your two-factor authentication on it now.
[01:06:39.960 --> 01:06:40.280]  Right.
[01:06:40.280 --> 01:06:41.800]  Like, you should have, you should make sure,
[01:06:42.600 --> 01:06:44.200]  Apple is letting you set up, like,
[01:06:44.200 --> 01:06:46.120]  I could set you up as my friend contact,
[01:06:46.120 --> 01:06:48.360]  and then you'd get a code and you could send it to me.
[01:06:48.360 --> 01:06:50.520]  Or also, like, you might have a recovery account
[01:06:50.520 --> 01:06:52.600]  as a separate email account with a random name
[01:06:52.600 --> 01:06:54.760]  that nobody knows that's authorized to,
[01:06:55.400 --> 01:06:57.400]  that stores this kind of stuff for you.
[01:06:57.400 --> 01:06:59.880]  So recovery is going to be a little more complicated.
[01:06:59.880 --> 01:07:02.120]  Everyone is making much better systems to handle it,
[01:07:02.120 --> 01:07:04.120]  but it's no more complicated than recovery
[01:07:04.120 --> 01:07:05.800]  if you had two-factor going anyway.
[01:07:05.800 --> 01:07:08.440]  Yeah, but that's basically, that's where I was going,
[01:07:08.440 --> 01:07:11.560]  is the, okay, this sounds great, but what's the catch?
[01:07:11.560 --> 01:07:12.840]  There has to be a catch.
[01:07:12.840 --> 01:07:16.120]  And that sort of is the catch where,
[01:07:17.720 --> 01:07:19.080]  and I've mentioned this before,
[01:07:19.080 --> 01:07:21.480]  and I know Bruce Schneier himself,
[01:07:21.480 --> 01:07:24.120]  who's a terrific security expert, you know,
[01:07:24.120 --> 01:07:25.240]  has said this before too,
[01:07:25.240 --> 01:07:29.960]  that a lot of people frown upon the idea
[01:07:29.960 --> 01:07:32.360]  that some people just keep their passwords
[01:07:32.360 --> 01:07:35.080]  in like a notebook, like a paper and pen notebook.
[01:07:35.080 --> 01:07:37.240]  You know, my parents do this with some of their stuff.
[01:07:37.240 --> 01:07:39.560]  And they're like, oh my God, you know,
[01:07:39.560 --> 01:07:41.560]  because there's no encryption, obviously,
[01:07:41.560 --> 01:07:43.400]  and it's totally insecure.
[01:07:44.040 --> 01:07:47.000]  And I'm totally stealing this from Bruce Schneier,
[01:07:47.000 --> 01:07:49.640]  but human beings are actually very good
[01:07:49.640 --> 01:07:52.680]  at keeping physical things secret, you know?
[01:07:53.400 --> 01:07:55.400]  Obviously, somebody who breaks into your home,
[01:07:55.400 --> 01:07:56.680]  you're in a lot of trouble already
[01:07:56.680 --> 01:07:59.720]  if a thief is broken into your home, right?
[01:07:59.720 --> 01:08:00.760]  Yeah, like what's in your desk?
[01:08:01.800 --> 01:08:02.840]  What's in your bedroom?
[01:08:02.840 --> 01:08:04.840]  Right, but, you know, so yes,
[01:08:04.840 --> 01:08:07.480]  there is a theoretical risk to just writing down stuff
[01:08:07.480 --> 01:08:10.840]  on paper and keeping it in a known secure location,
[01:08:10.840 --> 01:08:13.080]  or not even secure, just a known, you know,
[01:08:13.080 --> 01:08:15.880]  I know that I keep it in this drawer on this desk
[01:08:16.600 --> 01:08:18.200]  where my iMac is set up.
[01:08:19.000 --> 01:08:23.000]  That's actually very secure and it can't leak online.
[01:08:23.000 --> 01:08:24.680]  There's all sorts of things that have happened
[01:08:24.680 --> 01:08:26.520]  that happened to digital stuff and you don't,
[01:08:27.080 --> 01:08:28.360]  you know, people are good at that.
[01:08:28.360 --> 01:08:30.120]  That's not as insecure as you think.
[01:08:30.120 --> 01:08:34.920]  There's a mutual friend, Dave Nanian, who does SuperDuper.
[01:08:34.920 --> 01:08:36.840]  Yeah, amazing, amazing app.
[01:08:36.840 --> 01:08:41.160]  I was doing a story on encryption and security
[01:08:41.160 --> 01:08:43.720]  and I talked to like, you know, Will Chronic on Twitter,
[01:08:44.280 --> 01:08:46.440]  brilliant info sec guy, talked to a bunch of them
[01:08:46.440 --> 01:08:48.760]  and everyone was telling me like how to lock down everything.
[01:08:48.760 --> 01:08:49.880]  Then I talked to Dave and he's like,
[01:08:49.880 --> 01:08:52.200]  yes, but you have to remember that, you know,
[01:08:52.200 --> 01:08:54.520]  like most people, their biggest threat
[01:08:54.520 --> 01:08:55.800]  isn't someone stealing their stuff,
[01:08:55.800 --> 01:08:58.600]  it's them losing access to their stuff.
[01:08:58.600 --> 01:09:00.440]  And that's why a lot of like fail safe
[01:09:00.440 --> 01:09:02.600]  has to be balanced with fail secure.
[01:09:02.600 --> 01:09:04.360]  And it's like, we seem now like we're in a world
[01:09:04.360 --> 01:09:05.880]  where we want to lock down everything,
[01:09:05.880 --> 01:09:08.840]  but still to your point about like that list of passwords,
[01:09:08.840 --> 01:09:11.240]  if anything is more important to you
[01:09:11.240 --> 01:09:12.840]  or so important to you that it is better
[01:09:12.840 --> 01:09:15.160]  that it is stolen than it is lost.
[01:09:15.160 --> 01:09:17.000]  Like for example, for some people, it's baby pictures.
[01:09:17.000 --> 01:09:19.000]  They have very little value to anybody else,
[01:09:19.000 --> 01:09:21.000]  wedding pictures, some documents
[01:09:21.000 --> 01:09:22.920]  that have no like financial or health thing,
[01:09:22.920 --> 01:09:25.160]  but like they're just family memories or artifacts.
[01:09:25.160 --> 01:09:28.440]  Any of that stuff, it is far less likely
[01:09:28.440 --> 01:09:30.040]  that they'll be stolen and far more likely
[01:09:30.040 --> 01:09:31.480]  that you'll lose access to them.
[01:09:31.480 --> 01:09:33.080]  So that's the kind of stuff that you don't need
[01:09:33.080 --> 01:09:35.000]  to be so security paranoid about.
[01:09:35.000 --> 01:09:36.600]  You just want to have like, make sure
[01:09:36.600 --> 01:09:38.360]  that you never lose the access to them.
[01:09:38.360 --> 01:09:40.040]  So it's better to have a balanced approach.
[01:09:40.040 --> 01:09:43.240]  Right, but then that sort of seems to be the weakness here
[01:09:43.240 --> 01:09:46.440]  is what happens, you know, and a house fire,
[01:09:46.440 --> 01:09:51.400]  you know, or other natural disaster is the,
[01:09:51.960 --> 01:09:54.440]  you know, obviously and thankfully rare,
[01:09:54.440 --> 01:09:56.920]  but you know, it certainly happens, right?
[01:09:56.920 --> 01:10:01.160]  There's, you know, now annual wildfires in California.
[01:10:01.160 --> 01:10:03.240]  A lot of people around the world live places
[01:10:03.240 --> 01:10:06.200]  where, you know, hurricanes hit every decade
[01:10:06.200 --> 01:10:08.440]  and tornadoes might hit or something like that.
[01:10:08.440 --> 01:10:12.520]  It's, you know, it is possible for, you know,
[01:10:12.520 --> 01:10:14.120]  something to happen and every device
[01:10:14.120 --> 01:10:16.120]  in your house is destroyed.
[01:10:17.000 --> 01:10:20.040]  What happens, like if you just, you have a one Mac,
[01:10:20.040 --> 01:10:23.640]  one iPad, one iPhone, and they're all destroyed,
[01:10:23.640 --> 01:10:24.760]  how do you get back in?
[01:10:24.760 --> 01:10:28.760]  And the answer is it's complicated, right?
[01:10:28.760 --> 01:10:31.160]  Now, of course, if all your passwords
[01:10:31.160 --> 01:10:34.120]  were written in a paper notebook and your house burns down,
[01:10:34.120 --> 01:10:35.960]  that notebook is probably gone too, you know?
[01:10:35.960 --> 01:10:40.040]  So it's not like the password world was a, you know,
[01:10:40.040 --> 01:10:43.080]  panacea for the, what happens if a tornado
[01:10:43.080 --> 01:10:44.040]  destroys your house.
[01:10:44.040 --> 01:10:46.920]  But I put a copy in my safety deposit box,
[01:10:46.920 --> 01:10:48.520]  oh, but the key was in the house and it melted,
[01:10:48.520 --> 01:10:50.200]  like you can just drive yourself nuts on all this stuff.
[01:10:50.200 --> 01:10:53.400]  Right, but there is, you know, so one of the,
[01:10:53.400 --> 01:10:58.200]  so one of the answers to this is that you can set up,
[01:10:58.200 --> 01:10:59.160]  what does Apple call them?
[01:10:59.160 --> 01:11:01.640]  Not necessarily emergency contacts, but like.
[01:11:03.000 --> 01:11:04.280]  Oh, yeah, I just mentioned it,
[01:11:04.280 --> 01:11:05.400]  and now it's gone out of my head again.
[01:11:06.600 --> 01:11:09.400]  Is it a legacy contact, then does something contact?
[01:11:09.400 --> 01:11:11.640]  Right, and the legacy contact thing too,
[01:11:12.200 --> 01:11:15.720]  and again, we as human beings tend to-
[01:11:15.720 --> 01:11:16.520]  Recovery contact.
[01:11:16.520 --> 01:11:17.720]  Yeah, recovery contact.
[01:11:17.720 --> 01:11:18.920]  So you can set that up.
[01:11:19.480 --> 01:11:21.960]  Now, it doesn't help if your recovery contact
[01:11:21.960 --> 01:11:25.480]  is your spouse or somebody else who lives with you
[01:11:26.200 --> 01:11:29.880]  and all of their stuff gets destroyed in the same disaster,
[01:11:29.880 --> 01:11:31.880]  but it is something that could happen, you know.
[01:11:31.880 --> 01:11:34.920]  So for example, if you're traveling and you're,
[01:11:34.920 --> 01:11:38.520]  all of your devices are in one bag and the bag gets stolen,
[01:11:38.520 --> 01:11:42.600]  a recovery contact whose devices
[01:11:42.600 --> 01:11:45.320]  are still in their possession could definitely help you.
[01:11:45.320 --> 01:11:45.800]  What else?
[01:11:45.800 --> 01:11:48.360]  What else can you do for the situation of,
[01:11:48.360 --> 01:11:50.280]  okay, I'm all in on passkey,
[01:11:50.280 --> 01:11:53.240]  I'm gonna move as many of my accounts to this as possible,
[01:11:53.240 --> 01:11:56.040]  and I'm in the Apple ecosystem,
[01:11:56.040 --> 01:11:58.280]  I'll do it through the keychain and iCloud.
[01:11:58.920 --> 01:12:00.840]  What happens if you lose all your devices?
[01:12:00.840 --> 01:12:01.960]  I mean, what else is there?
[01:12:03.240 --> 01:12:04.360]  You can still go to, like right now,
[01:12:04.360 --> 01:12:06.360]  if you lose your two-factor for your Apple account,
[01:12:06.360 --> 01:12:08.200]  you can go, and as long as you can prove ownership,
[01:12:09.160 --> 01:12:11.400]  it's a long and laborious process because it has to be,
[01:12:11.400 --> 01:12:13.800]  because the chance of social engineering is so high,
[01:12:13.800 --> 01:12:16.840]  but you can go to AppleCare and Apple Store,
[01:12:16.840 --> 01:12:18.440]  a place like that, prove ownership,
[01:12:18.440 --> 01:12:20.280]  and then they'll do account recovery for you.
[01:12:20.280 --> 01:12:22.120]  So there's that, so there are, you know,
[01:12:22.120 --> 01:12:25.400]  and I think if you balance this all out,
[01:12:25.400 --> 01:12:28.920]  it's worth being aware of what happens if the,
[01:12:28.920 --> 01:12:33.160]  in the worst case scenario or a near worst case scenario,
[01:12:33.160 --> 01:12:35.160]  and being aware of what you should do.
[01:12:35.160 --> 01:12:37.000]  You know, it's like figuring out,
[01:12:37.000 --> 01:12:40.520]  hey, you should know what to do with your homeowner's insurance
[01:12:40.520 --> 01:12:42.600]  before you need it, you know, and have an idea.
[01:12:42.600 --> 01:12:45.000]  By the way, this is the reason Apple moved to the new forum,
[01:12:45.000 --> 01:12:46.920]  like before everything was completely locked
[01:12:46.920 --> 01:12:49.560]  and you got a recovery key for your iCloud account,
[01:12:49.560 --> 01:12:51.320]  but people just lost those all the time.
[01:12:51.320 --> 01:12:52.920]  Apple literally could not get their,
[01:12:52.920 --> 01:12:54.680]  I know people complain that iCloud backups
[01:12:54.680 --> 01:12:56.360]  aren't as secure as they want them to be,
[01:12:56.360 --> 01:12:58.840]  like they're not zero knowledge, Apple can unlock them,
[01:12:58.840 --> 01:12:59.720]  but that's the reason,
[01:12:59.720 --> 01:13:01.640]  is that when Apple looked at the support requests
[01:13:01.640 --> 01:13:04.200]  they were getting in, the harm wasn't subpoenas,
[01:13:04.200 --> 01:13:06.680]  the harm was people losing access by,
[01:13:06.680 --> 01:13:10.120]  like exponentially more people just lost their recovery keys
[01:13:10.120 --> 01:13:12.440]  and never had legal demands on that stuff
[01:13:12.440 --> 01:13:13.560]  and it just wasn't worth it for them.
[01:13:13.560 --> 01:13:14.920]  So they created the new system
[01:13:14.920 --> 01:13:15.960]  where they could help you recover.
[01:13:15.960 --> 01:13:18.920]  PAUL Right, and this is a recurring topic
[01:13:18.920 --> 01:13:21.240]  between me and you, both on the show
[01:13:21.240 --> 01:13:24.440]  and in our, you know, personal iChat,
[01:13:24.440 --> 01:13:27.080]  I call it iChat, iMessage over the years,
[01:13:27.080 --> 01:13:29.880]  but something that you and I have talked about
[01:13:29.880 --> 01:13:32.520]  at great length is this topic of,
[01:13:34.520 --> 01:13:36.440]  you know, and again, it circles back
[01:13:36.440 --> 01:13:38.040]  to the law enforcement angle
[01:13:38.040 --> 01:13:40.200]  we talked about earlier in the show
[01:13:40.200 --> 01:13:47.240]  of your iCloud backup is not end-to-end encrypted.
[01:13:47.240 --> 01:13:49.720]  SIMON It's encrypted, but Apple has the key.
[01:13:49.720 --> 01:13:50.120]  PAUL Right.
[01:13:50.120 --> 01:13:51.880]  SIMON People get upset when they think it's not end-to-end.
[01:13:51.880 --> 01:13:54.120]  It is encrypted, it's just Apple can unencrypt it.
[01:13:54.120 --> 01:13:59.640]  PAUL Right, it is, for lack of a better analogy,
[01:14:00.200 --> 01:14:02.680]  if you can imagine a secure envelope
[01:14:03.560 --> 01:14:08.200]  and in transit from your device to iCloud,
[01:14:08.200 --> 01:14:11.800]  it is in a secure envelope that is cryptographically secure.
[01:14:11.800 --> 01:14:14.760]  Almost everything we do on the internet these days
[01:14:14.760 --> 01:14:17.080]  is encrypted over the wire.
[01:14:17.080 --> 01:14:20.120]  You know, there are very few websites I visit anymore
[01:14:20.120 --> 01:14:23.000]  that are HTTP without the S,
[01:14:23.000 --> 01:14:26.280]  and our email, when your mail client
[01:14:26.840 --> 01:14:30.840]  talks to your IMAP server, that's all over SSL
[01:14:30.840 --> 01:14:32.760]  in almost all cases these days.
[01:14:32.760 --> 01:14:35.320]  So everything over the wire tends to be encrypted.
[01:14:35.320 --> 01:14:36.520]  It's in a secure envelope.
[01:14:36.520 --> 01:14:39.720]  But in most cases, if it's not end-to-end encrypted,
[01:14:39.720 --> 01:14:41.080]  it gets to the server.
[01:14:41.080 --> 01:14:44.200]  The server can obviously decrypt it to be useful.
[01:14:44.200 --> 01:14:46.200]  Now it's not encrypted anymore.
[01:14:46.200 --> 01:14:46.760]  And then when...
[01:14:46.760 --> 01:14:47.800]  SIMON Yeah, like to show you your photos
[01:14:47.800 --> 01:14:48.920]  on a web browser, for example.
[01:14:48.920 --> 01:14:51.480]  PAUL Right, but when they write whatever it is to disk,
[01:14:51.480 --> 01:14:53.560]  now email's different, so let's forget about email.
[01:14:53.560 --> 01:14:57.080]  But your iCloud backup, your iCloud backup
[01:14:57.080 --> 01:15:01.080]  goes from your device to iCloud encrypted.
[01:15:01.080 --> 01:15:03.800]  The server gets it, decrypts it,
[01:15:03.800 --> 01:15:07.800]  and then writes it to their disks in the cloud
[01:15:07.800 --> 01:15:09.080]  in an encrypted format.
[01:15:09.080 --> 01:15:09.800]  SIMON Yeah.
[01:15:09.800 --> 01:15:13.720]  PAUL But they obviously can decrypt it,
[01:15:13.720 --> 01:15:15.960]  because when the backup comes back to you,
[01:15:15.960 --> 01:15:17.880]  you're like, I would like to restore this device
[01:15:17.880 --> 01:15:18.600]  to a backup.
[01:15:19.560 --> 01:15:21.880]  They take the backup, they decrypt it,
[01:15:22.520 --> 01:15:23.800]  then they send it, you know...
[01:15:23.800 --> 01:15:25.320]  SIMON Well, I think that one,
[01:15:25.320 --> 01:15:27.720]  technically, that's using your key to decrypt it,
[01:15:27.720 --> 01:15:28.600]  because it's coming to your device,
[01:15:28.600 --> 01:15:29.640]  and you've logged into that device,
[01:15:29.640 --> 01:15:30.760]  and I think that's using your key.
[01:15:30.760 --> 01:15:31.320]  PAUL Oh, maybe.
[01:15:31.320 --> 01:15:32.040]  SIMON But they have a...
[01:15:32.040 --> 01:15:33.160]  Like, the way my understanding is,
[01:15:33.160 --> 01:15:34.680]  it's almost like you live in a secure building
[01:15:34.680 --> 01:15:36.440]  in New York City, or you're in a hotel,
[01:15:36.440 --> 01:15:39.800]  and if you lose your door dingus,
[01:15:39.800 --> 01:15:41.400]  they can come and unlock it for you,
[01:15:41.400 --> 01:15:43.240]  but then also, like, the cops can force them.
[01:15:44.440 --> 01:15:45.880]  But, like, they don't get your current activity.
[01:15:45.880 --> 01:15:48.040]  Like, they don't get anything that is dynamic.
[01:15:48.040 --> 01:15:49.160]  They only get what's at rest.
[01:15:49.160 --> 01:15:51.240]  They only guess what's in the room at that time.
[01:15:51.240 --> 01:15:51.880]  PAUL Right.
[01:15:51.880 --> 01:15:55.240]  Encrypted in transit and encrypted in storage
[01:15:55.240 --> 01:15:58.280]  is good, but that's not end-to-end encrypted.
[01:15:58.280 --> 01:16:01.640]  End-to-end encrypted, E2E, is a way of talking
[01:16:01.640 --> 01:16:06.200]  about something where the only keys that can decrypt it
[01:16:06.840 --> 01:16:09.880]  are at the end points, meaning your devices,
[01:16:09.880 --> 01:16:13.160]  and there is literally nothing that can be done
[01:16:13.160 --> 01:16:16.840]  in the middle to decrypt it, other than the brute force
[01:16:16.840 --> 01:16:19.320]  and the mathematics behind the brute force
[01:16:19.320 --> 01:16:21.800]  of decrypting it, and the current state of computing
[01:16:22.760 --> 01:16:26.440]  is, you know, thousands to millions or longer
[01:16:26.440 --> 01:16:28.840]  of years of compute time to do it.
[01:16:28.840 --> 01:16:33.720]  So, effectively, undecryptable.
[01:16:34.520 --> 01:16:37.160]  That's end-to-end encryption, and iCloud backups
[01:16:37.160 --> 01:16:41.000]  are not end-to-end decrypted, and there's a lot of people
[01:16:41.000 --> 01:16:43.560]  who think that's solely because of pressure
[01:16:43.560 --> 01:16:45.640]  from law enforcement, and law enforcement,
[01:16:45.640 --> 01:16:49.480]  to be clear, is very happy about the current state of affairs.
[01:16:49.480 --> 01:16:51.960]  Maybe they'd, you know, like, even easier access,
[01:16:51.960 --> 01:16:55.000]  but the fact that they can go to Apple
[01:16:55.000 --> 01:16:58.360]  or all sorts of other providers with a warrant
[01:16:58.360 --> 01:17:00.200]  and have it looked at and say, okay, yes,
[01:17:00.200 --> 01:17:03.080]  this is a legit law enforcement request,
[01:17:03.080 --> 01:17:07.400]  and they can turn over the contents of your iCloud backup
[01:17:07.400 --> 01:17:09.960]  to law enforcement, because they also do have keys.
[01:17:09.960 --> 01:17:11.240]  It's a point of contention.
[01:17:11.240 --> 01:17:12.840]  It's something to be aware of, right?
[01:17:12.840 --> 01:17:16.440]  And, but my understanding, and I think your understanding
[01:17:17.720 --> 01:17:22.280]  from informed little birdies, for lack of a better word,
[01:17:22.280 --> 01:17:26.920]  is that this is not as much about, yes, law enforcement
[01:17:26.920 --> 01:17:29.480]  does like the way things are and would not like Apple
[01:17:29.480 --> 01:17:32.680]  to switch to end-to-end encrypted iCloud backups
[01:17:32.680 --> 01:17:38.920]  that cannot be unlocked by Apple for law enforcement purposes,
[01:17:38.920 --> 01:17:41.640]  where, you know, the only thing that Apple could turn over
[01:17:41.640 --> 01:17:44.920]  is an encrypted blob that they do not have the key to,
[01:17:44.920 --> 01:17:47.800]  not that by policy they refuse to use the key,
[01:17:47.800 --> 01:17:49.240]  but they literally don't have it.
[01:17:49.880 --> 01:17:51.320]  Law enforcement would not like that,
[01:17:51.320 --> 01:17:55.160]  but that, it's worth mentioning, it is a reason,
[01:17:55.160 --> 01:17:57.560]  but that the primary reason iCloud backups
[01:17:57.560 --> 01:18:00.760]  are not end-to-end encrypted is that, yes,
[01:18:00.760 --> 01:18:02.280]  exactly what you said a couple minutes ago,
[01:18:02.280 --> 01:18:04.440]  that Apple looked at the actual support load
[01:18:04.440 --> 01:18:08.760]  that their tech support, AppleCare and stores, face,
[01:18:08.760 --> 01:18:12.520]  and it is just, every single day,
[01:18:12.520 --> 01:18:15.240]  thousands and thousands of Apple customers come in
[01:18:15.240 --> 01:18:18.280]  and say, I'm locked out of my stuff, I cannot get in,
[01:18:18.280 --> 01:18:18.920]  what do I do?
[01:18:19.720 --> 01:18:22.120]  We've talked about this, I know, on the show before.
[01:18:22.120 --> 01:18:26.600]  I, what I would like to see, I guess, is,
[01:18:26.600 --> 01:18:29.160]  and I, but I also realize the downside of this,
[01:18:29.160 --> 01:18:31.080]  it's complexity, where all of a sudden,
[01:18:31.720 --> 01:18:33.640]  everybody's iCloud backup is the same,
[01:18:34.840 --> 01:18:36.840]  is, has weaknesses in the current world,
[01:18:36.840 --> 01:18:38.680]  because it's not end-to-end encrypted,
[01:18:38.680 --> 01:18:40.200]  but at least it's a simple story,
[01:18:41.000 --> 01:18:45.240]  whereas making it optional to make your iCloud backup
[01:18:45.240 --> 01:18:49.160]  end-to-end encrypted, so that Apple cannot decrypt it,
[01:18:49.160 --> 01:18:51.560]  cannot turn it over to law enforcement,
[01:18:51.560 --> 01:18:54.360]  even in the face of a warrant, has nothing to do it,
[01:18:54.360 --> 01:18:58.040]  but also, therefore, inherently, cannot help you
[01:18:58.040 --> 01:19:01.880]  if you lock yourself out and forget all of the ways
[01:19:01.880 --> 01:19:04.680]  to get in, you know.
[01:19:04.680 --> 01:19:06.120]  They literally can't, you just gotta,
[01:19:06.120 --> 01:19:07.400]  like, by pressing that button, you say,
[01:19:07.400 --> 01:19:09.560]  I'm a grown-ass adult, and I value encryption
[01:19:09.560 --> 01:19:13.080]  over a recoverability, and I'm doing this to myself.
[01:19:13.080 --> 01:19:17.080]  Right, you know, that scenario would have to have
[01:19:17.080 --> 01:19:20.520]  some outs in some way, right, like, you know,
[01:19:20.520 --> 01:19:24.200]  would it be that your recovery contact could do it?
[01:19:24.200 --> 01:19:28.520]  But then that opens up the scenario where law enforcement
[01:19:28.520 --> 01:19:30.920]  could go to your recovery contact and say,
[01:19:30.920 --> 01:19:34.280]  okay, here's our warrant, please, you know,
[01:19:34.280 --> 01:19:38.680]  use your recovery contact powers to give us access to,
[01:19:38.680 --> 01:19:40.440]  you know, this criminal suspect,
[01:19:40.440 --> 01:19:42.600]  Rene Ritchie's iCloud account.
[01:19:42.600 --> 01:19:43.100]  Yep.
[01:19:43.720 --> 01:19:45.080]  The whole thing is, like, it really depends,
[01:19:45.080 --> 01:19:47.320]  again, it depends on your threat model.
[01:19:47.320 --> 01:19:49.000]  For the vast majority of people,
[01:19:49.000 --> 01:19:50.760]  it's not even a consideration, like,
[01:19:50.760 --> 01:19:53.400]  the chance of loss is just so much more massive
[01:19:53.400 --> 01:19:55.480]  than the chance of subpoena or even, like,
[01:19:55.480 --> 01:19:57.240]  of hacking or, like, criminal activity.
[01:19:57.240 --> 01:19:59.640]  It's the reason why, like, my laptops are,
[01:19:59.640 --> 01:20:02.440]  like, I encrypt the drive, I use FileVault on my laptops,
[01:20:02.440 --> 01:20:04.280]  just because it's a laptop and it could get stolen
[01:20:04.280 --> 01:20:06.200]  or it could leave, but anything that I have,
[01:20:06.200 --> 01:20:07.960]  like, my family photos, they're on a drive
[01:20:07.960 --> 01:20:10.440]  that's not encrypted at all, because, you know,
[01:20:10.440 --> 01:20:12.840]  drives fail, and if a drive is encrypted and it fails,
[01:20:12.840 --> 01:20:15.000]  you can't even pay those exorbitant recovery companies
[01:20:15.000 --> 01:20:15.960]  to get them back for you.
[01:20:15.960 --> 01:20:17.640]  That stuff is just gone.
[01:20:17.640 --> 01:20:20.280]  Well, they could get back the ones and zeroes
[01:20:20.280 --> 01:20:21.560]  of the encrypted blob.
[01:20:21.560 --> 01:20:23.800]  Yes, but it would be, yeah, it would be useless.
[01:20:23.800 --> 01:20:24.440]  Right, it just doesn't...
[01:20:24.440 --> 01:20:27.080]  So, like, yeah, you have to, like,
[01:20:27.080 --> 01:20:28.600]  part of this is, like, I think that people
[01:20:28.600 --> 01:20:31.080]  fetishized encryption and fetishized security,
[01:20:31.080 --> 01:20:32.680]  and because we live in a world that has a bunch
[01:20:32.680 --> 01:20:34.200]  of terrible stories in the media,
[01:20:34.200 --> 01:20:35.400]  that everybody thought everything,
[01:20:35.400 --> 01:20:37.320]  and infosec people really do believe everything
[01:20:37.320 --> 01:20:38.680]  should be encrypted all the time,
[01:20:38.680 --> 01:20:40.920]  and backup people think that you're out of your mind
[01:20:40.920 --> 01:20:43.320]  if you encrypt anything important to you at all, ever,
[01:20:43.320 --> 01:20:44.680]  and I think there's, like, a middle ground
[01:20:44.680 --> 01:20:46.440]  and maybe Apple can find that.
[01:20:46.440 --> 01:20:47.800]  Like, PassKey works great.
[01:20:47.800 --> 01:20:49.960]  PGP never took off for email because it was just
[01:20:49.960 --> 01:20:51.880]  too damn hard for anybody to manage a system
[01:20:51.880 --> 01:20:53.960]  very similar to this, and they figured out a way
[01:20:53.960 --> 01:20:57.960]  to make, basically, SSH-style private public key system
[01:20:58.600 --> 01:21:01.560]  easy to work, and I'm hoping they can figure out a way
[01:21:01.560 --> 01:21:03.640]  where you can classify things as, like,
[01:21:03.640 --> 01:21:05.880]  please encrypt this and please make this recoverable.
[01:21:07.160 --> 01:21:11.960]  I somehow managed to graduate from a reputable university
[01:21:11.960 --> 01:21:15.480]  with a computer science degree and was, I think,
[01:21:15.480 --> 01:21:19.640]  one course short of graduating with a math minor.
[01:21:19.640 --> 01:21:21.880]  I think it was, I used to have it memorized.
[01:21:21.880 --> 01:21:25.240]  It was probability and statistic two,
[01:21:25.240 --> 01:21:27.560]  and oh, there was another, I think there were two classes,
[01:21:27.560 --> 01:21:30.760]  but PropStat one was so hard for me that I was like,
[01:21:30.760 --> 01:21:32.280]  forget it, I don't need a math minor.
[01:21:32.280 --> 01:21:33.640]  I don't care if it's one class.
[01:21:33.640 --> 01:21:38.360]  I'm never taking this again, and I care about privacy
[01:21:38.360 --> 01:21:42.600]  and security, and I did have and used PGP
[01:21:42.600 --> 01:21:43.720]  for my email for a while.
[01:21:43.720 --> 01:21:45.160]  I never encrypted every message
[01:21:45.160 --> 01:21:46.360]  because it never was necessary,
[01:21:46.360 --> 01:21:50.440]  but even with that background of somewhat technical expertise,
[01:21:50.440 --> 01:21:53.400]  it was still so convoluted and confusing
[01:21:53.400 --> 01:21:56.760]  that I'd go so far between actually using it
[01:21:56.760 --> 01:21:59.320]  where somebody would then send me an encrypted email,
[01:21:59.320 --> 01:22:01.720]  and it would be like starting all over from scratch.
[01:22:01.720 --> 01:22:03.000]  How the hell do I decrypt this?
[01:22:03.000 --> 01:22:03.640]  What do I do?
[01:22:03.640 --> 01:22:05.960]  It was just not a good match.
[01:22:05.960 --> 01:22:11.400]  It's better to just treat email like postcards.
[01:22:11.400 --> 01:22:13.640]  They're not even in an envelope.
[01:22:13.640 --> 01:22:15.640]  There's no envelope to steam open.
[01:22:15.640 --> 01:22:19.720]  Just assume every email you send and receive
[01:22:19.720 --> 01:22:21.960]  is like a postcard going through the mail
[01:22:21.960 --> 01:22:24.600]  where the message is just written on the back of the card,
[01:22:24.600 --> 01:22:27.080]  and you can use it.
[01:22:27.080 --> 01:22:30.520]  I use email every day, thousands of messages every month,
[01:22:30.520 --> 01:22:31.020]  probably.
[01:22:31.560 --> 01:22:34.280]  Just assume it has the security of a postcard
[01:22:34.280 --> 01:22:35.320]  and be done with it.
[01:22:35.320 --> 01:22:36.600]  Well, it's going to be like Panzerino
[01:22:36.600 --> 01:22:40.040]  and post your damn PGP key and all your social buttons.
[01:22:40.040 --> 01:22:41.640]  I do not use it anymore.
[01:22:41.640 --> 01:22:44.920]  And I used to publicly advertise my PGP key
[01:22:44.920 --> 01:22:47.560]  for daring Fireball readers to contact me securely.
[01:22:47.560 --> 01:22:50.120]  And now what I tell them to do is use Signal.
[01:22:50.120 --> 01:22:51.560]  And it's a much better...
[01:22:51.560 --> 01:22:54.360]  Because Signal, it's like the opposite of email
[01:22:54.360 --> 01:22:59.800]  where Signal is only and can only be cryptographically secure.
[01:22:59.800 --> 01:23:01.160]  On this point, though, this is great.
[01:23:01.160 --> 01:23:03.240]  It leads to another point that I wrote about
[01:23:03.240 --> 01:23:07.080]  just the other day is your health data, right?
[01:23:07.080 --> 01:23:10.200]  Where, again, circling back to this Supreme Court decision
[01:23:10.200 --> 01:23:13.080]  in the United States that removes, overturns,
[01:23:13.080 --> 01:23:16.760]  the Roe v. Wade that considered reproductive rights
[01:23:16.760 --> 01:23:19.720]  and abortion a constitutional right in the United States.
[01:23:19.720 --> 01:23:21.640]  And now it's literally overnight.
[01:23:21.640 --> 01:23:24.120]  There were states here that had laws set up
[01:23:24.120 --> 01:23:26.840]  to take up trigger laws that took effect
[01:23:26.840 --> 01:23:30.200]  as soon as it was overturned to criminalize abortion.
[01:23:30.200 --> 01:23:32.760]  Now people are worried, and this is, again,
[01:23:32.760 --> 01:23:33.960]  not a hypothetical scenario.
[01:23:33.960 --> 01:23:36.440]  The state of Texas, which is my understanding
[01:23:36.440 --> 01:23:40.360]  from having spoken to people in Texas is it's a large state,
[01:23:40.360 --> 01:23:45.880]  has a law that it allows people to collect $10,000 bounties
[01:23:45.880 --> 01:23:49.240]  for reporting people they know of to get abortions
[01:23:49.240 --> 01:23:50.760]  that are now illegal in the state.
[01:23:50.760 --> 01:23:53.000]  It's very serious high stakes.
[01:23:53.000 --> 01:23:54.520]  People are concerned about their...
[01:23:54.520 --> 01:23:57.400]  Women are concerned about period tracking,
[01:23:57.400 --> 01:24:00.520]  period tracking menstrual cycle data
[01:24:00.520 --> 01:24:03.240]  that they store in their computer devices
[01:24:03.240 --> 01:24:05.000]  for very good reason.
[01:24:05.000 --> 01:24:08.680]  Again, anybody who was concerned about this a year ago,
[01:24:08.680 --> 01:24:12.200]  two years ago, before this decision in the United States
[01:24:12.200 --> 01:24:15.000]  maybe was more on the hypothetical side,
[01:24:15.000 --> 01:24:17.800]  but the way that state laws have been going here,
[01:24:17.800 --> 01:24:19.880]  it hasn't been hypothetical for a while.
[01:24:19.880 --> 01:24:22.840]  Now it's the law of the land.
[01:24:24.120 --> 01:24:26.040]  People, women are rightly concerned.
[01:24:26.040 --> 01:24:26.920]  What do you do?
[01:24:26.920 --> 01:24:27.960]  What do you worry about?
[01:24:27.960 --> 01:24:29.160]  And one of the things I mentioned,
[01:24:29.160 --> 01:24:32.360]  and I really think it's worth mentioning in articles
[01:24:32.360 --> 01:24:36.440]  that bring this up is that your health data from Apple,
[01:24:36.440 --> 01:24:38.840]  your iCloud health data, whatever you want to call it,
[01:24:38.840 --> 01:24:42.520]  but using the health app on your iPhone is...
[01:24:42.520 --> 01:24:46.680]  If the apps you use or the menstrual cycle tracking
[01:24:46.680 --> 01:24:48.520]  is now a built-in feature of health,
[01:24:48.520 --> 01:24:51.320]  you can do it just using the health app.
[01:24:51.320 --> 01:24:52.520]  You don't need a third-party app,
[01:24:52.520 --> 01:24:55.320]  but if the app only stores the data in health,
[01:24:55.320 --> 01:24:59.000]  your data is end-to-end encrypted.
[01:24:59.000 --> 01:25:01.080]  But here's the important clause
[01:25:01.080 --> 01:25:03.560]  that I had to add to my article after posting it.
[01:25:03.560 --> 01:25:06.760]  If your iCloud account
[01:25:06.760 --> 01:25:09.320]  has two-factor authentication turned on.
[01:25:10.200 --> 01:25:14.360]  So if your iCloud account is two-factor authentication,
[01:25:14.360 --> 01:25:16.360]  which it should be, in my opinion,
[01:25:16.360 --> 01:25:18.120]  I don't think there's anybody who shouldn't be using
[01:25:18.120 --> 01:25:20.680]  two-factor authentication for their iCloud account,
[01:25:21.240 --> 01:25:23.960]  but it's not mandatory yet.
[01:25:23.960 --> 01:25:25.640]  I'd love to know the percentage
[01:25:25.640 --> 01:25:27.640]  because there's so many features now,
[01:25:27.640 --> 01:25:30.840]  and Apple is very gently steering everybody
[01:25:30.840 --> 01:25:33.240]  who doesn't have it on to enable it,
[01:25:33.240 --> 01:25:35.080]  but it is important to mention it.
[01:25:35.080 --> 01:25:37.560]  If you have two-factor on your iCloud,
[01:25:37.560 --> 01:25:42.600]  your health data is only decrypted on your devices,
[01:25:42.600 --> 01:25:45.960]  and yes, it does go through iCloud to sync
[01:25:45.960 --> 01:25:47.480]  to your other devices,
[01:25:48.760 --> 01:25:52.200]  but that data on iCloud
[01:25:52.200 --> 01:25:55.240]  in Apple servers is end-to-end encrypted,
[01:25:55.240 --> 01:25:57.640]  and Apple itself cannot see it,
[01:25:57.640 --> 01:26:01.080]  could never turn it over at the request of law enforcement.
[01:26:01.080 --> 01:26:02.760]  But only if your iCloud account
[01:26:02.760 --> 01:26:04.360]  is using two-factor authentication.
[01:26:04.360 --> 01:26:07.480]  If your iCloud account is not using two-factor authentication,
[01:26:07.480 --> 01:26:09.320]  your health data still syncs,
[01:26:09.320 --> 01:26:12.680]  but it's part of your regular iCloud backup,
[01:26:12.680 --> 01:26:15.800]  which we just got done saying is not end-to-end encrypted,
[01:26:15.800 --> 01:26:18.440]  and therefore could be turned over
[01:26:18.440 --> 01:26:20.840]  to law enforcement and a faceable warrant.
[01:26:20.840 --> 01:26:22.760]  Yeah, I think for a while,
[01:26:22.760 --> 01:26:24.120]  you couldn't sync health data at all.
[01:26:24.120 --> 01:26:25.400]  Apple just left it on device,
[01:26:25.400 --> 01:26:26.280]  and people would get angry
[01:26:26.280 --> 01:26:27.720]  because they'd get a new watch or they'd get a new phone,
[01:26:27.720 --> 01:26:29.720]  and they wouldn't bring their health data with them,
[01:26:29.720 --> 01:26:31.320]  and they ended up building the same thing
[01:26:31.320 --> 01:26:32.680]  for a lot of the other more personal data.
[01:26:32.680 --> 01:26:35.320]  They built that whole secure cloud kit system
[01:26:35.320 --> 01:26:38.520]  to move it in a way that was more locked down
[01:26:38.520 --> 01:26:40.120]  than all of your standard data,
[01:26:40.120 --> 01:26:42.280]  and they've been slowly building out that process as well
[01:26:42.280 --> 01:26:43.160]  till they felt comfortable,
[01:26:43.160 --> 01:26:44.040]  not only syncing it,
[01:26:44.040 --> 01:26:46.440]  but providing new features like share your health data
[01:26:46.440 --> 01:26:47.880]  and things like that.
[01:26:47.880 --> 01:26:49.800]  Yeah, right, but that's a good point.
[01:26:49.800 --> 01:26:54.040]  Yeah, right, but that stuff is from the ground up
[01:26:54.040 --> 01:26:56.760]  designed to be secure in this way.
[01:26:57.640 --> 01:26:58.280]  And it has to be,
[01:26:58.280 --> 01:26:59.720]  because there's all sorts of HIPAA requirements
[01:26:59.720 --> 01:27:01.000]  around a lot of it as well.
[01:27:01.000 --> 01:27:05.800]  There are period tracking apps that don't use health,
[01:27:05.800 --> 01:27:07.160]  whether because they're web-based
[01:27:07.160 --> 01:27:08.440]  or they're cross-platform
[01:27:08.440 --> 01:27:11.240]  and you have Android components or siblings,
[01:27:11.240 --> 01:27:12.840]  whatever you want to call them too,
[01:27:12.840 --> 01:27:15.400]  or just for the way the developer chose to do it,
[01:27:15.400 --> 01:27:17.160]  store the information on their own,
[01:27:17.160 --> 01:27:20.280]  and you should, like anything health-related,
[01:27:20.280 --> 01:27:22.760]  you should make yourself as aware as possible
[01:27:22.760 --> 01:27:25.880]  of the company or provider's policies,
[01:27:25.880 --> 01:27:27.880]  privacy policies regarding that data.
[01:27:27.880 --> 01:27:30.760]  But if the answer is they're doing it
[01:27:30.760 --> 01:27:33.640]  through the health app on iOS,
[01:27:33.640 --> 01:27:36.840]  it is secure if your account uses two-factor authentication.
[01:27:38.200 --> 01:27:39.160]  I would like to see,
[01:27:41.320 --> 01:27:44.200]  if Apple never makes all of iCloud backups
[01:27:44.200 --> 01:27:48.200]  two-factor or end-to-end encrypted as an option,
[01:27:50.360 --> 01:27:52.120]  if they start moving other things,
[01:27:52.760 --> 01:27:56.200]  I would like to see them do with iMessage
[01:27:56.200 --> 01:27:57.880]  what they've done with health,
[01:27:57.880 --> 01:28:03.400]  where, okay, take iMessage out of your regular iCloud backup
[01:28:03.400 --> 01:28:07.960]  and only sync to iCloud through iMessage in the cloud
[01:28:07.960 --> 01:28:09.000]  as a separate feature
[01:28:09.640 --> 01:28:13.160]  and make all of that two-factor or end-to-end encrypted.
[01:28:13.160 --> 01:28:18.600]  So that everything in iCloud that's related to iMessage
[01:28:18.600 --> 01:28:22.440]  is only decryptable on the device.
[01:28:22.440 --> 01:28:25.560]  And the downside to that would be
[01:28:25.560 --> 01:28:28.760]  somebody who needs Apple's help to recover their stuff
[01:28:28.760 --> 01:28:31.080]  would not recover their iMessage history.
[01:28:31.080 --> 01:28:34.120]  To me, that's a trade-off that would be worth making.
[01:28:35.000 --> 01:28:38.520]  So again, I used to have to deal with that a long time ago at iMore
[01:28:38.520 --> 01:28:39.960]  because people would always be writing in
[01:28:39.960 --> 01:28:42.360]  saying they lost access to their text.
[01:28:42.360 --> 01:28:44.760]  So for a lot of people, it is secure communication,
[01:28:44.760 --> 01:28:46.200]  but for a lot of other people,
[01:28:46.200 --> 01:28:48.680]  it's all the text they sent in a relationship,
[01:28:48.680 --> 01:28:49.880]  you know, when they got engaged,
[01:28:49.880 --> 01:28:51.240]  when they were talking about their kids,
[01:28:51.240 --> 01:28:52.680]  their history with people.
[01:28:52.680 --> 01:28:55.800]  And for a lot of people, that's the same as a photograph
[01:28:55.800 --> 01:28:58.280]  where it has almost no value to somebody else
[01:28:58.280 --> 01:29:00.600]  but has tremendous sentimental value to them.
[01:29:00.600 --> 01:29:01.560]  And they used to have to get,
[01:29:01.560 --> 01:29:02.920]  like Ecamm used to make a utility
[01:29:02.920 --> 01:29:04.600]  where you could go and copy it off one phone
[01:29:04.600 --> 01:29:06.440]  and transfer it to another phone.
[01:29:06.440 --> 01:29:08.120]  So it's one of those things where they weigh it
[01:29:08.120 --> 01:29:09.480]  and they still see like, yes,
[01:29:09.480 --> 01:29:11.800]  for like the nerdy people on Twitter
[01:29:11.800 --> 01:29:14.200]  and like people who have like threat levels,
[01:29:14.200 --> 01:29:16.600]  totally, you know, make it encrypted all the time.
[01:29:16.600 --> 01:29:18.040]  But for the vast majority of people,
[01:29:18.040 --> 01:29:20.280]  they are far more concerned about losing their messages
[01:29:20.280 --> 01:29:22.120]  than somebody else seeing them.
[01:29:22.120 --> 01:29:26.440]  Yeah, so I don't know what, so you'd think, you know, I, you know.
[01:29:26.440 --> 01:29:27.560]  I would like the option for all of it.
[01:29:27.560 --> 01:29:28.920]  Like I would turn it on in an instant
[01:29:28.920 --> 01:29:30.200]  and people would tell me I'm dumb for doing it
[01:29:30.200 --> 01:29:31.000]  because I would lose stuff.
[01:29:31.000 --> 01:29:32.920]  And I know I would, but like, I just have the,
[01:29:32.920 --> 01:29:34.520]  I'm paranoid, so I would do it.
[01:29:34.520 --> 01:29:37.240]  Yeah, I would like to turn it on for all of iCloud backup,
[01:29:37.240 --> 01:29:40.280]  but on a big, if not that,
[01:29:40.280 --> 01:29:43.320]  I would like to do it for iMessage specifically.
[01:29:43.320 --> 01:29:45.880]  But the downside of the thing about iMessage
[01:29:45.880 --> 01:29:47.240]  is it's not just your data.
[01:29:47.240 --> 01:29:51.320]  It matters who you're communicating with, right?
[01:29:51.320 --> 01:29:52.520]  Same with Gmail, like people like,
[01:29:52.520 --> 01:29:53.400]  I would never use Gmail,
[01:29:53.400 --> 01:29:55.320]  but half the people you email on the internet
[01:29:55.320 --> 01:29:56.440]  are using Gmail.
[01:29:56.440 --> 01:29:58.920]  Right, so it, you know, let's just say you and I
[01:29:58.920 --> 01:30:02.120]  get involved in a criminal conspiracy
[01:30:02.120 --> 01:30:04.760]  or a suspected criminal conspiracy.
[01:30:05.480 --> 01:30:09.000]  And I turn on, or let's say you turn it on
[01:30:09.000 --> 01:30:11.160]  because you're more security conscious than me,
[01:30:11.160 --> 01:30:15.640]  and you turn on, only store all of my iMessage history
[01:30:15.640 --> 01:30:17.080]  using end-to-end encryption
[01:30:17.080 --> 01:30:19.720]  and take it out of my regular iCloud backup.
[01:30:19.720 --> 01:30:21.400]  If I don't have that turned on
[01:30:21.400 --> 01:30:24.040]  or I'm using an old version of iOS
[01:30:24.040 --> 01:30:26.120]  where this feature that's at the moment,
[01:30:26.120 --> 01:30:28.440]  as I discussed it, hypothetical,
[01:30:28.440 --> 01:30:29.640]  my device isn't even on it,
[01:30:29.640 --> 01:30:30.680]  so I don't even have the option.
[01:30:31.800 --> 01:30:33.800]  And it's, you're in my conspiracy,
[01:30:33.800 --> 01:30:35.960]  even if law enforcement, even with a warrant,
[01:30:35.960 --> 01:30:38.280]  cannot access any of your iMessage data,
[01:30:38.280 --> 01:30:41.080]  they can just come to me and get my iCloud backup,
[01:30:41.080 --> 01:30:43.320]  and then here's the whole iMessage history
[01:30:43.320 --> 01:30:44.040]  between me and you.
[01:30:44.840 --> 01:30:46.760]  Basically, like, I don't want to be one of those people,
[01:30:46.760 --> 01:30:49.000]  but it goes back to the need for fundamental regulation
[01:30:49.000 --> 01:30:50.120]  and legislation around this.
[01:30:50.120 --> 01:30:51.720]  And unfortunately, almost every government
[01:30:51.720 --> 01:30:53.000]  is on the wrong side of it.
[01:30:53.000 --> 01:30:54.840]  But there's longstanding policies about,
[01:30:54.840 --> 01:30:56.680]  like, law enforcement would love to be able
[01:30:56.680 --> 01:30:59.000]  to get everybody's DNA and fingerprints at birth.
[01:30:59.000 --> 01:31:01.240]  We chose as a society not to allow that.
[01:31:01.240 --> 01:31:02.520]  They would love unfettered access
[01:31:02.520 --> 01:31:03.880]  to all of our data and communications,
[01:31:03.880 --> 01:31:05.640]  but we have to choose as a society
[01:31:05.640 --> 01:31:07.080]  to understand that these devices
[01:31:07.080 --> 01:31:08.920]  hold so much of our life that they're essentially
[01:31:08.920 --> 01:31:10.840]  external cybernetics at this point,
[01:31:10.840 --> 01:31:12.200]  and they'll be internal one day,
[01:31:12.200 --> 01:31:14.840]  and does the chip in your head have a right
[01:31:14.840 --> 01:31:17.560]  to not, like, be forced to testify against you?
[01:31:17.560 --> 01:31:19.240]  And we're gonna have to wrestle,
[01:31:19.240 --> 01:31:20.920]  as a society, with these laws,
[01:31:20.920 --> 01:31:23.240]  and it would be great if more forward-thinking,
[01:31:23.240 --> 01:31:25.880]  less dinosaur-like politicians came to realize
[01:31:25.880 --> 01:31:28.520]  that a lot of this stuff shouldn't have to be our burden.
[01:31:28.520 --> 01:31:32.280]  They should make laws that are as respectful of our privacy
[01:31:32.280 --> 01:31:34.600]  as fingerprint and DNA laws have been historically.
[01:31:34.600 --> 01:31:38.440]  PAUL Yeah, and a lot of this really comes down to the...
[01:31:40.680 --> 01:31:44.040]  Because people don't understand how the technology works,
[01:31:44.760 --> 01:31:47.160]  and it's not that everybody should have
[01:31:47.160 --> 01:31:48.600]  a computer science degree.
[01:31:48.600 --> 01:31:50.200]  It's obviously never going to be the case,
[01:31:50.200 --> 01:31:52.200]  and there's no reason to argue that they should.
[01:31:52.200 --> 01:31:55.320]  But it means that common-sense understanding
[01:31:55.320 --> 01:31:57.080]  of the matter just isn't there.
[01:31:57.080 --> 01:32:02.600]  So if anybody proposed that everybody in the US
[01:32:02.600 --> 01:32:04.840]  or Canada, Mexico, wherever you live,
[01:32:04.840 --> 01:32:05.640]  it doesn't matter.
[01:32:05.640 --> 01:32:07.400]  But everybody in the country
[01:32:07.400 --> 01:32:09.240]  should make a copy of their house key
[01:32:09.240 --> 01:32:10.920]  and mail it to the FBI,
[01:32:10.920 --> 01:32:14.440]  so that if ever needed, the FBI has a key to your house.
[01:32:15.560 --> 01:32:16.600]  They wouldn't go anywhere.
[01:32:17.160 --> 01:32:20.280]  Immediately, before they even finished making the proposal,
[01:32:20.280 --> 01:32:21.880]  they'd be laughed off the podium, right?
[01:32:21.880 --> 01:32:22.360]  Because people are like...
[01:32:22.360 --> 01:32:23.720]  SIMON They're even doing that with face ID.
[01:32:23.720 --> 01:32:25.880]  They're starting to pass laws about how face ID...
[01:32:25.880 --> 01:32:27.480]  facial recognition, I should say, sorry,
[01:32:27.480 --> 01:32:30.120]  like random cameras and who's allowed to track us
[01:32:30.120 --> 01:32:31.720]  and who's not and where they can be used.
[01:32:31.720 --> 01:32:33.400]  But for some reason, it's like open season
[01:32:33.400 --> 01:32:35.160]  on phones all the time.
[01:32:35.160 --> 01:32:37.960]  PAUL People intuitively understand
[01:32:37.960 --> 01:32:40.760]  that a physical key that goes in a physical lock
[01:32:40.760 --> 01:32:43.160]  is not something that you should be sending copies of
[01:32:43.160 --> 01:32:43.960]  to the government.
[01:32:43.960 --> 01:32:45.880]  Or you shouldn't do it willfully,
[01:32:45.880 --> 01:32:47.720]  and you certainly shouldn't be required to.
[01:32:47.720 --> 01:32:49.800]  But once it comes down to this stuff
[01:32:49.800 --> 01:32:52.760]  that's all just ones and zeros and stuff like that,
[01:32:52.760 --> 01:32:55.320]  it's a lot easier to conflate the arguments, right?
[01:32:55.320 --> 01:32:59.320]  And yeah, and the law enforcement people are like,
[01:32:59.320 --> 01:33:02.040]  this is all about fighting child pornography.
[01:33:02.040 --> 01:33:02.920]  SIMON And terrorism.
[01:33:02.920 --> 01:33:04.360]  That's the two things every time.
[01:33:04.360 --> 01:33:10.360]  PAUL And they're not lying that it has relationship
[01:33:10.360 --> 01:33:12.280]  to those cases and would make it easier
[01:33:12.280 --> 01:33:16.760]  to pursue those cases, which in and of itself is good.
[01:33:16.760 --> 01:33:21.000]  But on the whole, considering all of the balances,
[01:33:21.000 --> 01:33:22.200]  it is not worth it.
[01:33:22.200 --> 01:33:23.720]  But they make those arguments.
[01:33:23.720 --> 01:33:26.120]  Whereas when you're talking about actual physical stuff
[01:33:26.120 --> 01:33:30.600]  like keeping a key on file with your local police
[01:33:31.240 --> 01:33:34.280]  to your house, people just know, no, I don't need that.
[01:33:34.280 --> 01:33:36.520]  You need to get in my house, break the door down, you know?
[01:33:36.520 --> 01:33:39.560]  SIMON Yeah, again, law enforcement
[01:33:39.560 --> 01:33:40.200]  is doing their job.
[01:33:40.200 --> 01:33:42.120]  Their job is to want to do this stuff as quickly
[01:33:42.120 --> 01:33:43.240]  and easily as possible.
[01:33:43.240 --> 01:33:44.920]  And it would absolutely help them.
[01:33:44.920 --> 01:33:46.600]  So would enhanced interrogation.
[01:33:46.600 --> 01:33:48.360]  So would lack of a Fifth Amendment.
[01:33:48.360 --> 01:33:49.480]  So would all of these things.
[01:33:50.280 --> 01:33:52.600]  But again, as a society, as a civilization,
[01:33:52.600 --> 01:33:54.680]  we've decided that their job should be hard,
[01:33:54.680 --> 01:33:56.440]  that it is the benefit of society.
[01:33:56.440 --> 01:33:58.120]  That one in it, like, what is the saying?
[01:33:58.120 --> 01:33:59.560]  You know, five guilty people should go free
[01:33:59.560 --> 01:34:01.480]  rather than one innocent person go to jail.
[01:34:01.480 --> 01:34:02.760]  We've made those decisions.
[01:34:02.760 --> 01:34:04.680]  We just haven't taken them to their logical conclusion
[01:34:04.680 --> 01:34:05.640]  with digital devices.
[01:34:05.640 --> 01:34:06.040]  PAUL Yeah.
[01:34:06.040 --> 01:34:09.080]  Well, it's a lot more than five, I would think.
[01:34:09.080 --> 01:34:09.400]  I don't know.
[01:34:09.400 --> 01:34:11.560]  But, you know, at some point there is a number, right?
[01:34:11.560 --> 01:34:13.880]  What is the number of how many guilty people
[01:34:13.880 --> 01:34:15.720]  should go free before one innocent person?
[01:34:15.720 --> 01:34:18.040]  You know, there's some number, right?
[01:34:18.040 --> 01:34:18.440]  I don't know.
[01:34:18.440 --> 01:34:18.840]  SIMON Yeah.
[01:34:18.840 --> 01:34:21.720]  PAUL But it's obviously the, you know,
[01:34:21.720 --> 01:34:23.960]  it requires nuance.
[01:34:23.960 --> 01:34:29.080]  And we as a civilization and society still, you know,
[01:34:29.080 --> 01:34:31.160]  have some work to do on nuanced arguments.
[01:34:31.160 --> 01:34:32.520]  SIMON Yeah, yeah, yeah.
[01:34:32.520 --> 01:34:34.120]  All across the performative,
[01:34:34.120 --> 01:34:35.160]  they just want their soundbite.
[01:34:35.160 --> 01:34:37.240]  They don't care about the actual job anymore.
[01:34:37.240 --> 01:34:38.280]  PAUL All right, let me take a break here.
[01:34:38.280 --> 01:34:40.040]  Thank our third sponsor of the show,
[01:34:40.040 --> 01:34:42.280]  and it's our good friends at Memberful.
[01:34:42.280 --> 01:34:44.520]  You can monetize your passion
[01:34:44.520 --> 01:34:46.440]  and your audience with memberships.
[01:34:46.440 --> 01:34:48.920]  Memberful allows you, a creator,
[01:34:48.920 --> 01:34:50.840]  to build a sustainable, recurring revenue,
[01:34:50.840 --> 01:34:53.720]  and it is the easiest way to sell memberships
[01:34:53.720 --> 01:34:54.680]  to your audience.
[01:34:54.680 --> 01:34:56.920]  And it is used by some of the biggest creators
[01:34:56.920 --> 01:34:57.960]  on the internet.
[01:34:58.600 --> 01:34:59.960]  Memberful has everything you need
[01:34:59.960 --> 01:35:01.080]  to run a membership program,
[01:35:01.080 --> 01:35:03.480]  including custom branding, gift subscriptions,
[01:35:03.480 --> 01:35:06.600]  Apple Pay support, free trials, private podcasts,
[01:35:06.600 --> 01:35:07.720]  tons more.
[01:35:07.720 --> 01:35:10.280]  And it seamlessly integrates with the tools
[01:35:10.280 --> 01:35:11.960]  and systems you already use,
[01:35:11.960 --> 01:35:14.680]  like WordPress and mailing list managers
[01:35:14.680 --> 01:35:16.840]  and all sorts of other features.
[01:35:16.840 --> 01:35:18.520]  They have a world-class support team
[01:35:18.520 --> 01:35:21.000]  that is ready to help you simplify your memberships
[01:35:21.000 --> 01:35:22.600]  and grow your revenue.
[01:35:22.600 --> 01:35:25.000]  They're passionate about your success
[01:35:25.000 --> 01:35:28.680]  because they only make money when you make money,
[01:35:28.680 --> 01:35:29.960]  and you only make money
[01:35:29.960 --> 01:35:32.120]  when you're selling memberships to your audience
[01:35:32.120 --> 01:35:33.480]  that provide your audience
[01:35:33.480 --> 01:35:35.320]  with things they want access to.
[01:35:35.320 --> 01:35:37.560]  It is like a virtuous cycle,
[01:35:37.560 --> 01:35:40.360]  happy audience paying for things
[01:35:40.360 --> 01:35:41.400]  they're happy to pay for.
[01:35:42.040 --> 01:35:44.600]  You are a happy creator making money
[01:35:44.600 --> 01:35:45.560]  through the membership,
[01:35:45.560 --> 01:35:47.240]  and Memberful is a happy company
[01:35:47.240 --> 01:35:49.320]  providing you with the tools to do it
[01:35:49.320 --> 01:35:52.520]  by taking their small cut of your membership.
[01:35:52.520 --> 01:35:56.200]  They do it all with keeping your brand in front,
[01:35:56.200 --> 01:35:57.320]  not Memberful.
[01:35:57.320 --> 01:36:00.120]  You're building a relationship with your brand
[01:36:00.120 --> 01:36:01.080]  and your audience,
[01:36:01.080 --> 01:36:02.760]  not your audience and Memberful,
[01:36:02.760 --> 01:36:05.000]  and they make everything something
[01:36:05.000 --> 01:36:06.120]  you can take with you.
[01:36:06.120 --> 01:36:07.960]  Your entire subscriber list,
[01:36:07.960 --> 01:36:10.280]  everything if you would choose to move
[01:36:10.280 --> 01:36:12.280]  to a different service or system,
[01:36:12.840 --> 01:36:14.280]  you can take everything with you.
[01:36:14.280 --> 01:36:15.560]  That's how confident they are
[01:36:15.560 --> 01:36:18.040]  that you won't want to leave Memberful.
[01:36:18.040 --> 01:36:20.520]  There's no lock-in and no branding
[01:36:20.520 --> 01:36:21.880]  that sits atop your brand.
[01:36:21.880 --> 01:36:23.480]  It's all your brand.
[01:36:23.480 --> 01:36:24.600]  It is a great service.
[01:36:24.600 --> 01:36:27.240]  I subscribe to a slew of Memberful's
[01:36:27.240 --> 01:36:28.840]  Six Colors, Relay FM,
[01:36:28.840 --> 01:36:30.040]  the list goes on and on.
[01:36:30.040 --> 01:36:31.560]  I spend a ton every month on it,
[01:36:31.560 --> 01:36:32.600]  and I'm happy to do so
[01:36:32.600 --> 01:36:34.120]  because it's all creators
[01:36:34.120 --> 01:36:35.560]  who I'm happy to support
[01:36:35.560 --> 01:36:38.040]  and happy to get the members-only stuff that I get.
[01:36:38.600 --> 01:36:39.880]  Where do you go to find out more?
[01:36:39.880 --> 01:36:42.920]  Go to memberful.com slash talk show.
[01:36:42.920 --> 01:36:45.800]  Memberful.com slash talk show,
[01:36:45.800 --> 01:36:47.560]  and you can get started for free
[01:36:47.560 --> 01:36:49.320]  with no credit card required.
[01:36:49.320 --> 01:36:50.600]  Check them out if you have
[01:36:50.600 --> 01:36:51.880]  any sort of audience you're looking
[01:36:51.880 --> 01:36:53.480]  to monetize with memberships.
[01:36:53.480 --> 01:36:54.280]  What's left?
[01:36:54.280 --> 01:36:55.480]  We had something else, right?
[01:36:55.480 --> 01:36:57.080]  Yeah, the controversy,
[01:36:57.080 --> 01:36:58.280]  maybe controversy about
[01:36:58.280 --> 01:37:00.520]  the single NAND flash version.
[01:37:00.520 --> 01:37:02.600]  Ah, yes.
[01:37:02.600 --> 01:37:03.800]  Yes, this is great.
[01:37:04.680 --> 01:37:10.680]  The M2 13-inch MacBook Pros are out.
[01:37:10.680 --> 01:37:11.640]  You can order them.
[01:37:11.640 --> 01:37:12.600]  The reviews are out.
[01:37:12.600 --> 01:37:14.040]  They're in people's hands.
[01:37:14.040 --> 01:37:15.160]  We're waiting.
[01:37:15.160 --> 01:37:16.360]  It's still, as we record,
[01:37:16.360 --> 01:37:17.800]  this is still technically June,
[01:37:17.800 --> 01:37:19.320]  and Apple did promise us, quote,
[01:37:19.320 --> 01:37:22.200]  next month for the M2 MacBook Airs,
[01:37:22.200 --> 01:37:23.080]  so we don't have them.
[01:37:23.880 --> 01:37:26.760]  But people with the new 13-inch MacBook Pros
[01:37:26.760 --> 01:37:29.240]  with the M2 have seen that,
[01:37:29.240 --> 01:37:30.920]  well, you summarized the situation.
[01:37:30.920 --> 01:37:32.680]  Yeah, so I didn't know this
[01:37:32.680 --> 01:37:34.360]  because the editorial loans this time
[01:37:34.360 --> 01:37:36.040]  were the one terabyte models.
[01:37:36.040 --> 01:37:37.320]  You know, Apple doesn't always
[01:37:37.320 --> 01:37:38.360]  give you the maxed out version,
[01:37:38.360 --> 01:37:39.480]  but they usually give you
[01:37:39.480 --> 01:37:40.840]  more than the baseline.
[01:37:40.840 --> 01:37:42.120]  No, I should clarify that.
[01:37:42.120 --> 01:37:43.880]  Previous administration at Apple
[01:37:43.880 --> 01:37:44.600]  used to love to give me
[01:37:44.600 --> 01:37:45.720]  the lowest version of a product
[01:37:45.720 --> 01:37:46.680]  just so I could show,
[01:37:46.680 --> 01:37:47.560]  because they thought that I was good
[01:37:47.560 --> 01:37:48.520]  at showing what you could still do
[01:37:48.520 --> 01:37:49.480]  on the lowest version,
[01:37:49.480 --> 01:37:50.200]  but I usually get, like,
[01:37:50.200 --> 01:37:51.480]  the middle-level products now.
[01:37:51.480 --> 01:37:52.360]  You know, like, Marques will get
[01:37:52.360 --> 01:37:54.680]  the highest end because 8K, 6-whatever,
[01:37:54.680 --> 01:37:55.800]  8K 30 video.
[01:37:55.800 --> 01:37:58.040]  So I got the one terabyte model,
[01:37:58.040 --> 01:37:59.080]  and it turned out,
[01:37:59.080 --> 01:38:00.040]  when people started getting
[01:38:00.040 --> 01:38:02.040]  the 256 gigabyte model,
[01:38:02.040 --> 01:38:03.640]  that Apple had switched from using
[01:38:03.640 --> 01:38:06.440]  two 128 gigabyte chips for the NAND flash,
[01:38:06.440 --> 01:38:07.800]  because it's not drives anymore
[01:38:07.800 --> 01:38:08.440]  in these devices.
[01:38:08.440 --> 01:38:09.880]  They're just raw chips,
[01:38:09.880 --> 01:38:11.160]  you know, on the board,
[01:38:11.160 --> 01:38:14.680]  to using a single 256 gigabyte chip.
[01:38:14.680 --> 01:38:15.960]  And what that did is it means
[01:38:15.960 --> 01:38:16.840]  the chips are no longer
[01:38:16.840 --> 01:38:18.200]  working in parallel.
[01:38:18.200 --> 01:38:19.960]  So the higher models are,
[01:38:19.960 --> 01:38:21.720]  because they have multiple 256
[01:38:21.720 --> 01:38:23.560]  or 512 or whatever chips,
[01:38:23.560 --> 01:38:26.680]  but the single 256 gigabyte chip
[01:38:26.680 --> 01:38:28.280]  is working serially now.
[01:38:28.280 --> 01:38:29.480]  So if you plug it in to do,
[01:38:29.480 --> 01:38:30.520]  like, a file transfer
[01:38:30.520 --> 01:38:32.040]  or if you're writing to drive,
[01:38:32.040 --> 01:38:33.640]  it's getting about half the performance
[01:38:33.640 --> 01:38:35.240]  of the previous M1,
[01:38:35.240 --> 01:38:38.200]  which had two 128 gigabyte chips in it.
[01:38:38.200 --> 01:38:39.320]  And the reason for that is just,
[01:38:39.320 --> 01:38:40.440]  like, it's manufacturing
[01:38:40.440 --> 01:38:41.160]  at a certain point,
[01:38:42.120 --> 01:38:43.720]  companies stop making smaller stuff.
[01:38:43.720 --> 01:38:44.920]  That's why Apple has to move up
[01:38:44.920 --> 01:38:46.520]  the memory and move up the storage
[01:38:46.520 --> 01:38:47.560]  on devices over time,
[01:38:47.560 --> 01:38:49.000]  because whatever the most
[01:38:49.000 --> 01:38:50.840]  mainstream version of the chip is,
[01:38:50.840 --> 01:38:52.680]  not only does there much more volume
[01:38:52.680 --> 01:38:54.040]  about it, but the prices go down
[01:38:54.040 --> 01:38:55.880]  because of the economies of scale.
[01:38:55.880 --> 01:38:57.000]  So my guess is,
[01:38:57.000 --> 01:38:58.600]  because of a combination of 2020
[01:38:58.600 --> 01:39:01.960]  and just the progress in fabrication,
[01:39:01.960 --> 01:39:03.960]  that this is the most common module now.
[01:39:03.960 --> 01:39:05.720]  So Apple went to the 256
[01:39:05.720 --> 01:39:08.200]  and I should digress for a minute.
[01:39:08.200 --> 01:39:09.800]  The way Apple handles a lot of this stuff
[01:39:09.800 --> 01:39:11.080]  is that they have a minimum spec
[01:39:11.080 --> 01:39:12.200]  to deliver the experience
[01:39:12.200 --> 01:39:13.640]  they want for a device.
[01:39:13.640 --> 01:39:14.920]  And a good way of thinking about it
[01:39:14.920 --> 01:39:16.120]  is, like, if you have to make
[01:39:16.120 --> 01:39:16.840]  the Olympic team,
[01:39:16.840 --> 01:39:18.120]  and making the Olympic team means
[01:39:18.120 --> 01:39:19.640]  you've got to run 100 meters
[01:39:19.640 --> 01:39:21.640]  in under 10 seconds,
[01:39:21.640 --> 01:39:22.520]  that's all you need
[01:39:22.520 --> 01:39:23.480]  to make the Olympic team.
[01:39:23.480 --> 01:39:24.920]  Like, Apple will say what the spec
[01:39:24.920 --> 01:39:26.120]  the supplier has to meet.
[01:39:26.120 --> 01:39:27.240]  And if they overperform,
[01:39:27.240 --> 01:39:28.040]  Apple doesn't care.
[01:39:28.040 --> 01:39:30.040]  If they underperform, they're finished.
[01:39:30.040 --> 01:39:31.000]  But if they overperform,
[01:39:31.000 --> 01:39:31.800]  Apple doesn't care.
[01:39:31.800 --> 01:39:33.560]  So if it's 9.9 seconds,
[01:39:33.560 --> 01:39:36.440]  9.8 seconds, 9.7, they don't care.
[01:39:36.440 --> 01:39:38.200]  And that's resulted in some controversies,
[01:39:38.200 --> 01:39:40.360]  like when the A9 was dual-sourced
[01:39:40.360 --> 01:39:42.280]  between Samsung and TSMC.
[01:39:42.280 --> 01:39:44.680]  And TSMC's process was better,
[01:39:44.680 --> 01:39:45.960]  so you got longer battery life
[01:39:45.960 --> 01:39:46.920]  on that model.
[01:39:46.920 --> 01:39:48.280]  Or, like, the LG versus
[01:39:48.280 --> 01:39:50.040]  the Samsung display on some MacBooks,
[01:39:50.040 --> 01:39:51.480]  the Samsung one was better,
[01:39:51.480 --> 01:39:52.680]  or the LG, when I think back then,
[01:39:52.680 --> 01:39:53.000]  was better.
[01:39:53.000 --> 01:39:54.120]  So people tried to figure out
[01:39:54.120 --> 01:39:55.240]  which one they were buying.
[01:39:55.240 --> 01:39:56.920]  Even more recently, like, you know,
[01:39:56.920 --> 01:39:57.800]  sometimes there have been, like,
[01:39:57.800 --> 01:39:59.000]  models of iPads that have had
[01:39:59.000 --> 01:39:59.720]  more RAM in them
[01:39:59.720 --> 01:40:00.840]  that Apple doesn't talk about.
[01:40:00.840 --> 01:40:02.520]  There's always been cases like this,
[01:40:03.960 --> 01:40:04.920]  mutual friends of ours
[01:40:04.920 --> 01:40:05.880]  have gone nuts trying to find
[01:40:05.880 --> 01:40:06.680]  the versions that have
[01:40:06.680 --> 01:40:07.640]  the better stuff in it.
[01:40:07.640 --> 01:40:08.920]  This isn't exactly that.
[01:40:08.920 --> 01:40:10.760]  It's just this was the component
[01:40:10.760 --> 01:40:11.560]  that Apple could fit
[01:40:11.560 --> 01:40:12.840]  within getting enough supply
[01:40:12.840 --> 01:40:13.800]  to release it,
[01:40:13.800 --> 01:40:15.800]  and within the budget for the product,
[01:40:15.800 --> 01:40:17.320]  and it meets their minimum spec
[01:40:17.320 --> 01:40:18.200]  for the product.
[01:40:18.200 --> 01:40:20.120]  So yes, the old version was twice as fast,
[01:40:20.120 --> 01:40:21.400]  and that has, like,
[01:40:21.400 --> 01:40:22.600]  some benefit for people
[01:40:22.600 --> 01:40:24.040]  who really care about transfer speed,
[01:40:24.040 --> 01:40:26.360]  like if you're doing file transfers.
[01:40:26.360 --> 01:40:27.720]  It shouldn't be a huge impediment
[01:40:27.720 --> 01:40:28.520]  if you're doing anything else,
[01:40:28.520 --> 01:40:31.000]  because that's sort of written into
[01:40:31.000 --> 01:40:32.360]  what they designed the product for.
[01:40:32.360 --> 01:40:34.120]  But people on YouTube do a ton of tests now,
[01:40:34.120 --> 01:40:34.920]  and they find the stuff,
[01:40:34.920 --> 01:40:35.880]  and more power to them,
[01:40:35.880 --> 01:40:37.320]  because I don't think
[01:40:37.320 --> 01:40:39.400]  that pros would really get the 256 version.
[01:40:39.400 --> 01:40:40.040]  I would argue Apple
[01:40:40.040 --> 01:40:41.720]  shouldn't even make it anymore.
[01:40:41.720 --> 01:40:43.240]  I think it's far too small,
[01:40:43.240 --> 01:40:43.960]  and I think the hint
[01:40:43.960 --> 01:40:46.440]  they can't get those chips
[01:40:46.440 --> 01:40:47.560]  in parallel anymore to do it
[01:40:47.560 --> 01:40:48.760]  was probably a big indicator
[01:40:48.760 --> 01:40:49.640]  they shouldn't have done it.
[01:40:49.640 --> 01:40:50.680]  But that's basically
[01:40:50.680 --> 01:40:51.880]  what's happening right now.
[01:40:51.880 --> 01:40:54.680]  Yeah, and the disingenuous thing
[01:40:54.680 --> 01:40:55.400]  that I've seen,
[01:40:55.400 --> 01:40:56.760]  and I don't have anybody
[01:40:56.760 --> 01:40:58.200]  to throw into the bus handy,
[01:40:58.200 --> 01:41:00.200]  but I saw headlines when this came out
[01:41:00.760 --> 01:41:02.520]  that in the headline,
[01:41:02.520 --> 01:41:04.520]  and headlines matter so much
[01:41:04.520 --> 01:41:06.280]  it is impossible to overstate
[01:41:06.280 --> 01:41:08.040]  because so many people
[01:41:08.040 --> 01:41:09.000]  only read the headline.
[01:41:09.000 --> 01:41:11.400]  But if the headline and the subhead
[01:41:11.400 --> 01:41:12.280]  more or less saying
[01:41:12.920 --> 01:41:15.320]  the M2 13-inch MacBook Pro
[01:41:15.320 --> 01:41:17.640]  gets half the IO performance
[01:41:17.640 --> 01:41:18.440]  of the M1,
[01:41:19.400 --> 01:41:21.240]  and end of headline,
[01:41:21.240 --> 01:41:22.440]  end of summary,
[01:41:22.440 --> 01:41:24.360]  and then you'd have to read the story
[01:41:24.360 --> 01:41:25.800]  to see that it only applies
[01:41:25.800 --> 01:41:29.640]  to the 256 configuration,
[01:41:29.640 --> 01:41:31.160]  and that it's not true
[01:41:31.160 --> 01:41:33.880]  for the 512 or one terabyte.
[01:41:33.880 --> 01:41:35.400]  Does it max out at one terabyte?
[01:41:35.400 --> 01:41:36.440]  I don't even know.
[01:41:36.440 --> 01:41:37.160]  Two terabytes.
[01:41:37.160 --> 01:41:38.040]  There's a two terabyte.
[01:41:38.040 --> 01:41:40.440]  So you can get 256, 512,
[01:41:40.440 --> 01:41:42.520]  one terabyte, or two terabytes.
[01:41:43.080 --> 01:41:46.280]  Only the 256 gigabyte version
[01:41:46.840 --> 01:41:49.000]  has this decreased IO,
[01:41:49.000 --> 01:41:51.080]  and it is exactly for the reason you said,
[01:41:51.080 --> 01:41:53.640]  where with all of the M1 systems,
[01:41:53.640 --> 01:41:55.400]  no matter what level of storage,
[01:41:56.440 --> 01:41:58.120]  underneath the hood,
[01:41:58.120 --> 01:42:00.200]  literally, literally underneath the hood,
[01:42:02.200 --> 01:42:03.720]  the storage,
[01:42:03.720 --> 01:42:05.880]  the built-in storage on the M1 chip
[01:42:06.440 --> 01:42:09.480]  consisted of two SSD components.
[01:42:09.480 --> 01:42:11.160]  No matter if it was the lowest end
[01:42:11.160 --> 01:42:12.280]  or the highest end,
[01:42:12.280 --> 01:42:13.080]  there were two,
[01:42:13.080 --> 01:42:15.080]  and so even the lowest end configuration
[01:42:15.080 --> 01:42:18.760]  got the benefits of parallel read-write
[01:42:18.760 --> 01:42:20.760]  to the storage.
[01:42:20.760 --> 01:42:22.760]  You never see it as a user.
[01:42:22.760 --> 01:42:24.040]  There's no reason for you to.
[01:42:24.040 --> 01:42:26.280]  This is completely an implementation detail
[01:42:26.280 --> 01:42:27.880]  of Apple systems on a chip.
[01:42:30.120 --> 01:42:32.280]  You open up your brand new MacBook,
[01:42:32.280 --> 01:42:35.560]  and it's a 256 gigabyte configuration,
[01:42:35.560 --> 01:42:38.600]  and you see a startup disk,
[01:42:38.600 --> 01:42:39.720]  Macintosh HD,
[01:42:40.360 --> 01:42:41.400]  with the systems.
[01:42:41.400 --> 01:42:43.400]  You know, you don't have all 256 available
[01:42:43.400 --> 01:42:44.600]  because the system is there,
[01:42:44.600 --> 01:42:49.080]  but it looks like one 256 gigabyte drive.
[01:42:50.360 --> 01:42:50.680]  I don't know.
[01:42:50.680 --> 01:42:51.720]  What do we call them?
[01:42:51.720 --> 01:42:54.360]  SSDs, the D still stands for drive, right?
[01:42:54.360 --> 01:42:55.080]  It's not a spinning.
[01:42:55.080 --> 01:42:55.560]  Yeah.
[01:42:55.560 --> 01:42:56.200]  It's not a disk.
[01:42:57.000 --> 01:42:58.760]  Colloquially, you can still call it an SSD.
[01:42:58.760 --> 01:43:00.200]  It's like, it's just a NAND flash chip,
[01:43:00.200 --> 01:43:01.160]  but that sounds dumb.
[01:43:01.160 --> 01:43:01.640]  Right.
[01:43:01.640 --> 01:43:03.800]  But it's, you know, this flash,
[01:43:03.800 --> 01:43:05.880]  the long-term storage in the device
[01:43:05.880 --> 01:43:08.600]  technically consisted on all configurations
[01:43:09.240 --> 01:43:10.920]  of two or at least two.
[01:43:10.920 --> 01:43:11.560]  I don't even know.
[01:43:11.560 --> 01:43:14.040]  I don't even know if the two terabyte
[01:43:14.040 --> 01:43:15.000]  was more than two,
[01:43:15.640 --> 01:43:16.920]  but whatever the thing,
[01:43:16.920 --> 01:43:18.040]  you didn't have to worry about it.
[01:43:18.040 --> 01:43:19.160]  And now with the M2,
[01:43:19.160 --> 01:43:23.160]  the low-end one is a single 256 gigabyte chip
[01:43:24.040 --> 01:43:25.640]  on the system on a chip,
[01:43:26.200 --> 01:43:27.480]  and therefore performance.
[01:43:27.480 --> 01:43:30.680]  Now, is read-write performance slow?
[01:43:30.680 --> 01:43:31.160]  No.
[01:43:31.160 --> 01:43:34.840]  No, it still is very fast in the grand scheme of things.
[01:43:35.480 --> 01:43:36.600]  Is it worth knowing?
[01:43:36.600 --> 01:43:37.240]  It's just not ridiculously fast.
[01:43:37.240 --> 01:43:40.040]  It is absolutely an interesting thing to know.
[01:43:40.680 --> 01:43:46.120]  And if you were tempted to buy the 256 gigabyte one,
[01:43:46.120 --> 01:43:47.560]  you should definitely know about it.
[01:43:47.560 --> 01:43:48.680]  Is it a controversy?
[01:43:48.680 --> 01:43:49.320]  I don't think so.
[01:43:49.320 --> 01:43:49.880]  And again.
[01:43:49.880 --> 01:43:52.520]  I would, I would just maintain that if like,
[01:43:52.520 --> 01:43:55.320]  it is that 256 gigabytes is not usable
[01:43:55.320 --> 01:43:57.080]  for anybody who is doing any work
[01:43:57.080 --> 01:43:58.600]  that they would see the difference
[01:43:58.600 --> 01:43:59.640]  in the read-write speeds.
[01:43:59.640 --> 01:44:00.040]  Right.
[01:44:00.040 --> 01:44:02.120]  You'd have to be doing like a lot of heavy rendering
[01:44:02.120 --> 01:44:03.240]  and a lot of other things to,
[01:44:03.240 --> 01:44:04.280]  if you're using an external drive,
[01:44:04.280 --> 01:44:04.760]  you're, you're,
[01:44:04.760 --> 01:44:06.040]  the external drive is never as fast
[01:44:06.040 --> 01:44:07.240]  as internal storage anyway.
[01:44:07.240 --> 01:44:07.640]  Right.
[01:44:07.640 --> 01:44:09.000]  Because it's going through the Thunderbolt bus.
[01:44:09.640 --> 01:44:10.520]  It's, yeah.
[01:44:10.520 --> 01:44:11.240]  And my comment,
[01:44:11.240 --> 01:44:12.520]  the thing is exactly what you said.
[01:44:12.520 --> 01:44:13.240]  Like my comments,
[01:44:13.240 --> 01:44:14.680]  I did a review on the product and I didn't,
[01:44:14.680 --> 01:44:15.560]  I didn't know about this
[01:44:15.560 --> 01:44:17.480]  because I had the one terabyte version.
[01:44:17.480 --> 01:44:18.920]  So like I tested the SSD.
[01:44:18.920 --> 01:44:20.120]  It was roughly the same for me.
[01:44:20.760 --> 01:44:22.520]  But my comments are now filled with people
[01:44:22.520 --> 01:44:24.680]  who are saying this machine is DOA,
[01:44:24.680 --> 01:44:26.360]  that the whole thing is broken.
[01:44:26.360 --> 01:44:27.720]  And they believe it's every model.
[01:44:27.720 --> 01:44:27.880]  Right.
[01:44:27.880 --> 01:44:29.800]  They have no idea that it's just one.
[01:44:29.800 --> 01:44:30.280]  And that is,
[01:44:31.000 --> 01:44:31.640]  I have this,
[01:44:31.640 --> 01:44:33.000]  this issue in large is that like
[01:44:33.000 --> 01:44:34.840]  a lot of the tech coverage has become so cynical
[01:44:34.840 --> 01:44:36.840]  and so sensational and so gotcha.
[01:44:36.840 --> 01:44:38.760]  And I understand like everybody needs page views.
[01:44:38.760 --> 01:44:40.200]  Everybody needs headlines.
[01:44:40.200 --> 01:44:41.800]  Everybody wants attention.
[01:44:41.800 --> 01:44:42.200]  But the,
[01:44:42.200 --> 01:44:44.680]  the two costs of that are one,
[01:44:44.680 --> 01:44:45.880]  it's a sliding scale.
[01:44:45.880 --> 01:44:47.240]  So what got you attention last time,
[01:44:47.240 --> 01:44:49.640]  you got to escalate over that every time.
[01:44:49.640 --> 01:44:50.120]  But two,
[01:44:50.120 --> 01:44:50.760]  at a certain point,
[01:44:50.760 --> 01:44:52.440]  it becomes malware for the readers
[01:44:52.440 --> 01:44:54.440]  and it becomes bad for the customers
[01:44:54.440 --> 01:44:56.120]  because they don't get that context.
[01:44:56.120 --> 01:44:58.280]  They only get the gotcha conversation around it.
[01:44:58.280 --> 01:44:58.920]  Yeah, totally.
[01:44:58.920 --> 01:45:00.760]  And that's what I mean about the headline mattering.
[01:45:00.760 --> 01:45:01.320]  Right.
[01:45:01.320 --> 01:45:03.160]  And you start looking at these headlines
[01:45:03.160 --> 01:45:05.160]  floating around Twitter and wherever else
[01:45:05.160 --> 01:45:07.560]  you're just browsing the news.
[01:45:07.560 --> 01:45:09.240]  And that's the way it was posed
[01:45:09.240 --> 01:45:15.400]  that the M2 MacBooks have half the storage IO speed
[01:45:15.400 --> 01:45:16.840]  as the M1 versions,
[01:45:16.840 --> 01:45:19.640]  which sounds terrible and inexplicable.
[01:45:19.640 --> 01:45:23.000]  And if it were true across the board,
[01:45:23.000 --> 01:45:25.640]  would definitely be cause for,
[01:45:25.640 --> 01:45:26.680]  if not outrage,
[01:45:26.680 --> 01:45:29.560]  at least genuine concern for
[01:45:29.560 --> 01:45:31.000]  what the hell is Apple thinking?
[01:45:31.000 --> 01:45:31.320]  Right.
[01:45:31.320 --> 01:45:32.760]  It would be calamitous.
[01:45:33.720 --> 01:45:34.280]  You know,
[01:45:34.280 --> 01:45:35.800]  this is not the way things should work.
[01:45:35.800 --> 01:45:37.720]  But when the entire reason
[01:45:37.720 --> 01:45:41.080]  that the 256 gigabyte configuration exists
[01:45:41.080 --> 01:45:43.960]  is for price conscious buyers,
[01:45:44.920 --> 01:45:45.240]  you know,
[01:45:45.240 --> 01:45:46.120]  I don't,
[01:45:46.120 --> 01:45:48.680]  I don't think it's controversial at all.
[01:45:48.680 --> 01:45:49.320]  You know,
[01:45:49.320 --> 01:45:50.680]  it's mostly institutional buyers
[01:45:50.680 --> 01:45:51.960]  who want to use them as thin clients
[01:45:51.960 --> 01:45:53.400]  for web apps and custom apps.
[01:45:53.400 --> 01:45:55.560]  And it's the one,
[01:45:55.560 --> 01:45:58.600]  you know, the reduction in IO performance is fine.
[01:45:58.600 --> 01:46:00.440]  It is still so much faster
[01:46:00.440 --> 01:46:02.520]  than just about anything else on the market
[01:46:02.520 --> 01:46:03.960]  in that price range.
[01:46:03.960 --> 01:46:05.080]  It's fine.
[01:46:05.080 --> 01:46:05.880]  It does,
[01:46:05.880 --> 01:46:07.320]  unless I'm missing something
[01:46:07.320 --> 01:46:09.080]  looking at the pricing,
[01:46:09.080 --> 01:46:11.880]  the 13 inch starts at,
[01:46:12.520 --> 01:46:14.600]  I'm going to go with $1,300.
[01:46:14.600 --> 01:46:16.200]  It's technically $1,299,
[01:46:16.200 --> 01:46:18.040]  but the 99 pricing always,
[01:46:19.240 --> 01:46:19.480]  to me,
[01:46:19.480 --> 01:46:20.600]  it's a little disingenuous.
[01:46:20.600 --> 01:46:21.480]  It's $1,300.
[01:46:21.480 --> 01:46:25.640]  To get the exact same configuration,
[01:46:25.640 --> 01:46:28.120]  but only upgrade to 512.
[01:46:28.120 --> 01:46:30.120]  And if you're concerned about IO speed,
[01:46:30.120 --> 01:46:34.440]  you don't need the actual extra 256 gigabytes.
[01:46:34.440 --> 01:46:35.400]  You know,
[01:46:35.400 --> 01:46:37.640]  256 gigabytes is all you think you'll need,
[01:46:37.640 --> 01:46:38.920]  but you want the performance.
[01:46:39.480 --> 01:46:41.160]  It is a $200 upgrade.
[01:46:41.160 --> 01:46:42.280]  It goes to $1,500.
[01:46:42.840 --> 01:46:45.720]  So I can see why if in,
[01:46:45.720 --> 01:46:46.360]  I would,
[01:46:46.360 --> 01:46:48.200]  it would be shocking if Apple didn't think
[01:46:48.200 --> 01:46:51.960]  if the matrix of decisions came down of
[01:46:52.840 --> 01:46:56.040]  either we can't get the 128 chips
[01:46:56.040 --> 01:47:00.840]  to make a 256 gigabyte combined storage out of
[01:47:00.840 --> 01:47:01.400]  enough of them.
[01:47:01.400 --> 01:47:01.960]  Yeah.
[01:47:01.960 --> 01:47:03.080]  Or cheap enough price.
[01:47:03.080 --> 01:47:04.680]  Or would be more expensive
[01:47:04.680 --> 01:47:07.560]  or otherwise wouldn't fit in the design,
[01:47:07.560 --> 01:47:07.800]  you know,
[01:47:07.800 --> 01:47:10.600]  at a silicon level in the M2.
[01:47:11.160 --> 01:47:11.880]  Who knows what,
[01:47:11.880 --> 01:47:12.200]  you know,
[01:47:12.200 --> 01:47:14.280]  but let's just say,
[01:47:14.280 --> 01:47:15.960]  let's just give them the benefit of the doubt
[01:47:15.960 --> 01:47:17.240]  and assume it's good reasons,
[01:47:17.240 --> 01:47:18.840]  whether those reasons are cost,
[01:47:18.840 --> 01:47:19.960]  whether they're technical,
[01:47:19.960 --> 01:47:20.920]  whether they're actual,
[01:47:20.920 --> 01:47:24.760]  just pure availability or a combination of all three.
[01:47:24.760 --> 01:47:27.560]  Let's just assume that they made a rational decision
[01:47:27.560 --> 01:47:28.440]  that for a two,
[01:47:28.440 --> 01:47:33.320]  if we do a 256 gigabyte config of these machines,
[01:47:33.320 --> 01:47:36.440]  it would have to be one 256 gigabyte chip.
[01:47:36.440 --> 01:47:39.560]  And therefore IO will be slower compared to the M1.
[01:47:40.680 --> 01:47:44.760]  Should we still make this 256 gigabyte configuration?
[01:47:44.760 --> 01:47:49.400]  I'm with you, especially for the 13 inch pro,
[01:47:49.960 --> 01:47:52.440]  which it has the word pro in the name.
[01:47:52.440 --> 01:47:52.760]  Yes.
[01:47:52.760 --> 01:47:56.280]  Would it be disastrous if it started at $1,500?
[01:47:56.280 --> 01:48:00.360]  I mean, but I also see the counter argument on Apple's point,
[01:48:00.360 --> 01:48:05.480]  which might be that they know the sales data and know that the lots and lots of people
[01:48:05.480 --> 01:48:09.080]  come in and buy the $1,300 version with the M1.
[01:48:09.720 --> 01:48:13.240]  And therefore, if they got rid of the 256 gigabyte
[01:48:13.240 --> 01:48:17.800]  configuration, effectively, it would be a $200 increase in price
[01:48:17.800 --> 01:48:20.280]  for the low end model of the 13 inch MacBook Pro.
[01:48:21.000 --> 01:48:21.400]  Yeah.
[01:48:21.400 --> 01:48:23.880]  And again, it's the institutional buyers who buy like,
[01:48:23.880 --> 01:48:26.840]  you know, 1,000, 2,000, 8,000 of them at a time
[01:48:26.840 --> 01:48:28.360]  and really don't care about the storage.
[01:48:28.360 --> 01:48:28.600]  Right.
[01:48:28.600 --> 01:48:29.720]  And it does get to a point like,
[01:48:29.720 --> 01:48:31.240]  like a lot of people would just say Apple should eat it.
[01:48:31.240 --> 01:48:33.400]  And we're all very good at spending Tim Cook's money
[01:48:33.400 --> 01:48:35.240]  on everything all at once, always.
[01:48:35.240 --> 01:48:38.120]  And they do sometimes like sometimes the Mac announcements are only,
[01:48:38.120 --> 01:48:40.920]  it's got a new processor and double the storage at the same price.
[01:48:40.920 --> 01:48:43.640]  And that's when it's cost effective enough for them to do that.
[01:48:44.360 --> 01:48:44.680]  Yeah.
[01:48:44.680 --> 01:48:50.360]  And, you know, presumably it's not actually the $200 is not entirely component costs.
[01:48:50.360 --> 01:48:54.360]  There's, you know, some sort of profit margin built into that.
[01:48:54.360 --> 01:49:02.840]  But yeah, it's a at a percentage level, $100 out of $1,300 is still 7.7% or no point.
[01:49:02.840 --> 01:49:06.120]  The only other thing I think about about 8%, you know, rounding up.
[01:49:06.120 --> 01:49:08.600]  The only other thing I think is like important to add here is that,
[01:49:08.600 --> 01:49:11.160]  like, I don't think anybody should go easy on Apple.
[01:49:11.160 --> 01:49:12.520]  I love that people do these testing.
[01:49:12.520 --> 01:49:13.640]  I love that we get these stories.
[01:49:13.640 --> 01:49:15.320]  I wish the context was better.
[01:49:15.320 --> 01:49:18.440]  But one of the things that I think is bad is that they're not often done
[01:49:18.440 --> 01:49:20.440]  for other companies besides Apple.
[01:49:20.440 --> 01:49:22.520]  And I get that Apple is like a huge headline.
[01:49:22.520 --> 01:49:25.560]  It gets a lot of attention, but it creates two problems.
[01:49:25.560 --> 01:49:28.760]  One is that people who don't like Apple and are buying other stuff
[01:49:28.760 --> 01:49:31.880]  don't have the same amount of information about what they're buying.
[01:49:31.880 --> 01:49:34.360]  But two, it also makes Apple seem abnormal.
[01:49:34.360 --> 01:49:37.960]  Like if you go and look at a bunch of HP and Dell and other laptops,
[01:49:37.960 --> 01:49:41.560]  you will find incredible disparity in all of the components in those.
[01:49:41.560 --> 01:49:43.240]  But it is never, no one even looks for it.
[01:49:43.240 --> 01:49:46.440]  Even reporters at the same big like tech publications
[01:49:46.440 --> 01:49:50.200]  don't do the same kinds of tests on other products that they do on Apple products.
[01:49:50.200 --> 01:49:51.080]  Not all the time.
[01:49:51.080 --> 01:49:51.880]  Not always.
[01:49:51.880 --> 01:49:55.560]  And certainly like YouTubers don't often look for the same kind of things
[01:49:55.560 --> 01:49:57.400]  that we see on Apple review days.
[01:49:57.400 --> 01:50:00.280]  And it would, they should, like I'm saying don't go easy on Apple.
[01:50:00.280 --> 01:50:01.800]  Go just as hard on everybody.
[01:50:01.800 --> 01:50:06.280]  Like Pixel phones, Samsung phones, Dell laptops, HP, whatever.
[01:50:06.280 --> 01:50:09.400]  One, so that those customers with people who are interested in buying it
[01:50:09.400 --> 01:50:11.880]  benefit from the same type of deep testing.
[01:50:11.880 --> 01:50:14.920]  But especially now that AnandTech has been basically defunded.
[01:50:14.920 --> 01:50:17.000]  But also because then you would see like,
[01:50:17.000 --> 01:50:19.080]  is Apple normal here or abnormal here?
[01:50:19.080 --> 01:50:21.720]  And if they're doing something abnormal, that's even bigger story.
[01:50:22.840 --> 01:50:24.120]  Yeah, I think that's true.
[01:50:24.120 --> 01:50:28.280]  You know, I like to call those sort of, what do I like to call it?
[01:50:28.280 --> 01:50:33.240]  Grading on a curve on a curve, you know, where Apple gets is held accountable for
[01:50:33.240 --> 01:50:36.680]  not just A, B, and C, but down the list X, Y, and Z.
[01:50:36.680 --> 01:50:39.480]  And other companies are only held accountable for A, B, and C.
[01:50:39.480 --> 01:50:40.840]  And they never look at X, Y, and Z.
[01:50:40.840 --> 01:50:46.920]  And part of it is Apple is Apple and Apple in the headline, you know, gets, gets attention.
[01:50:46.920 --> 01:50:51.160]  That Dell or whoever else you want to, HP or whoever else does not.
[01:50:51.160 --> 01:50:52.760]  And that's the nature of being on top.
[01:50:52.760 --> 01:50:57.160]  And it's, you know, it's like the Jean-Louis Gasset saying that it's the monkey
[01:50:57.160 --> 01:51:01.000]  who's climbed highest up the pole, who everybody else gets to see their asshole.
[01:51:01.000 --> 01:51:07.400]  I'm sure it sounds better in French, but you get the point that when you're on top,
[01:51:07.400 --> 01:51:09.400]  you get more attention than when you're not.
[01:51:10.120 --> 01:51:15.880]  But it's, you know, it's exacerbating though, that now it's, now it's just out in the air
[01:51:15.880 --> 01:51:22.920]  and people quote unquote know that the M2 max are slower at disk IO than the M1 max
[01:51:22.920 --> 01:51:25.960]  by a significant factor, which is not true.
[01:51:25.960 --> 01:51:28.040]  And it's not that hard to explain it.
[01:51:28.040 --> 01:51:33.320]  It's the lowest end model, you know, and it's for a very, you know, good technical
[01:51:33.320 --> 01:51:35.480]  reason that it only has one chip.
[01:51:35.480 --> 01:51:39.720]  And so it can't read, write in parallel, presumably.
[01:51:39.720 --> 01:51:41.240]  And it's totally fair to dig them for that.
[01:51:41.240 --> 01:51:42.600]  Just make sure you give the context.
[01:51:42.600 --> 01:51:42.920]  Right.
[01:51:42.920 --> 01:51:44.600]  You have to give the context and the context.
[01:51:44.600 --> 01:51:47.480]  It's not like you're asking for a book length footnote.
[01:51:47.480 --> 01:51:49.160]  It's very easy to explain.
[01:51:49.160 --> 01:51:52.600]  The MacBook Airs are not available for purchase yet.
[01:51:52.600 --> 01:51:55.880]  And I can say, you know, it's like a canary test.
[01:51:55.880 --> 01:51:59.400]  I don't, you know, review units are not in my hand yet.
[01:52:00.120 --> 01:52:01.720]  So I don't know.
[01:52:01.720 --> 01:52:07.080]  We don't know if this is true for the MacBook Air, but it almost certainly will be because
[01:52:08.040 --> 01:52:09.720]  it is the M2, right?
[01:52:09.720 --> 01:52:16.440]  And so when they make an M2 with 256 gigabytes of storage, it's almost certainly the exact
[01:52:16.440 --> 01:52:23.160]  same or all, you know, with an exception I'll mention in a second, the it's the same chip
[01:52:23.160 --> 01:52:24.760]  that's going to be in the MacBook Air.
[01:52:24.760 --> 01:52:28.440]  So it's almost certainly going to have the exact same performance characteristics where
[01:52:28.440 --> 01:52:34.360]  the 256 gigabyte SSD one is going to have the same performance characteristics as the
[01:52:34.360 --> 01:52:35.720]  13 inch MacBook Pro.
[01:52:35.720 --> 01:52:39.960]  Now, the difference, I think I'm only looking at the MacBook Air page right now.
[01:52:40.920 --> 01:52:47.400]  The MacBook Air, the new one with the M2 starts at $1,200 and you have to go to the exact
[01:52:47.400 --> 01:52:56.040]  same price, $1,500 to upgrade to one with 512 SSD, which will have the two 256 gigabyte
[01:52:56.040 --> 01:53:00.600]  chips presumably and will have the faster IO than the M1 models.
[01:53:00.600 --> 01:53:09.880]  The difference is with the MacBook Air, they're binning the GPUs and you only get an eight
[01:53:09.880 --> 01:53:17.960]  core GPU on that $1,200 MacBook Air config, whereas I believe all of the 13 inch MacBook
[01:53:17.960 --> 01:53:24.840]  Pros, including the $1,300 model with 256 gigs of storage, still has a 10 core GPU.
[01:53:25.480 --> 01:53:26.680]  Let me double check that before.
[01:53:29.080 --> 01:53:30.040]  But I think that's true.
[01:53:30.040 --> 01:53:33.480]  So there's a there is believe that there's just no binned down MacBook Air is what it
[01:53:33.480 --> 01:53:35.400]  comes back with MacBook Pro.
[01:53:35.400 --> 01:53:40.200]  So there is some aspect of the 13 inch new MacBook Pro that is pro.
[01:53:40.200 --> 01:53:41.560]  It doesn't doesn't.
[01:53:41.560 --> 01:53:46.040]  Yes, it's still all the MacBook Pros that 13 inch all have 10 core GPUs.
[01:53:46.040 --> 01:53:46.840]  I just checked.
[01:53:46.840 --> 01:53:48.200]  So I was correct.
[01:53:48.200 --> 01:53:54.760]  Again, if you're thinking if you're looking at this as a consumer and you you're looking
[01:53:54.760 --> 01:54:00.040]  at the actual storage, your current laptop, whether it's a Mac or whatever, if you're
[01:54:00.040 --> 01:54:05.720]  moving from Windows and you know that you don't need more than 256 gigabytes of storage.
[01:54:06.360 --> 01:54:12.840]  And so you're looking at this $1,200 new MacBook Air with the M2 and thinking, I don't see
[01:54:12.840 --> 01:54:17.480]  why I would spend an extra $200 for 512 gigabytes of storage.
[01:54:17.480 --> 01:54:18.360]  I don't need it.
[01:54:18.360 --> 01:54:21.240]  Will this disk I.O. ruin your experience?
[01:54:21.240 --> 01:54:22.520]  No, you will not notice.
[01:54:23.880 --> 01:54:24.600]  I guarantee it.
[01:54:25.320 --> 01:54:29.800]  Is it possible that there's somebody whose budget is constrained and they want to do
[01:54:29.800 --> 01:54:35.000]  things where disk I.O. matters, but they don't have the $200?
[01:54:35.000 --> 01:54:36.040]  Yes, in theory.
[01:54:36.040 --> 01:54:42.680]  But even then, the performance is not going to be so much slower that you're really going
[01:54:42.680 --> 01:54:44.360]  to adversely affect your life.
[01:54:44.360 --> 01:54:49.720]  There's also if the $200 is so meaningful to you budget wise, I've been there when I
[01:54:49.720 --> 01:54:50.520]  was younger.
[01:54:50.520 --> 01:54:52.920]  Trust me, the disk I.O. is the least of your problems.
[01:54:53.640 --> 01:54:57.000]  And it's like, again, I'm not taking it like I know some people are going to be bothered.
[01:54:57.000 --> 01:54:57.960]  Some people will be hurt by this.
[01:54:57.960 --> 01:55:02.120]  But if you can afford the kind of storage where you notice the file transfer speeds
[01:55:02.120 --> 01:55:06.520]  anyway, you could probably afford the higher like the fast NVMe storage, like whether it's
[01:55:06.520 --> 01:55:10.360]  a thumb drive or an SSD drive or whatever is expensive.
[01:55:10.360 --> 01:55:14.440]  Yeah, I think it's best, you know, in general, I mean, I don't blame Apple.
[01:55:14.440 --> 01:55:20.280]  I'm sure that they, you know, they look at the sales and they know that these low end
[01:55:20.280 --> 01:55:29.320]  configurations, the $1200 MacBook Air and the $1300 13 inch new MacBook Pro sell in
[01:55:29.320 --> 01:55:35.560]  quantities that is worth keeping them in the lineup and slash or the fact that they can
[01:55:35.560 --> 01:55:42.840]  say this is the price they start at gets people in the door, even if they wind up buying the
[01:55:42.840 --> 01:55:46.120]  $1500 configurations with $512.
[01:55:46.120 --> 01:55:51.960]  It is a psychological thing with pricing that knowing that you're you're buying as not the
[01:55:51.960 --> 01:55:55.240]  bottom of the line model, you know, it it matters.
[01:55:55.240 --> 01:55:57.720]  I don't blame Apple for offering these configurations.
[01:55:57.720 --> 01:56:03.720]  I think for people who are buying this $1200 MacBook Air coming out next month, quote unquote,
[01:56:05.160 --> 01:56:07.640]  they're getting a great I think it'll be a great computer.
[01:56:07.640 --> 01:56:08.760]  Again, I haven't tested it yet.
[01:56:08.760 --> 01:56:09.640]  I don't have it in hand.
[01:56:09.640 --> 01:56:13.640]  But, you know, knowing what we know about Apple Silicon Macs and it'll be fine.
[01:56:13.640 --> 01:56:20.920]  But, you know, you're you are getting the bottom of the line new MacBook Air and, you know.
[01:56:20.920 --> 01:56:21.420]  Yeah.
[01:56:22.280 --> 01:56:26.040]  And the new MacBook Air, I mean, like just beyond that, it doesn't have active cooling
[01:56:26.040 --> 01:56:28.600]  and we don't know how the new chassis profiles.
[01:56:28.600 --> 01:56:30.600]  So we don't know what the saturation point is yet.
[01:56:30.600 --> 01:56:34.680]  So it's going to have it's not going to sustain performance as long as the MacBook Pro either
[01:56:34.680 --> 01:56:34.920]  way.
[01:56:34.920 --> 01:56:36.760]  So it's going to be an interesting thing to look at.
[01:56:36.760 --> 01:56:40.360]  You know, what's one of the interesting things is I look at the MacBook Air side by side
[01:56:40.360 --> 01:56:43.880]  with the $1200 base config.
[01:56:43.880 --> 01:56:50.040]  And it's again, it's a $300 upgrade to go to the next one up, which gets you two more
[01:56:50.040 --> 01:56:56.360]  GPU cores and 512 gigabytes of storage instead of 256.
[01:56:56.360 --> 01:57:03.000]  The other little difference that you get is in the $1200 config ships with Apple's, I
[01:57:03.000 --> 01:57:06.040]  believe, existing 30 watt USB-C power adapter.
[01:57:06.040 --> 01:57:15.000]  Yeah, when you upgrade to the $1500 version, you get the new 35 watt dual port USB compact
[01:57:15.000 --> 01:57:15.880]  power adapter.
[01:57:16.520 --> 01:57:18.760]  And I don't think that was clear in the keynote at all.
[01:57:18.760 --> 01:57:22.760]  I'm sure they said something or put a footnote or something, but it sounded to me watching
[01:57:22.760 --> 01:57:29.480]  the keynote live that this is the new 30-ish watt power adapter Apple is shipping.
[01:57:29.480 --> 01:57:31.880]  That to me is a little bit of a nickel and dime move.
[01:57:31.880 --> 01:57:32.520]  I don't know.
[01:57:32.520 --> 01:57:37.320]  And again, again, it's spending Tim Cook's money, but it's such a weird company.
[01:57:37.320 --> 01:57:40.680]  Like they'll spend billions of dollars to chamfer the edge on the phone, and maybe that
[01:57:40.680 --> 01:57:44.680]  does sell enough units that it makes up for it, but then they'll like not include like
[01:57:44.680 --> 01:57:46.600]  the better cable or the better charger.
[01:57:46.600 --> 01:57:52.200]  I think that having a dual port charger, a 35 watt, and again, that's not the fastest,
[01:57:52.200 --> 01:57:54.600]  you know, but most people don't need to fast charge their MacBooks.
[01:57:54.600 --> 01:57:59.320]  It's nice, it's small, it'll fit into open, you know, it doesn't, if you plug it in the
[01:57:59.320 --> 01:58:04.200]  top of the thing in the wall, it won't cover the bottom one, so it fits places.
[01:58:04.200 --> 01:58:10.520]  Having two USB-C ports on that charger is so nice because then you can just plug another
[01:58:10.520 --> 01:58:17.880]  thing in and, you know, it's a really convenient way to have two charging cables from one power
[01:58:17.880 --> 01:58:22.200]  adapter that fits in the wall, and there's all third parties like Anker and all sorts
[01:58:22.200 --> 01:58:28.360]  of other companies have been selling multiple port small wall chargers for years.
[01:58:28.360 --> 01:58:30.920]  It's great that Apple is making one now too.
[01:58:30.920 --> 01:58:36.360]  It really seems nickel and dimey that they're not supplying it with the $1,200 MacBook Air.
[01:58:36.360 --> 01:58:37.320]  That's just...
[01:58:37.320 --> 01:58:42.920]  Yeah, yeah, like their margins, like, of course, they have a fiduciary duty to provide value
[01:58:42.920 --> 01:58:46.680]  to their shareholders and make as much money as possible, all of those things understood.
[01:58:46.680 --> 01:58:51.400]  Like, they spend so much money for things that are experiential, that don't make a huge
[01:58:51.400 --> 01:58:55.080]  difference in the functionality of the product, that make it look nicer, like the unboxing
[01:58:55.080 --> 01:58:58.440]  experiences, they could throw it in a dumb box like every other manufacturer.
[01:58:58.440 --> 01:59:02.040]  But a lot of the boxes they make, especially in the higher end products, are ludicrously
[01:59:02.040 --> 01:59:06.280]  like origami designed, and then to have like these little things, like you call nickel
[01:59:06.280 --> 01:59:12.200]  and dime things, ruin the overall experience because they stand out so much as un-Apple-like.
[01:59:12.200 --> 01:59:13.400]  I just don't understand that.
[01:59:13.400 --> 01:59:18.040]  It would be, I mean, there's all sorts of things, it would be, you know, funny and you
[01:59:18.040 --> 01:59:22.920]  know they wouldn't do it, like, but if you bought the $1,200 MacBook Air, they don't
[01:59:22.920 --> 01:59:25.080]  put it in lesser packaging, right?
[01:59:25.080 --> 01:59:28.200]  Yes, yes, like I'm getting in a brown bag.
[01:59:28.200 --> 01:59:32.040]  You know how, like, when you send something to a friend, I just sent our mutual friend,
[01:59:32.040 --> 01:59:33.320]  Guy English, a thing.
[01:59:33.320 --> 01:59:37.720]  Long story short, I wound up with two play dates, and I'd sent him one of them.
[01:59:37.720 --> 01:59:41.960]  And I just, you know, when you send something to a friend, you just pick up, like, what
[01:59:41.960 --> 01:59:46.920]  I do is I just find an Amazon box that I just got something from the last day, scratch off
[01:59:46.920 --> 01:59:51.000]  all the stuff that was sent to me, and put a new sticker on top with the address of the
[01:59:51.000 --> 01:59:53.880]  person, and wrap up, you know, some used bubble tea.
[01:59:53.880 --> 01:59:57.720]  Imagine if that's how you got your $1,200 MacBook Air, just from the shit that
[01:59:57.720 --> 01:59:59.320]  Like your peasant packaging.
[01:59:59.320 --> 02:00:04.040]  boxes that were in the back of the Apple store from their delivery of paper towels
[02:00:04.040 --> 02:00:08.040]  or something like that for the restroom, and they just stick your MacBook Air in that with
[02:00:08.040 --> 02:00:10.920]  some craft paper and say, here you go, cheapskate.
[02:00:10.920 --> 02:00:11.960]  No, they don't do that.
[02:00:11.960 --> 02:00:15.960]  They give you a box that looks every bit as nice as the highest-end configuration MacBook
[02:00:15.960 --> 02:00:18.120]  Air.
[02:00:18.120 --> 02:00:24.200]  The not giving you the new charger, it just seems, argh, it really, I don't know.
[02:00:24.200 --> 02:00:28.760]  I would like to think that that was a very close decision within Apple, and that somebody
[02:00:28.760 --> 02:00:31.480]  is being appropriately shamed for it.
[02:00:31.480 --> 02:00:35.480]  They're like $5 a unit over a million units, $5 million.
[02:00:35.480 --> 02:00:36.840]  It can't be $5.
[02:00:36.840 --> 02:00:38.200]  I refuse to believe.
[02:00:38.200 --> 02:00:39.560]  No, it's probably pennies.
[02:00:40.520 --> 02:00:41.960]  You know, maybe, I don't know.
[02:00:41.960 --> 02:00:47.720]  But even if it is $5, for God's sake, it's a $1,200 computer, and you're so proud of
[02:00:47.720 --> 02:00:49.880]  this new charger that you put it in the keynote.
[02:00:49.880 --> 02:00:53.800]  You know, if it's nice enough to be in the keynote, it's nice enough to give to everybody
[02:00:53.800 --> 02:00:54.600]  who buys the new machine.
[02:00:54.600 --> 02:00:57.720]  It's like putting an ugly staircase in an Apple store, at that point, why?
[02:00:57.720 --> 02:00:59.400]  Anyway, that's all I've got.
[02:00:59.400 --> 02:01:00.200]  Thank you.
[02:01:00.200 --> 02:01:01.240]  It's always good to talk to you.
[02:01:01.240 --> 02:01:02.040]  It was so good to see you.
[02:01:02.040 --> 02:01:02.360]  You too.
[02:01:02.360 --> 02:01:06.920]  I will thank you, and everybody, of course, should be checking out your YouTube channel
[02:01:06.920 --> 02:01:09.160]  at youtube.com slash Rene Ritchie.
[02:01:09.160 --> 02:01:10.840]  You're also doing lots of work.
[02:01:10.840 --> 02:01:13.800]  I actually watch most of your videos on the Nebula these days.
[02:01:14.360 --> 02:01:14.840]  Oh, thank you.
[02:01:14.840 --> 02:01:17.800]  So tell me about Nebula real quick.
[02:01:18.600 --> 02:01:21.080]  Nebula, again, mutual friend, Dave Wiskus, and I...
[02:01:21.080 --> 02:01:21.560]  Never heard of him.
[02:01:21.560 --> 02:01:22.360]  And a bunch of other...
[02:01:22.360 --> 02:01:23.160]  Never heard of him.
[02:01:23.160 --> 02:01:25.080]  Yeah, he's the guy with the fancy Tony Stark sunglasses.
[02:01:25.080 --> 02:01:26.200]  Ah, okay.
[02:01:26.200 --> 02:01:31.400]  Yeah, so a bunch of mostly educational creators got together and made a platform for streaming
[02:01:31.400 --> 02:01:37.240]  video for stuff that wasn't always the best fit for YouTube, but also no ads, no sponsors,
[02:01:37.240 --> 02:01:40.040]  and it lets us fund a lot of amazing originals.
[02:01:40.040 --> 02:01:43.080]  Like Patrick Williams just did Night of the Coconut, an original movie.
[02:01:43.080 --> 02:01:48.040]  Like a 90-minute long movie about an extra-dimensional coconut who wanted to destroy humanity through
[02:01:48.040 --> 02:01:51.560]  the attainment of clout, and it's hilarious.
[02:01:51.560 --> 02:01:54.920]  So I'm always startled and surprised by the creativity of the people there.
[02:01:54.920 --> 02:01:57.240]  Yeah, it's a great service, and it's...
[02:01:57.240 --> 02:02:03.160]  Again, it is just you subscribe, and the money goes to the actual creators making the thing,
[02:02:03.160 --> 02:02:07.640]  and there's no ads, and it's really just a pleasant viewing experience.
[02:02:07.640 --> 02:02:09.320]  You can see Rene's stuff there.
[02:02:09.320 --> 02:02:13.320]  I will also thank our sponsors for the show in reverse order.
[02:02:13.320 --> 02:02:15.800]  We had Memberful, and we had...
[02:02:15.800 --> 02:02:20.680]  That's where you go to monetize your membership and Collide, where you can do endpoint security
[02:02:20.680 --> 02:02:24.600]  for your Mac, Windows, and Linux laptops in a better way.
[02:02:24.600 --> 02:02:26.600]  And Squarespace, where you can build a website.
[02:02:26.600 --> 02:02:27.240]  My thanks to them.
[02:02:27.240 --> 02:02:27.800]  Thanks, Rene.
[02:02:27.800 --> 02:02:39.560]  Thank you.
