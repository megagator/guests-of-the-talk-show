[00:00.000 --> 00:05.520]  All right, first time guests on the show, rich mogul, welcome to the talk show. Thanks
[00:05.520 --> 00:13.440]  for having me. I famously sometimes don't do a lot of prep for the show. But I do keep
[00:13.440 --> 00:19.120]  a running list of topics. And there in the time since my last episode, there has been
[00:19.120 --> 00:24.960]  a very consistent theme on topics that are here to talk about. And they relate to computer
[00:24.960 --> 00:30.320]  security, privacy, which are not necessarily two sides of the one coin, but are certainly
[00:30.320 --> 00:34.000]  related. And I thought I've been thinking about having you on the show for a long time.
[00:34.000 --> 00:37.680]  And I thought, well, for God's sake, if with a big list of security related topics, why
[00:37.680 --> 00:44.080]  not have rich on the show now? Thanks. Yeah, I mean, it's excited to be here. So for people
[00:44.080 --> 00:47.280]  who aren't familiar with you, what's your background?
[00:47.280 --> 00:58.720]  Alcoholism. Take my kids rock climbing. So now the money is. Yeah, yeah, not at the same
[00:58.720 --> 01:04.560]  time. Okay. Yeah. So I've been doing security for like 20 years at this point. So it's actually
[01:04.560 --> 01:08.240]  a analyst over Gartner. That's when I first started getting involved with Apple stuff,
[01:08.240 --> 01:13.280]  spun out, started my own company about 11 years ago. It's done pretty darn well. Just
[01:13.280 --> 01:20.480]  doing security advisory services and such, and also have a new startup that's doing security
[01:20.480 --> 01:24.240]  stuff over in the cloud. So this has been kind of my security has been bread and butter for
[01:24.240 --> 01:30.400]  a while now pays the bills. And it never, never gets old. It's a growth industry. So
[01:30.400 --> 01:36.640]  it's like plastics. It is, you know, there are, I always say it's like, it's kind of
[01:36.640 --> 01:45.520]  an interesting exercise is what are what are the fields where it's it, you've got a bright
[01:45.520 --> 01:50.720]  future, or at least opportunities ahead. And I would say computer security is one of those,
[01:50.720 --> 01:56.000]  you know, there's all sorts of fields, you know, and it's not funny when when, you know,
[01:56.720 --> 02:01.440]  industries get disrupted, and people lose jobs, etc. It's, you know, but security is one that's
[02:01.440 --> 02:07.840]  good to go. It's weird to see, you know, when I started, like 1520 years ago, it was
[02:07.840 --> 02:11.680]  kind of a small thing. I mean, even for the Apple stuff, we were, you know, almost screaming into
[02:11.680 --> 02:20.320]  the void. And now there's things like Russia, and China and cars and rockets. And so it's been,
[02:20.320 --> 02:24.880]  it's been a pretty wild ride. Yeah, yeah, I don't remember talking about the Russians 20 years ago,
[02:24.880 --> 02:29.440]  or I mean, I remember talking about him, but not as it relates to personal security. See,
[02:29.440 --> 02:33.600]  we knew there was a movie Red Dawn, if you watch that it was a precursor to everything.
[02:35.360 --> 02:41.840]  I haven't watched Red Dawn since I think Red Dawn came out when I was exactly the age of the
[02:41.840 --> 02:46.480]  protagonists of Red Dawn. I'm going to guess Red Dawn without looking at IMDb. I'm going to guess
[02:46.480 --> 02:54.480]  it came out in 1984. I probably because I think you and I are about the same age that it resonated.
[02:54.480 --> 03:02.240]  Let's see. Let's see how close I am. Let's see. Red Dawn. Yep, 1984. Damn.
[03:03.920 --> 03:10.000]  Why in the world I was just talking to a friend about this about it. I can remember very
[03:10.000 --> 03:15.440]  specifically weird things from like the 80s. Like I could tell you exactly what year certain world
[03:15.440 --> 03:20.400]  series were. I believe I was talking it was we were talking about Super Bowls. And I remember
[03:20.400 --> 03:26.160]  that 1984 was the season where Dan Marino broke all sorts of passing records. Like why in the
[03:26.160 --> 03:31.920]  world? I don't remember it vaguely as the mid 80s, but specifically 1984. What a waste of brain
[03:31.920 --> 03:36.800]  cells. Whereas stuff that happened two, three years ago, I'm like, I don't know, I got to go
[03:36.800 --> 03:41.040]  look that up. Like when did the iPhone eight come out? And I got to sit there and count on my fingers
[03:41.040 --> 03:49.200]  like, what year it was? Yeah, I can't remember two days ago. But I remember my exact emotional state
[03:49.200 --> 03:55.200]  when I saw the very first preview to the Michael Keaton Batman. It was it was really exciting going
[03:55.200 --> 04:01.120]  back to the 80s. I just writing my moped after around town when I was 16 years old or 17 after
[04:01.120 --> 04:07.120]  watching Top Gun. I was man I was I was right there on my crotch rocket moped bright orange.
[04:07.120 --> 04:13.040]  It is funny because I remember the Keaton Batman to very vividly and it was in hindsight looking
[04:13.040 --> 04:22.320]  at it. It is so Tim Burton-y and sort of comical and farcical. You know, it's almost it almost
[04:22.320 --> 04:29.280]  verges on the border of a musical, you know, like the scene where Nicholson's Joker comes in and,
[04:29.280 --> 04:36.560]  you know, in the restaurant where Vicki Vale is waiting for him. And his his henchmen are, you
[04:36.560 --> 04:41.840]  know, spray painting all the artwork and knocking stuff over. It's almost a musical, right? Whereas
[04:41.840 --> 04:47.440]  as a kid being used to the Adam West that this is what televised Batman looks like,
[04:48.160 --> 04:54.320]  right? The the the Keaton Batman look like this is this is seriously dark and serious.
[04:55.920 --> 04:58.960]  Well, it's like all those two because you've got Burton or like Zack Snyder,
[04:58.960 --> 05:03.440]  who's now remade Watchmen three, four or five different times because all of his DC movies
[05:03.440 --> 05:07.600]  look the same, right? But with Burton, I mean, he's more creative than that. But it's just that
[05:07.600 --> 05:12.080]  tone. But that was early enough that, you know, that that tone was like new and original.
[05:12.080 --> 05:16.560]  Yeah, I still think casting Michael Keaton as Batman is still one of the masterstrokes of
[05:16.560 --> 05:21.440]  casting and movie history. Like and everybody was like, Oh, my God, this is ridiculous. How can
[05:21.440 --> 05:22.720]  Mr. Mom be Batman?
[05:23.840 --> 05:28.560]  That was not only was that genius, but the way they brought him back for Spider-Man Homecoming
[05:28.560 --> 05:32.480]  and that role. Yeah, yeah, yeah. The maturity and the gravitas he brought to that and that
[05:32.480 --> 05:37.120]  that history for those of us who are old enough. Yeah, sorry, I'm nerding out here. I know you
[05:37.120 --> 05:40.800]  want to talk about security. But he's a terrific actor. That's the thing. That's the thing is that
[05:40.800 --> 05:45.520]  Michael Keaton is just absolutely a terrific actor. And it's one of those things where
[05:45.520 --> 05:49.200]  there's all sorts of serious actors who cannot do comedy. And there's all sorts of actors who
[05:49.200 --> 05:54.720]  can do comedy who, who can do serious stuff. You know, Robin Williams would be a terrific
[05:54.720 --> 05:58.960]  example of that where, my God, when he got serious, it was, you know, he could be,
[05:59.760 --> 06:05.600]  you know, as good as any actor on the planet. Then you got jerks like Chris Hemsworth and
[06:05.600 --> 06:11.120]  Channing Tatum, who are like these buff action gods, and they just act really well. And they
[06:11.120 --> 06:16.720]  are funny as shit. I just saw Bad Times at the Old Royale. And I'm like, Oh, Jesus, where did
[06:16.720 --> 06:20.720]  he come from in this movie? It was amazing. Yeah, I like that movie. That was pretty good. Yeah.
[06:20.720 --> 06:30.320]  reminded me sort of a 90s throwback, you know, like the, the almost theater like limitation of,
[06:30.320 --> 06:36.320]  of, of how few sets there were, you know, like the whole thing took place at the hotel. I don't
[06:36.320 --> 06:40.320]  want to spoil it for people who haven't watched it, but it's, it's very good entertaining. Does
[06:41.600 --> 06:47.120]  like my main thing now these days is just, can I predict exactly where this movie is going or not?
[06:47.120 --> 06:52.400]  And the answer was no. And so I was interested. Well, I mean, that's why I love Last Jedi,
[06:52.400 --> 06:56.080]  which is so controversial for a lot of people, because I had no idea where that movie was going
[06:56.080 --> 07:01.600]  till the end. Yeah, I didn't love it. But I liked it, you know, and I don't expect them,
[07:01.600 --> 07:05.600]  you know, if I wanted to, you know, I should have devoted my life to trying to become a writer for
[07:06.240 --> 07:10.800]  Star Wars if I wanted to. I don't know, there's an awful lot of people who seem to want to write,
[07:10.800 --> 07:15.920]  write the next Star Wars movie. And it's like, you know, just sit back and enjoy. Yep.
[07:17.760 --> 07:22.400]  I have a one piece of follow up. There was on the last episode, I mentioned offhandedly. And of
[07:22.400 --> 07:27.120]  course, I mentioned something offhand, it suddenly becomes a big story on Mac rumors,
[07:27.120 --> 07:32.640]  that that somebody told me, somebody who would know told me, basically, that Apple has been
[07:32.640 --> 07:39.200]  selling Apple TV pretty much since its inception since the original Apple TV, that they more or
[07:39.200 --> 07:44.560]  less sell them at cost. And they don't, they're not a big profit maker, and that they're selling
[07:44.560 --> 07:52.640]  the HomePod for under cost. And this got picked up by Mac rumors. And oh, my God, did I hear from
[07:52.640 --> 07:58.320]  people who said no way Apple doesn't sell anything, you know, everything they have is super high
[07:58.320 --> 08:02.960]  margin, blah, blah, blah, I can't prove it. I'm not saying that I know this for a fact. All I'm
[08:02.960 --> 08:08.800]  saying is a source that I trust very much said so. And to me, it makes a lot of sense as to why
[08:08.800 --> 08:16.560]  HomePod is so much more expensive than its ostensible competition. You know, that's all
[08:16.560 --> 08:21.200]  I can say about it is, boy, oh, boy, did I hear it from people who said Apple doesn't sell anything
[08:21.200 --> 08:26.960]  at less than a high margin. And I just think people are underestimating like when 99% of
[08:26.960 --> 08:33.760]  everything they sell is either a Mac, iPad or, or, of course, iPhone, that something like HomePod
[08:33.760 --> 08:42.240]  or Apple TV that sells in relatively minuscule amounts, if it was selling at cost or about cost,
[08:42.240 --> 08:44.160]  would have no effect on their margin whatsoever.
[08:46.320 --> 08:51.600]  I think the I mean, I use both of those devices, we're all in on that. And there's such a clear
[08:51.600 --> 08:56.560]  quality difference. I mean, not just the build construction, but the, the reliability that most
[08:56.560 --> 09:01.200]  of the other things that I've tried, including the the Alexis, it's pretty noticeable would not
[09:01.200 --> 09:06.000]  surprise me at all. Or at least in the first year or two of the manufacturing of those.
[09:06.000 --> 09:10.160]  Yeah. Well, and I just think that, you know, in the backstory that I've heard on HomePod
[09:10.160 --> 09:18.000]  makes a lot of sense was that initially, it was meant as a sort of, you know, like a more of a
[09:18.000 --> 09:22.960]  Apple TV peripheral where you'd plug, you know, hook it up, or maybe not hook it up with wires,
[09:22.960 --> 09:26.640]  but that it was meant to be like the speakers for your Apple TV or something like that. But it
[09:26.640 --> 09:31.840]  really was I get to it's another one of those cases where my rule of thumb of take Apple at
[09:31.840 --> 09:37.600]  its word, it's usually the truth where their description of it as primarily a speaker, not
[09:37.600 --> 09:43.760]  not really a, hey, this is a Siri, this is our answer to the talking devices thing. You know,
[09:43.760 --> 09:50.480]  that first and foremost, it's a, you know, a serious, high quality speaker system. And then it
[09:50.480 --> 09:55.920]  just happens to have Siri as the interface. Secondarily, that's, that's honestly the truth of
[09:55.920 --> 10:01.200]  it. I don't know. People make a lot of hay over the, you know, the market share this thing. But
[10:01.200 --> 10:05.840]  there's, there's no question. I mean, Apple has there's no way that Apple introduced this
[10:05.840 --> 10:15.440]  thing at $350 and expected it to compete in a unit sale comparison with Amazon things that cost $60
[10:15.440 --> 10:19.840]  or Google's low end, Google home things, Google voice, whatever they call them.
[10:19.840 --> 10:27.520]  Well, you write about this all the time. I mean, one of my personal pet peeves is Apple come out
[10:27.520 --> 10:32.240]  with a new product and everyone assumes because it's not a one to one parody with something else
[10:32.240 --> 10:38.640]  that's already on the market down to price and features. And, and it, if it doesn't work exactly
[10:38.640 --> 10:43.120]  the way the thing that was there first worked, then there's got to be a flaw with it. And yet
[10:43.760 --> 10:49.600]  they built all these class defining devices. And in many cases have taken the time to nurture those
[10:49.600 --> 10:53.200]  through the process. I mean, some of them, they drop like, but whatever, what was the high-fi
[10:53.760 --> 10:58.960]  stereo thing and a few others. But, um, I mean, it, it seemed pretty clear that this
[10:58.960 --> 11:04.240]  was a long-term bet from the beginning on the home pods at least. No. Have you ever heard
[11:04.240 --> 11:10.480]  much? You ever heard my story about Steve jobs in the iPod high-fi? No. I had a friend who worked at
[11:10.480 --> 11:18.480]  Apple and long story short, had to have a meeting in jobs is sweet and, uh, got up there, you know,
[11:18.480 --> 11:26.640]  five minutes early, of course. And, uh, you know, checked in with jobs as personal assistant and,
[11:26.640 --> 11:31.920]  you know, w relatively small, you know, it's, it's, you know, I think Tim Cook's, I've seen
[11:31.920 --> 11:36.000]  Tim Cook's office. I don't know if it's the new one, I guess it was his old one at infinite loop,
[11:36.000 --> 11:41.680]  not that the new one at the ring, but, uh, you know, very humble, you know, relative to,
[11:41.680 --> 11:47.920]  to what it could be. Um, but he's, you know, got a receptionist or a personal assistant and then
[11:47.920 --> 11:51.600]  he's got his own personal office and then over to the other side, there's, there's, you know,
[11:51.600 --> 11:55.520]  like a boardroom type table where the meeting was going to be, but he could look right into
[11:55.520 --> 12:04.080]  jobs as office. I forget what year this was, maybe, you know, 2009, 2010. Um, and he said
[12:04.080 --> 12:09.920]  he expected it to be like this austere monk, like, you know, there'd be nothing in there except a
[12:09.920 --> 12:17.520]  glass desk with like mint condition, one mint condition, iMac and nothing else. And he said,
[12:17.520 --> 12:22.960]  instead it was a relatively small office and it was just stacked from floor to ceiling with
[12:22.960 --> 12:30.400]  boxes of the now at that point, discontinued iPod high-fi like they discontinued it,
[12:30.400 --> 12:34.080]  but jobs loved it so much where he was like, well, put 40 of them in my office,
[12:35.360 --> 12:43.600]  just like in the box, like the retail boxes, like that's right. That's where they were storing them
[12:43.600 --> 12:47.680]  and it like threw him off and like, he was like, I got to get my head in the game because this is
[12:47.680 --> 12:54.240]  a big meeting. That's awesome. I have no idea what happened to those iPod high-fives, but that's my,
[12:55.280 --> 13:00.560]  doesn't Jason still have one? Uh, he probably does. I don't know. I never bought one cause
[13:00.560 --> 13:07.520]  I never really had an interest at that time of a, I never had any kind of plug my iPod into a
[13:07.520 --> 13:13.520]  speaker system thing. I don't know why, but I didn't, I was the same way, but I worked at a desk
[13:13.520 --> 13:18.160]  and so I had iTunes and I had computer speakers and that was fine. Yeah. That sort of was my
[13:18.160 --> 13:24.480]  situation was I already had decent speakers, decent enough speakers connected to my computer
[13:24.480 --> 13:29.040]  in my office, my home office, and there was no other nowhere else in the house where we needed
[13:29.040 --> 13:33.600]  speakers. So I never really got one, but everybody said I had good sound. I don't know.
[13:33.600 --> 13:41.600]  So small consolation. All right, let's get started though. Let's see first on my list.
[13:42.800 --> 13:48.880]  Well, there is a little bit of other news. A WWDC 2009 Mac rumors has published their best
[13:48.880 --> 13:54.640]  guests pretty educated that it's going to be June three to seven in San Jose. No surprise there.
[13:56.240 --> 14:00.800]  There's sort of a cottage industry and guessing the WWDC dates because people want to see if they
[14:00.800 --> 14:06.720]  can get, you know, like a cheaper hotel room or, you know, book it in advance or whatever. And
[14:06.720 --> 14:11.360]  everybody I know has been guessing June three to seven for months. I mean, going back to December,
[14:12.960 --> 14:18.800]  but Mac rumors uncovered some like, like a wherever, whatever the name of the
[14:19.440 --> 14:25.600]  public field is where the bash, the annual bash is held. There's already, there was already
[14:25.600 --> 14:30.480]  something on the schedule for the Thursday of that week and it even had Apple's name on it,
[14:30.480 --> 14:36.400]  which is sort of like an up. Oh, but the big tell to me, and I didn't know, I didn't realize this
[14:36.400 --> 14:42.320]  was that O'Reilly is holding something called the velocity conference the next, the very next week
[14:42.320 --> 14:48.240]  at San Jose, because that would be the only other guests that would be a normally scheduled WWDC
[14:48.240 --> 14:55.360]  would be the week starting June 10th. So, you know, I feel like this is a lock. I've actually
[14:55.360 --> 15:01.600]  made a hotel reservation based on this. Do you go to WWDC? I usually fly in for like a day. I
[15:01.600 --> 15:06.640]  will have me come for the, uh, the keynote stuff, but I tend not to have the time to stay cause that
[15:06.640 --> 15:11.520]  time of year just with the security industry stuff going on. Yeah. Yeah. I think I usually run into
[15:11.520 --> 15:17.120]  you there. Yeah. Yeah. Yeah. Most shares the, um, I'm so glad they moved it though. Like, uh,
[15:17.120 --> 15:21.040]  I have to go out in a couple of weeks, San Francisco for our big security trade show
[15:21.040 --> 15:26.320]  conference of the year. Uh, hotel rooms are for anything within walking distance of Moscone
[15:26.320 --> 15:31.440]  between 700 and a thousand dollars a night. Oh my God. That's nuts. It actually makes San Jose's
[15:32.080 --> 15:39.280]  four to five 50 rates look reasonable. I got friends staying in Oakland for 500 a night.
[15:41.280 --> 15:46.320]  That's the discount. The discount is you stay at Oakland. You're probably a good,
[15:46.320 --> 15:51.120]  I've taken the Muni for that because I've flown into, I used to fly into Oakland all the time on
[15:51.120 --> 15:58.160]  Southwest. So I know that Muni trip, it's like about 30 minutes and then you get off at, uh,
[15:58.160 --> 16:03.600]  right on market street. If you time it right. If I time it right. Right. Yeah. Uh, I mean,
[16:03.600 --> 16:07.760]  it's not super inconvenient, but it's not certainly not super convenient, but yeah,
[16:07.760 --> 16:13.760]  when the, all the hotels are $700 to start and, and the ones that are lower end in that area,
[16:13.760 --> 16:22.240]  are not, are not $700 a night hotels. It's some of the places I've stayed over the years when,
[16:22.240 --> 16:25.440]  especially when Mac world was still over there and because I have to pay my own way to those.
[16:26.640 --> 16:32.000]  The, uh, yeah, there wasn't not good when they first moved, when Apple first moved it to San
[16:32.000 --> 16:38.880]  Jose two years ago. Um, I'm not, Mr. I'm not miscounting, right? We've had two, two and
[16:38.880 --> 16:46.400]  counting. This will be the third WWDC back in San Jose, I believe. Speaking of my shoddy
[16:46.400 --> 16:53.040]  modern memory, pretty sure. So, cause I skipped one. Yeah, I'm almost sure. Went to one. I remember
[16:53.040 --> 16:57.520]  the first year, two years ago, they even mentioned, Hey, this should be more affordable. You know,
[16:57.520 --> 17:02.560]  San Jose should be more affordable compared to San Francisco. And it's like at this point,
[17:02.560 --> 17:08.880]  the hotels for San Jose are more expensive now than San Francisco was three years ago.
[17:08.880 --> 17:14.000]  So it's like, what's the point. But then if you like price out San Francisco, it is like,
[17:14.000 --> 17:19.920]  Oh my God, this is crazy land. It is. It depends on what it is. Like I've had to go out for
[17:19.920 --> 17:23.360]  meetings with Apple on occasion. I haven't done those in a while, but when I was like,
[17:23.360 --> 17:26.880]  depending on the amount of warning, it was a hundred dollars a night for halfway decent
[17:26.880 --> 17:31.200]  room or $500 a night because somebody had some event going on. So it's like,
[17:31.200 --> 17:35.680]  because somebody had some event going on in the Valley. Yeah, it definitely varies tremendously.
[17:39.200 --> 17:43.280]  I don't know what this, I guess I'm glad it's not in San Francisco anymore. I,
[17:43.280 --> 17:48.880]  I like San Francisco as a city better than San Jose, but my God, the cost is just,
[17:48.880 --> 17:55.040]  it's just off the charts. Yeah. I think they both suck. I mean, no offense to everybody listening
[17:55.040 --> 18:00.880]  in the area, but at least that the Moscone area, San Francisco, if I never have to go back there,
[18:00.880 --> 18:05.280]  I'd be fine. And it's, you know, what sucks. I tell you what really is sucks is getting from
[18:05.280 --> 18:10.560]  SFO to San Jose. I guess I got to get, I guess I got to get an Uber last year. I made the huge
[18:10.560 --> 18:15.360]  mistake and just got in a cab and I think I should have known from the year before. Don't,
[18:15.360 --> 18:22.000]  don't do it. Cause it's like $110 cab ride. There's more with traffic. It was like ridiculous.
[18:22.000 --> 18:29.440]  And it's not that far, but you know, I guess cause they are technically San Francisco cabs,
[18:29.440 --> 18:34.560]  San Jose is, you know, you know, I don't know, you cross some sort of perimeter of,
[18:34.560 --> 18:40.640]  this is a normal San Francisco cab ride and all of a sudden it's like $110 cab ride. And you already
[18:40.640 --> 18:46.960]  feel, I already feel like, like there's a vacuum sucking money out of my pocket. Like you're not
[18:46.960 --> 18:51.920]  flying to San Jose airport. Cause that's, I mean, it's right there. I could, in theory, it is
[18:51.920 --> 18:59.520]  extremely close and convenient, but from Philadelphia, it's it, even with the hassle of
[18:59.520 --> 19:04.080]  getting from San Jose to SFO, it's, there's no direct flights from Philly. So I'd rather have
[19:04.080 --> 19:12.480]  a direct flight to SFO and deal with getting there. Then, then, then have a, a stop. I don't
[19:12.480 --> 19:18.080]  know, as I've gotten older, I have become almost allergic to anything other than nonstop flights,
[19:18.080 --> 19:25.040]  mostly be, not necessarily because of it takes longer, but because it just, it doubles your
[19:25.040 --> 19:29.760]  chances of something going wrong, right. A mechanical problem with the plane, weather
[19:29.760 --> 19:35.600]  from the inbound city, you know, whether at the city that you're at, you know, that anything can
[19:35.600 --> 19:41.840]  happen to delay it. Yeah. I mean, I fly two, three times a month. I'll take the 6am direct over a,
[19:42.400 --> 19:46.960]  you know, a 10am with a connection or something, even if it's less convenient. Yeah. And,
[19:46.960 --> 19:50.880]  and never, ever, ever check bags. If you have a connecting flight or you're, I mean, that's just,
[19:50.880 --> 19:55.840]  that's another one too. Right. And for, for, uh, cause, uh, my wife always comes to WWDC to
[19:55.840 --> 20:00.640]  help with the live show and stuff. And so we definitely check bags and yes, checking bags with
[20:00.640 --> 20:07.280]  a connection is just, you might as well just kiss your luggage. Goodbye. It's like rolling the dice.
[20:08.240 --> 20:12.880]  Yeah. I've been known to pack, spare clothes and toiletries in my carry on if I have to check back.
[20:12.880 --> 20:20.160]  Yeah. Uh, all right. First, first topic, big one is this, I almost forgot to put on the list,
[20:20.160 --> 20:25.280]  but the Jeff Bezos story with the national enquirer, which, uh, when I first started
[20:25.280 --> 20:30.720]  writing about it on daring fireball, it was a week old at least. And I had sort of ignored it
[20:30.720 --> 20:37.120]  because initially I had filed it under sort of personal gossip, which I tend to stay away with,
[20:37.120 --> 20:43.680]  not out of, I'm not trying to be holier than now, but it's just not my bag, you know, that if
[20:43.680 --> 20:49.360]  so and so is an industry famous industry executive and they are, you know, having marital
[20:49.920 --> 20:54.560]  strife or, or running around on their spouse or something like that, that's just not
[20:54.560 --> 20:59.040]  daring fireball material. And I sort of filed it under that and didn't really give it much thought,
[20:59.760 --> 21:06.400]  but then, wow. Uh, Bezos, I'm sure everybody listening to this knows the basic gist of it,
[21:06.400 --> 21:12.960]  where Bezos, uh, wrote a post on medium, which is interesting. And I think it was a sort of a,
[21:12.960 --> 21:18.400]  I'm sort of anti medium in general and really wish more people would have their own blogs,
[21:18.400 --> 21:21.840]  but this was sort of a perfect use of medium because I don't think it would have been
[21:22.480 --> 21:29.920]  appropriate at all for him to use either the Amazon, some sort of Amazon blog or somehow
[21:29.920 --> 21:34.400]  like publish an op ed in the Washington post, which he owns. I don't think either of the,
[21:34.400 --> 21:37.760]  I think both of those would have been very inappropriate. I've go so far to say,
[21:37.760 --> 21:44.960]  and I'm sure he knows it too. So having medium as this neutral platform and then being able to
[21:44.960 --> 21:50.880]  tweet the link and every, you know, his Twitter is verified like his tweet while it didn't contain
[21:50.880 --> 21:56.960]  the meat of the subject was the sort of, well, this was definitely Bezos. This isn't somebody
[21:56.960 --> 22:03.840]  saying they're Jeff Bezos, um, more or less coming out and having email proof that the
[22:03.840 --> 22:10.320]  national enquirer was trying to extort him into saying, uh, things that weren't true,
[22:10.320 --> 22:18.480]  lest they reveal, uh, even more personal information, texts, pictures, uh, from his,
[22:18.480 --> 22:26.560]  uh, uh, girlfriend, you know, as, as like coming out of the, I'll put my security hat on instead
[22:26.560 --> 22:33.120]  of my movie fan boy hat. Um, and this is a really fascinating story because you look between the
[22:33.120 --> 22:40.560]  lines and like one of the problems in security, you get really paranoid and you'll hear zebras.
[22:40.560 --> 22:45.600]  Um, you know, you'll think zebras when you hear those, those hoof beats, uh, and then, but then
[22:45.600 --> 22:53.120]  sometimes it's a zebra and boy, this, this sounds like a zebra to me because the story of the
[22:53.120 --> 22:58.160]  brother of the, you know, of the woman he was having an affair with, uh, somehow getting her
[22:58.160 --> 23:02.160]  texts and releasing it in that way, maybe he was responsible for it, but the way all of this lines
[23:02.160 --> 23:08.720]  up and the Trump stuff and the Saudi Arabia stuff, uh, I suspect it's not the extreme of the
[23:08.720 --> 23:15.040]  conspiracy, but I mean, it really, I don't know. I, I like to sit back and wait and see how these
[23:15.040 --> 23:20.960]  stories develop versus jumping to conclusions. So the basic story is that, and it seems as though
[23:20.960 --> 23:25.680]  everybody is in agreement at this point, although it's not, there's been no conclusive proof that
[23:25.680 --> 23:33.440]  her brother somehow is, was the one who, who released the, these text messages, which I'll
[23:33.440 --> 23:38.400]  come back, which phrase I'll come back to in a moment and photos exchange between the two,
[23:38.400 --> 23:45.760]  to the national enquirer. Um, and it just bizarrely, I mean, it just like, what are the
[23:45.760 --> 23:53.040]  odds that Jeff Bezos is girlfriend would have a brother who is in a long time associate of Roger
[23:53.040 --> 23:59.440]  stone, who's now been indicted and a well-known Trump supporter, the most innocent looking,
[23:59.440 --> 24:08.560]  walking out of jail photo in history with Trump being a, a, a vocal and continuous, uh,
[24:08.560 --> 24:15.760]  critic of the Washington post and Bezos is ownership thereof. Uh, Trump seems convinced
[24:15.760 --> 24:20.560]  that, you know, uh, Trump had clearly, whatever you think of Trump politically,
[24:20.560 --> 24:25.040]  he clearly comes at this from the perspective of, if you're a billionaire who owns a newspaper,
[24:25.040 --> 24:32.960]  of course you wield your ownership to pursue personal vendettas, uh, which is a,
[24:32.960 --> 24:38.560]  not out of the Washington post should work and be not out. Jeff Bezos, you know, by all,
[24:38.560 --> 24:44.000]  by all accounts and appearances, Jeff Bezos is doing is his ownership of the Washington post
[24:44.000 --> 24:52.160]  is editorially is completely hands off. He is, you know, and, and knowing that, you know,
[24:52.160 --> 24:56.720]  I don't know anybody personally at the Washington post, but, uh, you know, from what I know of,
[24:56.720 --> 25:01.440]  of their leadership that, you know, they wouldn't stand for it, you know, they're not going to,
[25:01.440 --> 25:04.640]  but it like good luck convincing Donald Trump of that.
[25:06.240 --> 25:10.160]  Well, I mean, it's, it's inconceivable to someone like, like him. I mean, look at,
[25:10.160 --> 25:14.320]  let me look at the history with AMI and the national inquirer, which has directly been
[25:14.320 --> 25:18.240]  used. So, so why would he possibly think that Bezos would have ethics or something?
[25:18.240 --> 25:22.640]  Right. And so Trump doesn't own the national inquirer, but he's longtime friends with
[25:22.640 --> 25:30.560]  the guy who does, whose name unbelievably is David pecker, which has used his friendship,
[25:31.280 --> 25:35.200]  not even ownership, but just friendship with the owner of the national inquirer to a
[25:35.200 --> 25:40.640]  bury stories damaging to him famously. And now part of the Mueller investigation in terms of
[25:40.640 --> 25:46.240]  the payoffs that they made to keep women, that Donald Trump had affairs with their stories,
[25:46.240 --> 25:50.560]  you know, what do they call it? Catch and kill where the national inquirer would give them
[25:50.560 --> 25:55.680]  a hundred thousand dollars for the exclusive rights to their story. And then, uh, just bury
[25:55.680 --> 26:02.480]  it, put it in a literally put it in a safe and use the national inquirer to, to, uh, slander
[26:02.480 --> 26:09.040]  ice in, in the non-legal term, let's say in the colloquially Donald Trump's enemies, for example,
[26:09.040 --> 26:13.040]  national inquirer was the publication that ran on the front page, on the cover,
[26:13.600 --> 26:19.520]  a story alleging that Ted Cruz's father, uh, helped kill JFK.
[26:21.520 --> 26:27.200]  Oh, by the way, I found the headline. All right. Bezos exposes pecker exposes. How could I forget?
[26:27.200 --> 26:34.480]  Bezos exposes pecker. Well, and so the most stunning thing to me, other than that is,
[26:34.480 --> 26:38.640]  I mean, you write that headline, you retire. I mean, it doesn't get any better than that,
[26:38.640 --> 26:44.640]  but, um, the, uh, I can't believe anything that inquire publishes is even true. Like,
[26:44.640 --> 26:51.360]  I just assumed that was fiction. It was like the onion for disparaging there.
[26:51.360 --> 26:55.840]  See, I knew that that's not the case. So like the weekly world news, I don't even know if
[26:55.840 --> 27:00.320]  they still exist, but in terms of like the Heyday of supermarket tabloids, the weekly world news was
[27:00.320 --> 27:05.040]  the one that was like the onion where they had like bat boy and, and aliens and stuff like that.
[27:05.040 --> 27:11.760]  The national inquirer, uh, because they deal with real people and could therefore be sued for real
[27:11.760 --> 27:18.240]  money, you know, plays by different standards and, and famously, I would say one of the biggest,
[27:18.240 --> 27:22.960]  you know, bits of, you know, I don't want to say history altering, but certainly affected the
[27:22.960 --> 27:28.320]  political landscape was that they were the publication that revealed that John Edwards
[27:28.320 --> 27:41.040]  was cheating on his wife. I forgot about that. You know, who, who was in 2008, uh, or was he,
[27:41.040 --> 27:48.080]  was he Kerry's vice president in 2004 nominee? I believe he was right. I think so. And then in
[27:48.080 --> 27:54.560]  2008, everybody remembers 2008 as a very close democratic, uh, primary between Hillary Clinton
[27:54.560 --> 28:01.280]  and Barack Obama. Uh, but John Edwards was in that. I mean, and it was, he was, it was a three,
[28:01.280 --> 28:08.400]  a very serious three way race for quite a while. I mean, it's not, you know, certainly not out of,
[28:08.400 --> 28:13.760]  uh, the imagination that he could have been vice president under John Kerry or, or presidential
[28:13.760 --> 28:20.160]  nominee in 2008, but they broke the story that he was cheating on his wife. Uh, you know, no,
[28:20.880 --> 28:23.440]  I, you know, I was, there's probably a lot of people like you who think everything in the
[28:23.440 --> 28:29.600]  enquirers is fake or made up, but it's, it's not, it's, you know, I don't know that it's,
[28:29.600 --> 28:36.240]  I don't want to swear that everything is the God off, you know, the exact truth, but it's,
[28:36.240 --> 28:40.160]  you know, it's more true than not in terms of what they, what they print because they're,
[28:40.160 --> 28:46.480]  you know, liable for slander and libel. Well, and this, this kind of spun off the conversation,
[28:47.120 --> 28:51.920]  you know, how, how did they potentially get those texts? Right. That is right. And,
[28:51.920 --> 28:57.840]  and my thing that I wrote about, and I've always, I really wish that publications would get more
[28:57.840 --> 29:05.680]  serious about it is what, what format, what, what medium were these quote unquote texts sent as?
[29:05.680 --> 29:10.320]  I would think if you really, you know, most people would think by, by a technical definition,
[29:10.320 --> 29:18.320]  a quote unquote text message is an SMS message or MMS, whatever you want to say. But, you know,
[29:18.320 --> 29:24.560]  that's a text, but colloquially people call texting pretty much anything that has DMS.
[29:25.280 --> 29:32.560]  And they certainly, people certainly refer to I, I, I message messages as quote unquote texts,
[29:32.560 --> 29:39.200]  quote unquote texts, which is, you know, it's only natural for people to say that because Apple,
[29:39.200 --> 29:44.800]  one of the reasons I message has become so successful, so popular is that Apple
[29:45.760 --> 29:54.080]  integrated it with the with the same app, you know, that's used for text messaging messages.
[29:54.080 --> 29:58.720]  So if you send your text, your actual SMS text messages with the same app as I message,
[29:58.720 --> 30:03.520]  it's only natural to refer to them as text. But once you get to the technical level of how did
[30:03.520 --> 30:07.680]  they get these, boy, is there a big difference between I message and SMS?
[30:08.480 --> 30:16.320]  Yeah, I mean, they're night and day SMS being completely broken and insecure at this point. And
[30:16.320 --> 30:22.160]  I message, you know, not perfect, but, but really good. I mean, almost as close as you can get
[30:22.160 --> 30:28.880]  without introducing barriers to usability, right? That way, that would take away that, that seamless
[30:28.880 --> 30:36.320]  experience. You know, and the big one is that I message is end to end encrypted. Where the
[30:36.320 --> 30:41.360]  encryption, you know, Apple doesn't see the messages now, we, one of the reasons I want you
[30:41.360 --> 30:47.600]  on this to show is, is that doesn't mean that there's no way to get them from your iCloud
[30:47.600 --> 30:53.760]  account. But they're not on your iCloud account stored on encrypted. And there's no way that
[30:53.760 --> 30:59.280]  Apple sees them. They like I send you an I message, it goes, you know, I type it on my phone.
[31:00.000 --> 31:05.920]  And it is encrypted as it leaves my phone. And it isn't, it isn't decrypted until it gets to
[31:05.920 --> 31:12.000]  your devices. Yeah, and it's a really, really interesting way that they set that up where
[31:12.000 --> 31:17.600]  you've got these key rings. So think of it as a ring of your house keys. And every time you
[31:18.240 --> 31:24.240]  basically are sending a message to somebody, and I'm dramatically oversimplifying, but your every
[31:24.240 --> 31:28.640]  message is encrypted with the key for that person and device. And these are all different. And the
[31:28.640 --> 31:33.280]  encryption is entangled with device IDs, depending on kind of which versions of things you're on and
[31:33.280 --> 31:37.760]  which things you're using. Obviously, it's stronger on on iPhones and iPads, and it's
[31:37.760 --> 31:43.200]  going to be a bit weaker on Macs, if you've got those synced up, as well as now we've got it up,
[31:43.200 --> 31:48.640]  as you said, stored up in iCloud, but encrypted in and no keys up in iCloud. And so it's all
[31:48.640 --> 31:54.880]  device to device keys exchanged. So in a situation like this, for somebody else to gain access to
[31:54.880 --> 32:00.160]  those, if it's, if it's SMS, stupid easy, just go to the carrier and you get a record of it,
[32:00.160 --> 32:05.280]  or you hack the phone. For iPhone, you either have to hack the device, which we know is possible,
[32:05.280 --> 32:10.000]  but like kind of nation state level stuff, for the most part or very serious attacker level stuff.
[32:10.640 --> 32:14.560]  You have to have physical access to the device. But even then, I mean,
[32:14.560 --> 32:19.520]  was he doing screenshots and then forwarding these on via email to the enquirer? Like somehow
[32:19.520 --> 32:25.440]  it sounds like the enquirer has those texts and has those images, which means somehow they weren't
[32:25.440 --> 32:31.040]  just viewed and conveyed verbally to them. They were actually forwarded on and that's a big deal.
[32:31.040 --> 32:36.880]  Yeah. So that involves that means some deeper level of access if it was iMessage.
[32:36.880 --> 32:43.680]  Yeah. And, uh, well, but one of the things, one of the things that is game over is device access,
[32:43.680 --> 32:47.840]  right? So if her brother, if she had a relationship where she trusted her brother,
[32:47.840 --> 32:54.400]  she being Bezos's girlfriend, um, if she, if her brother, she trusted her brother enough that he
[32:54.400 --> 33:00.240]  could have her phone and either knew her passcode or God forbid she doesn't even use a passcode,
[33:00.240 --> 33:05.600]  but if he could get into her phone or could get her to say, Hey, can I use your phone for a second?
[33:05.600 --> 33:08.400]  You know, let me look something up. And she trusted him enough for that. It's
[33:08.400 --> 33:12.800]  game over no matter what you're using. If I'm hanging out with the president's
[33:12.800 --> 33:18.560]  public enemy number three and my brother's friends with his like buddy, Roger Stone,
[33:18.560 --> 33:23.600]  I'm probably not going to let my sister touch log into my devices, but yeah, I mean, that's
[33:24.400 --> 33:28.240]  feasible. And, um, and there's no interception by the way, just to like,
[33:28.240 --> 33:32.560]  maybe he could trick her and add a device, but then you have to approve that and put in your wife.
[33:32.560 --> 33:36.720]  Like it's really hard to, it's doable. It's not, not easy.
[33:36.720 --> 33:43.840]  Yeah. Uh, I do wonder though how he exported them. I like, I do want to know, like, I,
[33:43.840 --> 33:51.200]  I don't want to, I don't want to see the pictures, honestly. Uh, but I do want to know how, like how
[33:52.000 --> 33:55.760]  I still want to know, even if he's the guilty party, you know, even if that's true,
[33:55.760 --> 34:00.800]  I still want to know how he got the pictures. Did he screenshot them? Did he? And if so,
[34:00.800 --> 34:05.840]  how did he get the screenshots off her phone? Right. It's, you know, did he take pictures
[34:05.840 --> 34:11.120]  of her phone with his phone? Is that, you know, did she leave her Mac unlocked during Thanksgiving?
[34:12.320 --> 34:15.760]  Who knows? Right. Because it doesn't have to be your phone. It could be something like that.
[34:15.760 --> 34:21.360]  Then there was even a story along those lines in the, uh, I don't forget. I forget the title,
[34:21.360 --> 34:27.920]  the latest tell all book to come out of a former Trump administration official, uh, some low level
[34:27.920 --> 34:33.760]  guy who was like a speech writer who got called into, um, Kellyanne Conway's office. Do you see
[34:33.760 --> 34:42.640]  this story where, uh, I'll have to look up his name, but, uh, basically he needed to write a
[34:42.640 --> 34:48.720]  speech and he came into Kellyanne Conway's office and she said, here, just use my Mac book. And,
[34:48.720 --> 34:54.080]  um, she opened her Mac or was open or whatever. And he sat there and was typing this thing on
[34:54.080 --> 34:58.640]  her Mac book air and she's over at her desk. He's like at a table, you know, she has a fair,
[34:58.640 --> 35:02.640]  you know, she's high enough up to, she has a fairly spacious office in the West wing
[35:03.520 --> 35:10.720]  as he's writing, doing this speech writing thing, her he's seeing all these I messaged alerts come
[35:10.720 --> 35:16.960]  in and notification center. And she's totally like blabbing inappropriate stuff to the press.
[35:16.960 --> 35:24.640]  Oh, I did hear about that. Yeah. Uh, just, and, and you know, like, I don't think she's,
[35:24.640 --> 35:31.120]  she's not a stupid woman. Uh, I just think it was easily, she just easily forgot, you know, that,
[35:31.120 --> 35:36.720]  that she was logged into iMessage on that and that whatever you're doing on your phone and
[35:36.720 --> 35:42.000]  iMessage is going to, if, you know, if you're signed into iMessage on your Mac too is going to,
[35:42.000 --> 35:48.080]  is it's going to be mirrored over there. You know, I mean, mistakes happen. I was
[35:48.080 --> 35:52.880]  given a presentation the other day for off my iPad pro and, uh, you know, face ID timed out
[35:52.880 --> 35:58.240]  and I logged in. I'm like, oh shit, I'm screen mirroring. Like everybody just saw my password.
[35:59.040 --> 36:04.000]  Oh really? My device. And I've never done that. I mean, I teach training classes at like black hat
[36:04.000 --> 36:08.000]  and Defcon and how did they, how did they see it though? If it wasn't just bullets,
[36:08.000 --> 36:10.560]  is it because the letters show up before they turn into bullets?
[36:10.560 --> 36:14.480]  They've show up for a split second before it goes into bullets. And I'm like, oh, that's fine. And
[36:14.480 --> 36:18.080]  it's like, I use different, we'll get to passwords later. I use different ones for everything and I
[36:18.080 --> 36:25.040]  can change it. It's no big deal, but yeah, it's, uh, I think it's called pit of vipers or a team,
[36:25.040 --> 36:31.680]  a team of vipers, my 500 extraordinary days in the Trump white house by a cliff Sims,
[36:31.680 --> 36:37.680]  I will copy and paste this and put it in the show notes. But let's also be clear. We don't know
[36:37.680 --> 36:42.880]  we don't know if she was on an iPhone as well. We know Bezos uses predominantly an iPhone based on
[36:42.880 --> 36:50.880]  his tweets. We don't know if she was. Uh, but the thing is, is even as, uh, SMS, it's not like
[36:51.440 --> 36:55.920]  direct interception of SMS is something that the average person on the street can pull off.
[36:55.920 --> 37:02.800]  Right. So, but it's on the table and, and, and, and part of what made Bezos post extraordinary
[37:02.800 --> 37:09.440]  was that he alleged that there was some sort of nation state involved in it at some level without
[37:09.440 --> 37:16.880]  getting specific, but then said that they were told by the AMI is the parent company of the
[37:16.880 --> 37:21.360]  national enquirer by, by representatives of AMI that, that David pecker, the owner was
[37:21.360 --> 37:29.440]  particularly quote apopleptic, uh, very angry about allegations or an investigation into their
[37:29.440 --> 37:36.640]  relationship with, uh, the Saudis. And it has since come out, somebody else reported a couple
[37:36.640 --> 37:42.560]  of days ago that at some point, I guess at some point, this guy, uh, MBS, whatever his full name
[37:42.560 --> 37:47.840]  is, was coming to the United States and it was going to be a big deal. And, and somehow AMI
[37:47.840 --> 37:55.120]  printed up a bunch of these, uh, the new kingdom, a big flashy, glossy magazine talking about how
[37:55.120 --> 38:02.880]  great Saudi Arabia is and, uh, under MBS is new leadership, blah, blah, blah. Um, apparently at,
[38:02.880 --> 38:08.640]  you know, paid by the Saudis to some degree, to the degree that, that AMI went to the U S
[38:08.640 --> 38:15.600]  justice department and asked whether they should register as foreign agents for Saudi Arabia.
[38:16.640 --> 38:23.200]  Um, the Saudis, you know, uh, like you said, mean, you know, me, can I go, if I wanted to go snoop
[38:23.200 --> 38:29.040]  on my neighbor's SMS is, is it within my means? Would I even know where to start? How to intercept
[38:29.040 --> 38:34.480]  their, their unencrypted SMS or their cell phone calls? No, I don't even know where I would start,
[38:34.480 --> 38:38.560]  but call me. It's not that I wasn't going to say that, but that actually,
[38:38.560 --> 38:43.440]  that actually is where I would start. Hey, rich is going to sound weird.
[38:43.440 --> 38:52.800]  This is going to sound weird. Uh, but you know, can the Saudis do that? Uh, without question,
[38:52.800 --> 38:57.680]  there's no, no question that the Saudis have it within their capabilities to intercept cell phone
[38:57.680 --> 39:03.520]  calls or SMS messages. And part of this, you know, just to not to go too far in the Trump area,
[39:03.520 --> 39:07.840]  but this is one reason why a lot of people are very upset that Donald Trump apparently
[39:07.840 --> 39:16.080]  makes a lot of phone calls using an iPhone. Uh, not good. Well, and let's tie this to a
[39:16.080 --> 39:20.000]  previous security story, which was the, and I can't remember the name of it, but it came out
[39:20.000 --> 39:24.560]  that there was that tool that they were remote exploiting iPhones with, uh, that, that is still
[39:24.560 --> 39:29.360]  in use. It just has more limited utility, but some zero day that gave them persistence and been able
[39:29.360 --> 39:34.800]  to, uh, track on the device. And they're, um, you know, bunch of mercenary types from the U S and
[39:34.800 --> 39:40.880]  Israel, which unfortunately do exist, uh, had, had come up with that kind of like the gray box for,
[39:40.880 --> 39:45.200]  you know, governments trying to break into phones and that story broke right before this story.
[39:45.200 --> 39:50.960]  So I really interesting, I, like I said, I don't want to rush to judgment. Something absolutely
[39:50.960 --> 39:56.560]  smells off and it does not smell like there's, this is a, you know, an Occam's razor simplest
[39:56.560 --> 40:01.200]  explanation. Is it kind of a thing? I think as this story unfolds, I don't think we even,
[40:01.200 --> 40:04.880]  I don't think we even, we're going to see things. I don't think we're even predicting right now.
[40:04.880 --> 40:10.800]  It could go in so many different directions. It is not as simple as Bezos is a famous public figure.
[40:10.800 --> 40:18.160]  And the enquirer happened to, you know, somebody said, here, take these embarrassing photos of him
[40:18.160 --> 40:23.040]  and publish them. It it's more than that. We, what more? I don't know, but it's,
[40:23.040 --> 40:29.920]  it's not as simple. Here's some embarrassing images of, of a famous person in there.
[40:29.920 --> 40:35.440]  Uh, extramarital affair. I'm by the way, very glad you made it so clear. You don't want to see the
[40:35.440 --> 40:39.600]  photos cause I was getting a little worried about that. Uh, I, you know, gotta say, you know, it
[40:39.600 --> 40:44.640]  isn't, it, it, it also is interesting and I have to, I have to say, I'm not saying anything that
[40:44.640 --> 40:49.680]  a thousand other people haven't said, but I salute Jeff Bezos for his getting in front of this
[40:49.680 --> 40:55.040]  because he's obviously risking further embarrassment, right? They like the gist of the
[40:55.040 --> 41:01.840]  national enquirer's offer to him, whether it amounts to legal extortion or not. Apparently
[41:01.840 --> 41:06.240]  I've read enough stories about it that there's actually, you know, that's questionable. It's,
[41:06.240 --> 41:13.440]  it's not cut and dry whether what they, what they've put into writing amounts to legal
[41:13.440 --> 41:19.840]  extortion. But certainly in, in the common sense of the word they were trying to extort him,
[41:19.840 --> 41:27.120]  which was to say, shut up and say, say that you've concluded that there was no political motivation
[41:27.120 --> 41:32.400]  in what we published and we won't publish the following list of pictures, which we have and
[41:32.400 --> 41:38.640]  have been holding back. You know, that's extortion. I'm not, again, a lot of lawyer amounts to legal
[41:38.640 --> 41:42.960]  extortion. I'm just, but the legal definition of extortion in the United States, but certainly in
[41:42.960 --> 41:48.880]  the common sense word saying you're going to say something that you know isn't true because you're
[41:48.880 --> 41:53.360]  going to say something that you know isn't true because apparently Jeff Bezos is under the belief
[41:53.360 --> 41:56.880]  that there were political motivations. So they were asking him to say something he doesn't
[41:56.880 --> 42:05.440]  believe in exchange for something in his, in his favor. Yeah, I'm going to say that's extortion.
[42:05.440 --> 42:10.320]  And so getting in front of it, I say extortion. Yeah. I mean, it's not even like regular extortion
[42:10.320 --> 42:16.080]  and it's a huge issue in society and a growing issue. Like, I mean, my kids are younger than
[42:16.080 --> 42:20.880]  daughters and a boy and the conversations I'm going to, I need to have with them on this very
[42:20.880 --> 42:27.600]  sooner. Not going to be comfortable. Yeah. It was weird having a 15 year old. You know, I mean,
[42:27.600 --> 42:31.360]  we've tried to be upfront about it. We've even talked about the Bezos thing and like, here, look,
[42:31.360 --> 42:38.400]  you know, don't take pictures. So I had a, uh, don't make jokes. Don't, don't put anything,
[42:38.400 --> 42:43.360]  don't, don't text anybody, whether it's a group text, whether it's a single text, don't text
[42:43.360 --> 42:47.360]  anything where you wouldn't want a screenshot of it seen by the principal at the school.
[42:48.240 --> 42:50.240]  Seriously, don't do it because it's going to happen.
[42:50.800 --> 42:56.240]  Or in 20, 30 years when that pops up. Oh my God. I can't even think about it,
[42:56.240 --> 43:01.360]  but there's a, I'll just tell one story related to this real quick. The, I, I can't tell you how
[43:01.360 --> 43:05.840]  the various associations work. They get back to me on this one in large part because I
[43:05.840 --> 43:12.400]  know clue because my wife told me I didn't listen to that part. But, um, the, uh, there was a, a
[43:12.400 --> 43:18.480]  high school kid and some relation of ours knows the mom through like, you know, three separations
[43:18.480 --> 43:23.600]  removed or something. And this car full of girls shows up banging on the door and the woman opens
[43:23.600 --> 43:27.600]  up the door and there, these girls are like, we need to talk to your son about child porn.
[43:27.600 --> 43:32.080]  Oh my God. Like what, like what? And you, you know, you're thinking what I was thinking when
[43:32.080 --> 43:38.400]  I first heard that part of the story and cut to the chase for time's sake, the girl had sent him
[43:38.400 --> 43:44.560]  a naked selfie unsolicited, like fully unsolicited. He got it. And this kid's 15,
[43:44.560 --> 43:48.480]  16. He's like, Whoa, I don't want to be dealing with this and wiped it right away.
[43:49.120 --> 43:55.440]  And because he spurned her, she was going to get her revenge by pick up his phone. It's child porn.
[43:55.440 --> 43:59.840]  He's got a picture of me on there, which he didn't because he had erased it. And, uh, needless to say
[43:59.840 --> 44:04.000]  the mom gave these girls a massive talking to once the story kind of unfolded there.
[44:04.000 --> 44:07.840]  And my wife's response is like, have some self-esteem man. It's like, they weren't even in
[44:07.840 --> 44:12.080]  a relationship and she sent the picture. Yeah. I don't know. I've seen a lot of stories along
[44:12.080 --> 44:17.360]  those lines where it's just, you know, two 16 year olds and, you know, the starts sending pictures
[44:17.360 --> 44:24.560]  and legally, you know, a picture of a naked 16 year old is child pornography, but the interaction
[44:24.560 --> 44:34.240]  isn't, you know, it's, it, the, the technical advances, what's possible for kids to send each
[44:34.240 --> 44:44.880]  other and what is what they would be tempted to do are so far ahead of what the law has been written
[44:44.880 --> 44:51.680]  to protect against, you know, it's, it is. I don't know. I mean, literally, I mean, I'm, I'm so old
[44:51.680 --> 44:58.560]  that literally we had to, uh, we had to write our notes on pieces of paper. I mean, it sounds goofy
[44:58.560 --> 45:03.360]  now, right? I mean, I can't even imagine that that's even a thing anymore, right? Kids don't
[45:03.360 --> 45:08.720]  send notes to each other with tiny little slips of paper cut off, you know, you just rip off the
[45:08.720 --> 45:14.640]  corner of a piece of paper and tiny, you know, with your, with your unbelievably acute 16 year
[45:14.640 --> 45:23.040]  old eyes, right in microfilm size text. I never even occurred to me. I never got caught exchanging
[45:23.040 --> 45:29.280]  notes as a kid. I came close one time, but, uh, never got caught, but it never even occurred to
[45:29.280 --> 45:34.000]  me that with as small as I would write the note that the teacher wouldn't even be able to read.
[45:36.640 --> 45:41.120]  Yeah. The, what they're growing up with, and even the socialization is different. I mean,
[45:41.120 --> 45:46.240]  I look at my, so I've got older nieces, nephews, and then my kids who are nine, uh, eight and five.
[45:46.960 --> 45:53.440]  And for them being social is being on devices. Yeah, absolutely. Like I, my wife and I, we can't
[45:53.440 --> 45:56.800]  say, go out and play with your friends. Cause they all play with their friends on their devices now.
[45:56.800 --> 46:00.880]  No, it's definitely social. I mean, it's, it's hard to parent because I don't know. We don't
[46:00.880 --> 46:10.080]  know how much to say, you know, step away from the device, but if we, if we pull him off his
[46:10.080 --> 46:15.760]  that his computer, uh, it's not going to make him call up a friend and go to the friend's house.
[46:15.760 --> 46:22.240]  It just means he's not in the, the, you know, group text that is all playing different games
[46:22.240 --> 46:27.760]  at the same time, but texting and talking to each other in some ways, it actually makes
[46:27.760 --> 46:30.960]  parenting easier if you accept it because I'm not schlepping him all over town.
[46:32.560 --> 46:36.640]  Actually it's, you know, and, and we don't have to worry about where he is and he doesn't have
[46:36.640 --> 46:40.640]  to come home late. We know where he is. He's in his room. So it's actually, you know, in,
[46:41.200 --> 46:45.680]  in terms of being a lazy parent, it's actually kind of convenient, whether it's doing any
[46:45.680 --> 46:52.160]  long-term social harm to his socialization and whatever. I don't, I hope it's all right, but
[46:52.160 --> 46:54.240]  I got to tell you, it's a little bit easier in some ways.
[46:55.280 --> 47:00.320]  You know what I'm looking forward to my, uh, I guess my kids becoming brooding teenagers instead
[47:00.320 --> 47:03.920]  of, uh, elementary school kids where we have to drive to eight different activities a day per
[47:03.920 --> 47:10.080]  child. We're hitting the brooding years. They come on quick. It's it, it happens. It happens fast,
[47:11.680 --> 47:15.280]  happens very fast and it leaves you fast. I'm trying to remember what grade it was.
[47:15.280 --> 47:22.080]  I think it was eighth grade, eighth grade social studies. Mr. Remember that? Well, I told you
[47:22.080 --> 47:26.480]  before we started recording, I have very vivid memories of exactly when and where certain things
[47:26.480 --> 47:33.920]  happen. Mr. Wheeler had a basement classroom and he, he had a, uh, he had a classroom with an
[47:33.920 --> 47:39.280]  office, like a little, a little cubby hole, like not in a cubby hole, but he had a door behind his
[47:39.280 --> 47:44.880]  desk and he, it was a private room for him. Wasn't part of his classroom. Not many of the classrooms
[47:44.880 --> 47:51.760]  had this. Uh, so his Matt Lauer room. Yeah. Effectively. Well, he, he, he definitely smoked
[47:51.760 --> 47:56.800]  it back there cause he would sometimes come out and in a way that like you might, you might think
[47:56.800 --> 48:00.560]  like they're never going to know I smoked, but as a, certainly as a teenager where you have these
[48:00.560 --> 48:04.720]  acute senses, you could, you know, you could smell that he smelled like smoke and it was no way he
[48:04.720 --> 48:10.480]  went outside because he wasn't, you know, it wasn't close to an exit. Uh, but my friend Mark
[48:10.480 --> 48:16.080]  and I had a note going where we had drawn, you know, the, the gist of the note was what's in
[48:16.080 --> 48:23.680]  missed, what's back there, Mr. Wheeler's back room. And we would draw things and, uh, you know,
[48:23.680 --> 48:28.400]  like every, you'd add something then fold it back up and give it to Mark and Mark would add something
[48:28.400 --> 48:34.480]  and you know, you'd like one of them was like a big pile of boogers because you could occasionally
[48:34.480 --> 48:39.280]  catch Mr. Wheeler picking his nose. So there's like a big pile of dots and it's just, and then
[48:39.280 --> 48:43.520]  you'd label it with an arrow. It's a pile of boogers. How many people listen to this podcast
[48:43.520 --> 48:47.680]  and you just name dropped his nose? I'm hoping that I'm hoping, I'm hoping Mr. Wheeler doesn't
[48:47.680 --> 48:55.280]  listen to it. Sorry. Uh, I tragically died of ALS three years ago, but it was just, I mean,
[48:55.280 --> 48:58.640]  I'm not even going to, I don't even remember the specifics, but let's just say that the boogers
[48:58.640 --> 49:04.240]  were the least of the problems, you know, in terms of like, it was rated R it was a rated R note,
[49:05.680 --> 49:11.760]  let's just say, uh, you know, it's just, I, I, off the top of my head, I'm going to guess that
[49:11.760 --> 49:16.640]  somebody drew, uh, like a pile of porno mags, you know, just a pile of, you know, just to,
[49:17.440 --> 49:21.760]  it doesn't matter if it was artistically look like a pile of magazines. You just label it pile
[49:21.760 --> 49:26.320]  of porno mags. And remember when you used to have to actually go buy your porn in a store,
[49:27.600 --> 49:31.440]  thinking about generational differences. Yeah, that was, those were the days. Well, anyway,
[49:31.440 --> 49:38.080]  I got caught with the note, Mr. Wheeler said, John, what do you got there? And I had the note.
[49:38.080 --> 49:46.960]  And, and I made, I made an immediate, I made an, I'm very proud of this is why I remember I,
[49:46.960 --> 49:53.680]  I, I I'm calm, cool and collected under pressure. I took the note. I wadded it up. I put it right
[49:53.680 --> 50:04.480]  in my mouth with him looking right at me and I swallowed it. I'm going to need a moment here.
[50:04.480 --> 50:10.320]  Uh, this is the honest to God truth. I, I wadded the note up right with him looking right at me.
[50:10.320 --> 50:14.240]  Cause I realized that I got no, there's no other way out of this. I'm either coughing up this note,
[50:14.240 --> 50:18.640]  which was literally maybe the worst possible, you know, literally had a diagram of the
[50:18.640 --> 50:25.920]  room behind his classroom with all sorts of awful things. Or I can swallow the note. The only way I
[50:25.920 --> 50:30.960]  could get rid of it would be to eat it. And it was on a small piece of paper relative, you know,
[50:30.960 --> 50:35.440]  to the, you know, to the amount of content in it. It wasn't like an entire eight and a half by 11
[50:35.440 --> 50:40.160]  sheet of paper. It was maybe like, uh, probably about the size of an index card, you know,
[50:41.200 --> 50:45.200]  I'm laughing, but that probably saved your ass. I wadded it up, put it right in my mouth,
[50:46.240 --> 50:50.160]  chewed it up, swallowed it right there in front of him. And I said nothing.
[50:52.080 --> 50:57.520]  And I don't think I got into any trouble. I don't believe that I was, I, I, I believe that he was so
[50:57.520 --> 51:07.120]  amused by the reaction that, uh, that I escaped any sort of, uh, uh, any sort of, uh, penalty.
[51:07.120 --> 51:11.840]  I really do. Uh, and it turned out he was my homeroom teacher. And then in ninth grade,
[51:11.840 --> 51:16.080]  I no longer had him for social studies, but I had him for, it was a, I went to a relatively
[51:16.080 --> 51:20.480]  small public school where it was seven to 12, grade seven to 12, all in the same building.
[51:20.480 --> 51:24.480]  So I still had, I didn't go to like a different school between eighth and ninth grade. I still
[51:24.480 --> 51:29.200]  had him for homeroom. He was a basketball coach, not at our high school, but at a neighboring high
[51:29.200 --> 51:35.760]  school. And we, we, we ended up as contentious as our relationship was as, as a student. Uh,
[51:35.760 --> 51:40.160]  by the next year I was, I was like the, maybe like the pet favorite in homeroom.
[51:41.840 --> 51:44.720]  So we patched it up, but that's my story of, of,
[51:47.360 --> 51:51.840]  I've got nothing of that level of, uh, I don't know what you would do. I guess,
[51:51.840 --> 51:56.080]  I guess what you would do is delete your text, right? If somebody catches you with bad text,
[51:56.080 --> 52:00.560]  you can, well, I mean, kids today too, like for a lot of this stuff they're using, you know,
[52:00.560 --> 52:06.640]  signal or, uh, or wicker style apps are the ones that look like it's one app. And then there's
[52:06.640 --> 52:14.320]  the hidden text messaging behind it. Um, yeah, there's a whole list of, uh, if you'll look for
[52:14.320 --> 52:18.160]  it and I don't know where it is, or I pull it up and pop it in the show notes, but there's list of
[52:18.160 --> 52:23.360]  apps that actually have hidden messengers. Then you flick things a certain way. Uh, and it looks
[52:23.360 --> 52:28.160]  like some other kind of a game or something more innocuous. So the, the teens that are like under
[52:28.160 --> 52:32.480]  the gun, cause I've had, I've known people that have had issues with troubled kids. So, uh,
[52:32.480 --> 52:36.720]  depression or suicide or drugs and those kinds of things. Um, unfortunately you get old enough,
[52:36.720 --> 52:41.520]  you've got friends with those kids and those are the kinds of apps that the smarter kids,
[52:41.520 --> 52:45.600]  um, and they all verbally, you know, kind of network those things among each other.
[52:45.600 --> 52:50.560]  So there are ways of doing it that they can't get caught. I think probably most kids just use text
[52:50.560 --> 52:54.160]  message and get caught, but let me take a break here and thank our first sponsor. It's our good
[52:54.160 --> 52:59.840]  friends at LinkedIn. Look, the right hire can make a huge impact on your business. That's why
[52:59.840 --> 53:06.560]  it's important to find the right person, but where do you find that individual? You can post a job on
[53:06.560 --> 53:10.800]  a job board and hope the right person will find your job, but think about it. How often do you
[53:10.800 --> 53:16.880]  hang out on job boards? Don't leave finding someone great to chance when you can post your
[53:16.880 --> 53:22.640]  job to a place where people go every day to make connections, grow their career, discover job
[53:22.640 --> 53:29.120]  opportunities. LinkedIn, most LinkedIn members haven't recently visited the top job boards,
[53:29.120 --> 53:36.160]  but nine out of 10 members are open to new opportunities. And with 70% of the U S workforce
[53:36.160 --> 53:41.680]  on LinkedIn posting on LinkedIn is the best way to get your job opportunity in front of more of the
[53:41.680 --> 53:47.760]  right people. People who are qualified for the jobs you're looking to fill are right there on
[53:47.760 --> 53:52.720]  LinkedIn right now. It's the best way to find the person who will help you grow your business
[53:52.720 --> 53:58.480]  and why a new hire is made every 10 seconds using LinkedIn. That's a phenomenal number.
[53:58.480 --> 54:06.720]  So go to linkedin.com dot, uh, linkedin.com slash talk, just T a L K. That's the code for the show.
[54:07.360 --> 54:15.520]  And by going to that URL, you will get 50 bucks off your first job listing. That's linkedin.com
[54:15.520 --> 54:21.680]  slash talk, and you will say 50 bucks off your first job post terms and conditions apply
[54:21.680 --> 54:28.960]  linkedin.com slash talk. Ah, all right. So we, we need to know, we need to know what,
[54:28.960 --> 54:35.680]  what Bezos was using. What was it? I message. Was it SMS? Was it something like signal or WhatsApp
[54:35.680 --> 54:40.320]  or something like that? I don't know, but I, I can't wait to find out. I hope we do find out
[54:40.320 --> 54:46.000]  eventually. And I'm also curious, just whatever it was, how they got, how those messages got to the
[54:46.000 --> 54:51.600]  enquirer's hands and what, what format they were well. And if there was an obvious route, like,
[54:51.600 --> 54:55.840]  Oh, I let my brother use my computer during Thanksgiving. I think we would have known that
[54:55.840 --> 55:01.440]  earlier. I guess at least the investigators would have. Yeah. I mean, I mean, I guess what you could
[55:01.440 --> 55:07.280]  do, you know, uh, off the top of my head, what would I do? I don't know. I mean, and did he know,
[55:07.280 --> 55:11.520]  was it like an open secret within the, within their, you know, the brother sister relationship
[55:11.520 --> 55:17.680]  that she was having an affair with Bezos? Uh, I mean, it's, I'm guessing it was, you know,
[55:17.680 --> 55:22.160]  that somehow they were close enough that he knew and he betrayed her. But I guess if he got access
[55:22.160 --> 55:28.000]  to one of her devices, like a Mac or even her phone and he could take screenshots or, you know,
[55:28.000 --> 55:32.560]  you can't really forward text messages, you know, there's no forward command like email.
[55:33.120 --> 55:38.560]  So I think there is actually an iPhone. Oh, really? Yeah. Yeah. Let me, uh, I'm pretty sure
[55:38.560 --> 55:43.200]  let me pull this up, but I'm pretty sure I've forwarded messages. I can't remember how. Yeah.
[55:43.200 --> 55:50.240]  Uh, let's see, tap and hold, click more and then click forward and you can send it to somebody
[55:50.240 --> 55:56.000]  else. So there is a forwarding. I don't see it on iPad. How do you do that? So, uh, yeah,
[55:56.000 --> 55:59.840]  if you select the message and I just tap, oh, I see. And then there's a little arrow message.
[55:59.840 --> 56:05.280]  Yeah. Oh, same, same place where the tap back comes. Yeah. Yeah. Yeah. It's the same little,
[56:05.280 --> 56:10.320]  uh, curvy arrow as the wife. And I use that all the time, sending out like addresses for,
[56:10.320 --> 56:14.480]  you know, kid drop off and pickups and those kinds of things where we're coordinating. Yeah. Well,
[56:14.480 --> 56:18.720]  there you go. I just learned something. Yeah. Well, I guess he could forward them if he knew
[56:18.720 --> 56:24.880]  about that screenshots would be an easy way, uh, you know, and then, you know, there'd be this
[56:24.880 --> 56:32.240]  trail of those messages between her and him, you know, and then just quick delete those,
[56:32.240 --> 56:38.560]  from why you still have her device. And then all of a sudden she's got no sign that he did it.
[56:38.560 --> 56:45.200]  Well, I mean, there's trails here. So the hardest thing for the private investigator to find would
[56:45.200 --> 56:51.040]  be if it was, uh, handled at the carrier level, uh, and anything within the system there, because,
[56:51.040 --> 56:55.280]  uh, first of all, I doubt the guy built an MZ catcher and was like sniffing SMS and doing phone
[56:55.280 --> 56:59.360]  cloning stuff. I mean, maybe he was, but that that's serious. And those are all sort of things
[56:59.360 --> 57:04.560]  federal crimes. The other side of it is it breaking into the iCloud stuff. Like if all
[57:04.560 --> 57:09.760]  he did was add a device to the device chain. First thing I do, if I'm, if I go into a situation
[57:09.760 --> 57:14.320]  like that is I look at all the registered devices because that'll tell me now, maybe he took the
[57:14.320 --> 57:22.400]  device off, uh, and it would disappear. But, and also if, uh, if, um, multi-factor authentication
[57:22.400 --> 57:25.440]  was set up, you know, you do an interview. I mean, there's no way to sneak that through
[57:25.440 --> 57:29.920]  without punching that code. So there's, and you can even dig through key chains and stuff. If you
[57:29.920 --> 57:35.600]  really got into the deep forensics of this. Um, so, I mean, it, it, it should be traceable
[57:35.600 --> 57:43.920]  if it involved one of their devices at all. Yeah. Uh, the other things, you know, and I,
[57:43.920 --> 57:48.000]  I think it's pretty simple. I think it's as simple as the brother having access to something,
[57:48.000 --> 57:51.920]  but, you know, in terms of how safe are you with your iMessage, uh,
[57:51.920 --> 57:57.920]  number one, I do think everybody, I really do think everybody should have two factor on their,
[57:57.920 --> 58:02.240]  their Apple ID accounts. And I don't know at what point, I don't even know, like if you sign
[58:02.240 --> 58:08.000]  up for a new one, did they even let you do it without two factor? You can, cause I don't have
[58:08.000 --> 58:12.640]  that turned on on my kids' accounts. Right. I don't know that my son does either. I don't think.
[58:12.640 --> 58:18.640]  Yeah. So I've got that on mine, but, but it really wants you to put it in. You've got to delete
[58:18.640 --> 58:24.800]  it, but it really wants you to put it in. You've got to deliberately, you know, make choices to not
[58:24.800 --> 58:30.160]  implement it. Right. And, and, you know, but if you do the iMessage in the cloud,
[58:31.680 --> 58:36.720]  it does open you up to somebody being able to, if they only have your password,
[58:36.720 --> 58:40.880]  being able to read your iMessages, if you're not using two factor.
[58:42.880 --> 58:47.680]  Yeah. Well, I wonder, I haven't logged in, you know, I'm going to log in iCloud because
[58:47.680 --> 58:51.600]  I can't remember. I thought it doesn't, it didn't display the messages in iCloud.
[58:51.600 --> 58:55.920]  Yeah, maybe it doesn't. I got no messages. No, there is no messages. So even when you
[58:55.920 --> 59:00.640]  have iMessage in the cloud, you can't read them online. So I guess the other way you could get
[59:00.640 --> 59:06.960]  them would be through the backup. If you, if you know, if you could somehow restore somebody's
[59:06.960 --> 59:16.640]  iCloud backup to a device, you could restore the text messages that are in their backup.
[59:16.640 --> 59:20.720]  But I think if you have iMessages in the cloud, then those are no longer backed up in the backup.
[59:20.720 --> 59:25.200]  No, but if you add the device to your device ring, now the thing is for that you get a
[59:25.200 --> 59:29.760]  notification on every device. So when you add one, you have to, even if you have two factor
[59:29.760 --> 59:34.400]  authentication turned off, you get notifications on every other device. Like every time I, I mean,
[59:35.120 --> 59:40.080]  you and I both test stuff, you more than me, every time I like add a new Apple watch or whatever,
[59:40.080 --> 59:45.200]  it's like, Oh crap. You know, then I, I can't believe at one point this fall,
[59:45.200 --> 59:50.480]  I couldn't believe that I could still continue to add devices to my iCloud account. Cause I had like,
[59:50.480 --> 59:59.600]  I still had my year old iPhone, a two year old iPhone, my new iPhone, three iPhones for testing,
[01:00:01.280 --> 01:00:07.200]  my Apple watch, my personal Apple watch, Apple watch for testing. I mean, it was unbelievable.
[01:00:07.200 --> 01:00:11.120]  I mean, it was, but you know, it, it actually works. And it kind of makes sense cause you
[01:00:11.120 --> 01:00:16.240]  would guess that there are people within Apple who are going through testing devices at a rate
[01:00:16.240 --> 01:00:21.920]  that even makes me as a product reviewer, you know, seem like I don't have a lot of devices.
[01:00:23.440 --> 01:00:27.280]  Yeah. I mean, they probably have accounts with a hundred devices on there just to see what happens.
[01:00:27.280 --> 01:00:31.680]  But boy, when I like at the end of the review season, when I went in and like started like,
[01:00:31.680 --> 01:00:37.200]  forget this device, forget this device, forget this, it was, it was kind of cathartic. Cause I,
[01:00:37.200 --> 01:00:41.200]  even though I'm just like eliminating them from this list of, of devices, it,
[01:00:41.200 --> 01:00:47.760]  it felt like I was cleaning up a mess. I haven't done that spring cleaning and I've got,
[01:00:47.760 --> 01:00:52.960]  you know, like you since, you know, iPhone one stuff still in there and half these devices I've
[01:00:52.960 --> 01:00:58.640]  handed off to other family members. So they're fully wiped and, and I know I need to, to be
[01:00:58.640 --> 01:01:02.960]  better about that. I'm, I'm, I'm revealing my passwords. I'm talking about not cleaning my
[01:01:02.960 --> 01:01:08.640]  devices. I'm really sound like a security expert on this thing, but it's like a lot to remember
[01:01:08.640 --> 01:01:13.600]  to go through and clean those out. Uh, so what do you, what do you think about, here's the one
[01:01:13.600 --> 01:01:18.320]  thing too, about this Bezos story that I would like to come out. And if it comes out that they
[01:01:18.320 --> 01:01:24.320]  really are just SMS text messages, and I guess it would be technically MMS if, if there were,
[01:01:25.360 --> 01:01:31.360]  as there apparently were photos involved, uh, if they were intercepted. And again,
[01:01:31.360 --> 01:01:34.560]  it doesn't look like that's the case, but I kind of almost hope that it is
[01:01:36.000 --> 01:01:41.680]  just in terms of serving as a public service announcement that SMS and MMS are not end to end
[01:01:41.680 --> 01:01:47.760]  encrypted. And therefore somebody could intercept them over the air. Like it's just a bad state of
[01:01:47.760 --> 01:01:52.240]  affairs. And the thing that the, the thing that I would really like to catch on is the idea that
[01:01:52.240 --> 01:02:00.080]  this next generation, uh, carrier based texting system, RCS is also not end to end encrypted.
[01:02:00.080 --> 01:02:06.880]  And I just think it is not that the, that we, as a society are going to accept a new standard that,
[01:02:06.880 --> 01:02:14.720]  that does not involve any encryption whatsoever. It's we're building foundations of society on
[01:02:14.720 --> 01:02:20.160]  text messages at this point. And like in the security industry, it was a huge deal. So NIST,
[01:02:20.160 --> 01:02:24.960]  national Institute for standards and technology came out and said, SMS doesn't count as two factor
[01:02:24.960 --> 01:02:29.840]  authentication. They like a year or two ago, they just flat out said it. And people were screaming
[01:02:29.840 --> 01:02:34.880]  a lot of vendors and like providers and stuff that no, you have to allow that. You can't say
[01:02:34.880 --> 01:02:38.400]  it's not allowed because once it goes into that government standard, when you do your security
[01:02:38.400 --> 01:02:43.360]  audits and stuff like the stuff I spend most of my day doing, um, you know, that becomes a huge
[01:02:43.360 --> 01:02:48.960]  issue. And it was interesting cause Apple is really early on the edge of circumventing SMS
[01:02:48.960 --> 01:02:55.680]  and using their own mechanism and like way early on that compared to, uh, other alternatives.
[01:02:55.680 --> 01:02:59.440]  And even some of the alternatives like my bank, I'm not going to tell you who they are, but
[01:02:59.440 --> 01:03:04.000]  like I log in and I have two factor turned on and it's every phone and every email address I
[01:03:04.000 --> 01:03:07.760]  have registered with them. That's not two factor like sending those messages out. Cause it's
[01:03:07.760 --> 01:03:15.040]  insecure. But to go back to the encryption piece, uh, this, this is a generational tragic mistake
[01:03:15.040 --> 01:03:19.680]  that we're probably going to make at this point. And the carriers don't want to implement it for
[01:03:19.680 --> 01:03:25.200]  cost reasons or whatever else. Um, and we do actually have, uh, would not surprise me if
[01:03:25.200 --> 01:03:29.440]  there's government pressure behind the scenes because they want to, but they, they would still
[01:03:29.440 --> 01:03:34.080]  be able to build an intercept. I mean, that's the law. They have to have lawful intercept at carrier
[01:03:34.080 --> 01:03:40.640]  level stuff, uh, as well as the, um, like we even saw pressure. There were financial services
[01:03:40.640 --> 01:03:47.280]  companies lobbying Congress, not to use the stronger encryption standard in TLS one dot three,
[01:03:47.840 --> 01:03:51.440]  because then they couldn't, couldn't sniff their employees. And they did that. It's like
[01:03:51.440 --> 01:03:57.280]  your convenience, you want to fundamentally make the internet weaker. Yeah. But that's how people
[01:03:57.280 --> 01:04:03.440]  think sometimes, uh, the same people that think global warming is a scam. I don't know. Yeah.
[01:04:03.440 --> 01:04:09.280]  I, I, well, let's not go down that path. I am not going to brag about my personal
[01:04:09.280 --> 01:04:17.360]  security practices cause I'm, I'm sure it's lacks in many ways, but I don't reuse passwords
[01:04:17.360 --> 01:04:23.920]  and haven't for many years. I mean, at some point I did have my quote unquote standard throwaway
[01:04:23.920 --> 01:04:30.560]  password, uh, and I still remember it and I don't think I actively use anything that still has it,
[01:04:30.560 --> 01:04:35.040]  but it was sort of, you know, anything where I wasn't really concerned about it. Like, ah,
[01:04:35.040 --> 01:04:38.640]  for this got hacked, you know, my credit card is not even hooked up to this thing. I don't care.
[01:04:38.640 --> 01:04:47.520]  I had a password. I don't do that anymore. Um, I, last year, uh, maybe a year ago, maybe even
[01:04:47.520 --> 01:04:55.120]  a little more. I, um, was talking to Mac Jay Cichlowski. I hope that's how you pronounce
[01:04:55.120 --> 01:05:02.800]  his name, but the pinboard guy. And he spent a lot of last year helping, uh, he spent much of
[01:05:02.800 --> 01:05:09.280]  2018, probably 2017 and 2018 helping democratic candidates around the country, small grassroots,
[01:05:09.280 --> 01:05:16.960]  uh, candidates running for Congress and other offices, um, with fundraising and with getting
[01:05:16.960 --> 01:05:22.720]  their security together, you know, because there were an awful lot of security lapses in the 2016
[01:05:22.720 --> 01:05:28.560]  election. I mean, you know, John Podesta's email was the Gmail account was hacked. I mean, it was
[01:05:28.560 --> 01:05:33.600]  that literally, you know, it turns out there really wasn't anything skating in there, but there was
[01:05:33.600 --> 01:05:39.120]  stuff that was embarrassing. A couple of Democrats had their Gmail accounts hacked pretty much
[01:05:39.120 --> 01:05:44.400]  because they got spearfished from my understanding, um, you know, where their account was really only
[01:05:44.400 --> 01:05:50.240]  protected by a password. Uh, they got an email that either they or somebody who worked for them
[01:05:50.240 --> 01:05:55.600]  clicked the link, followed through and entered their Gmail password. And that's game over with,
[01:05:55.600 --> 01:06:01.200]  without two factor. They already knew your address because they sent you the spear phishing email
[01:06:01.200 --> 01:06:05.840]  and then you give them your password and there it is. Then they use it, they download all your
[01:06:05.840 --> 01:06:10.560]  email and even if you know it, it's too late. They got it all. Um, so he was helping people
[01:06:10.560 --> 01:06:16.960]  with that and he was in Philadelphia and we had a beer and you know, it's, you know, after the
[01:06:16.960 --> 01:06:21.440]  Matt Honan thing a couple of years ago where Matt Honan, who's now a editor at Buzzfeed,
[01:06:21.440 --> 01:06:26.080]  he worked at various other places before, but he more or less had his entire digital life stolen.
[01:06:26.080 --> 01:06:31.920]  Uh, and you know, one of the ways that that happened and it wasn't through entirely lax,
[01:06:31.920 --> 01:06:36.320]  it wasn't because he was really doing bad stuff, but one of the ways that it happened was,
[01:06:37.200 --> 01:06:45.440]  uh, the people trying to hack him took control of his, uh, cell phone account at the carrier.
[01:06:45.440 --> 01:06:49.440]  I don't know if it was Verizon or AT&T or whoever it was, but they went there and, and
[01:06:49.440 --> 01:06:55.520]  socially engineered, Hey, I'm, I'm Matt, you know, I, I lost my phone or whatever their story was.
[01:06:55.520 --> 01:07:00.320]  I need, you know, I need to get a new SIM with my phone because, you know, and they get, and somehow
[01:07:00.320 --> 01:07:05.200]  they got a SIM with his phone number and then all of his two factor stuff that was sent by SMS
[01:07:05.200 --> 01:07:11.040]  went to them instead and they took his iCloud and his iCloud was the, you know, his email was
[01:07:11.040 --> 01:07:15.520]  in charge of all sorts of other stuff and all of a sudden, you know, he was in a world of hurt.
[01:07:15.520 --> 01:07:23.280]  You know, he was in a world of hurt. So I took his advice and I, I believe to the best of my ability,
[01:07:23.280 --> 01:07:29.440]  anything that uses SMS as the second factor, no longer uses SMS as the second factor.
[01:07:29.440 --> 01:07:35.600]  I use something else like the, uh, I don't use Google's authenticator app. I use Authy,
[01:07:35.600 --> 01:07:40.080]  but it's the same one I use. It's the same. I'm glad to hear that you use it, but you know,
[01:07:40.720 --> 01:07:44.720]  it is a way, well, how would you describe it? You can probably describe it better than I can.
[01:07:44.720 --> 01:07:49.120]  Yeah, it's called, we call it OTP or one-time passwords, which is there's a,
[01:07:49.120 --> 01:07:53.200]  and it used to be like this crazy expensive thing. RSA secure ID was the only way to get
[01:07:53.200 --> 01:07:58.080]  it with the little key fobs. It's the same thing in an app where there's some cryptographic
[01:07:58.080 --> 01:08:04.240]  synchronization that goes on between the app and your phone and whatever the server is on
[01:08:04.240 --> 01:08:08.240]  the backend. They share a couple of keys and then they use time codes. And so every minute
[01:08:08.240 --> 01:08:12.960]  it generates a new key, um, which leads to a new kind of anxiety. Cause I have to use it multiple
[01:08:12.960 --> 01:08:16.400]  times a day. I flick it open and I'm like, Oh shit, three seconds left. Can I type it fast enough
[01:08:17.280 --> 01:08:22.960]  or not? Um, I hate that. Well, three seconds is no question. If I open it up and I've got
[01:08:22.960 --> 01:08:28.960]  three seconds left, no question. I'll just wait on the line. They're like, all right,
[01:08:28.960 --> 01:08:35.280]  well, have I been drinking? What's my dexterity like right now? And for me, it's one of the rare
[01:08:35.280 --> 01:08:42.720]  things that to me is harder to do on a, a laptop or desktop with a real keyboard
[01:08:42.720 --> 01:08:49.920]  than on a phone because the numbers are arranged. Like I can enter a six digit number
[01:08:49.920 --> 01:08:57.760]  on a three by three, you know, with the zero at the bottom keypad way faster than I can on a
[01:08:57.760 --> 01:09:03.280]  keyboard where they're arranged across in a row one to zero. See, I've got an iMac pro with the
[01:09:03.280 --> 01:09:09.680]  big keyboard, which I don't use for anything except entering those passcodes. So on a laptop,
[01:09:09.680 --> 01:09:14.000]  it actually does take me longer. And so if it's like 15, 10 seconds, I'm like, I think I could
[01:09:14.000 --> 01:09:18.320]  do this, but if I make one mistake, I'm screwed. Oh, what do I do? But anyway, but yeah, you get
[01:09:18.320 --> 01:09:23.280]  like a one time password that lasts 30 seconds at a time that you use with your regular password.
[01:09:23.280 --> 01:09:30.960]  Also, that's why it's the, yeah. Right. So you enter in, you know, your name at your email.com,
[01:09:30.960 --> 01:09:35.200]  your regular password, and then it'll say what, you know, here, enter your six digit,
[01:09:35.200 --> 01:09:40.640]  whatever account and, you know, you go to, you know, you go to your app that and it has a
[01:09:40.640 --> 01:09:46.080]  different one for every service you register. And you get have these series of 30 second
[01:09:46.720 --> 01:09:53.520]  one time password. So I switched everything I could to that. You know, and I feel better for it.
[01:09:54.240 --> 01:09:59.120]  Yeah, that or like what Apple where there's the out of band. Like I use a product called
[01:09:59.120 --> 01:10:03.840]  duo security. It's a commercial tool at Cisco bottom. I've got some friends that work over
[01:10:03.840 --> 01:10:08.320]  there, but, uh, like for some of my work related stuff, like one of my VPNs and it's cool because
[01:10:08.320 --> 01:10:12.800]  it sends a push notification that pops up on my Apple watch. And I just click approved,
[01:10:12.800 --> 01:10:17.680]  like right there on the watch when I'm logged in, which is slick. And I don't have to kind of type
[01:10:17.680 --> 01:10:22.320]  in the code, but it's the same level because it's locked to the device I have on me that I've already
[01:10:22.320 --> 01:10:26.640]  authenticated with. Right. And like your Apple watch is only going to show that if you've already
[01:10:26.640 --> 01:10:30.960]  unlocked the Apple watch through your Apple watches passcode or through the connection to
[01:10:30.960 --> 01:10:35.120]  your phone. So in other words, if your Apple watch is just sitting there on your desk
[01:10:35.120 --> 01:10:39.920]  off your wrist, it isn't going to show the alert. You did inspire me. I pulled up my one
[01:10:39.920 --> 01:10:44.000]  password cause I use like you unique password. I don't know. Do you use one password or I don't
[01:10:44.000 --> 01:10:49.920]  pass. I don't use any of those things to be honest. I built in just teaching stuff, which
[01:10:49.920 --> 01:10:56.960]  we have to get to might be, might be a mistake. Yeah. Um, and I use one password. I just looked
[01:10:56.960 --> 01:11:05.520]  it up. I've got 1,358 unique passwords and going back to 2008. And, uh, although apparently I had
[01:11:05.520 --> 01:11:10.480]  no password on my secure assist blog for at some point, I don't know. Cause that's a blank, but
[01:11:10.480 --> 01:11:16.560]  it's actually kind of fun to look back my mobile, me password, my Yahoo from 2008,
[01:11:16.560 --> 01:11:22.960]  three hack Yahoo. That's good stuff. No, no, I do have a, I do keep passwords outside the key chain
[01:11:22.960 --> 01:11:29.120]  in, uh, Yojimbo an app from barebone software that is only for the Mac that's still supported.
[01:11:29.120 --> 01:11:34.480]  Yeah. It's still actively developed. It's okay. It hasn't really had a major update in a while.
[01:11:34.480 --> 01:11:41.600]  So I don't know, but yeah. Um, still works perfectly on, uh, on Mojave. So I have 637
[01:11:41.600 --> 01:11:46.320]  passwords there. I don't know how many are in my key chain and that does have encryption. If I
[01:11:46.320 --> 01:11:51.840]  remember. Oh yeah, yeah, no. And I, you know, part of it is that satisfy. I know the people who wrote
[01:11:51.840 --> 01:11:59.440]  it. So I know, I know I've I trust it. It is, uh, you know, and I don't keep that password in my
[01:11:59.440 --> 01:12:04.880]  key chain. So that is, you know, that is one of the, that's like one of the few passwords where I
[01:12:04.880 --> 01:12:10.480]  have to keep it somewhere outside the key chain. Yeah. My iCloud and my one password password or
[01:12:10.480 --> 01:12:14.960]  the two and my system login or the, I guess the three passwords I don't have stored anywhere.
[01:12:14.960 --> 01:12:21.360]  Uh, my friend, uh, Brent Simmons, uh, collaborator on, on our app, Vesper,
[01:12:21.360 --> 01:12:25.280]  and just a longtime friend. And he's been on the show many times, but we were on Slack together
[01:12:25.280 --> 01:12:31.360]  and a couple of weeks ago he, he, he just wrote, I can't believe this. I've had the same login
[01:12:31.360 --> 01:12:39.440]  password on my Mac for 10 years. Uh, and I suddenly can't remember it. Uh, and that has
[01:12:39.440 --> 01:12:45.280]  happened to me because it is a muscle memory thing. I type it without really thinking about
[01:12:45.280 --> 01:12:52.160]  it. And if I, I can think about it right now and I could write it down. Uh, but it has happened to
[01:12:52.160 --> 01:12:58.320]  me where, where it's a different sort of memory from just actually typing it on the keyboard with
[01:12:58.320 --> 01:13:02.560]  muscle memory than actually thinking about it. And if you actually think about it, sometimes you can
[01:13:02.560 --> 01:13:08.560]  like temporarily forget it. And it is, it is terrifying. So I've had to read mine over the
[01:13:08.560 --> 01:13:13.760]  phone to my wife. Like I'm traveling and my, she needs something off the computer and I don't
[01:13:14.400 --> 01:13:17.920]  have, I'm not having an affair and I don't have Dick pics on my computer. So it's okay.
[01:13:17.920 --> 01:13:22.720]  I don't care if she gets in there and I can't remember it. I, so I, I will pull out a keyboard
[01:13:23.680 --> 01:13:28.720]  and just type it in. Yeah. Dictate it as I'm typing it,
[01:13:28.720 --> 01:13:32.480]  like type it into a temporary, like text, edit documents. So you can see it and be like,
[01:13:32.480 --> 01:13:36.320]  Oh yeah. Oh, that's actually, it looks like a goofy password. There you go. Yeah. No,
[01:13:36.320 --> 01:13:40.080]  I'm like that. And it's, you know, and then he like, he was at work at the Omni group and then
[01:13:40.080 --> 01:13:43.520]  he like went home and he went just like the commute and he went home and then he could
[01:13:43.520 --> 01:13:47.520]  just type it and it was like, okay, I got it. You know, but he needed that like sort of
[01:13:48.800 --> 01:13:52.480]  contextual break, you know? Yeah. Well, that's my biggest fear,
[01:13:52.480 --> 01:13:57.040]  like getting hit by a bus and then my wife can't get into stuff because she does. And I have some
[01:13:57.040 --> 01:14:02.800]  of those written down and I've got a copy I sent to my, my lawyer and we, but she like, my wife's
[01:14:02.800 --> 01:14:07.040]  technical. And for some reason she just doesn't want to use one password. Now that is, that is
[01:14:07.040 --> 01:14:11.040]  something that I want to set up. I want to set up, like, I don't know if it would be a safe deposit
[01:14:11.040 --> 01:14:15.600]  box. I don't know if it would be something I give to my lawyer, but some kind of like binder of
[01:14:15.600 --> 01:14:21.520]  written stuff that is exists in a place where there is a, if John is incapacitated, here's,
[01:14:21.520 --> 01:14:25.840]  you know, everything you need. I don't have that set up. There's an awful lot that it's
[01:14:25.840 --> 01:14:32.400]  just, it's in my brain. Well, you got to maintain it. Cause once you set it up, like the passwords
[01:14:32.400 --> 01:14:36.320]  change. So I'm like, all right, well, I'll set up so she can get into my computer and one password.
[01:14:36.320 --> 01:14:39.760]  And I just told her, if things get really bad, you, you know, you call one of my partners from
[01:14:39.760 --> 01:14:44.160]  work and like Chris Pepper, cause he's a friend and he's super technical and can kind of walk
[01:14:44.160 --> 01:14:48.400]  through and figure it out. And maybe they can open up to the hacker network and somebody can
[01:14:48.400 --> 01:14:54.320]  get into my stuff. If, if the paperwork is no longer current. Chris Pepper is one of the handful
[01:14:54.320 --> 01:14:58.000]  of people who should be an employee of daring fireball, but I've never paid him a nickel,
[01:14:58.000 --> 01:15:03.600]  but he's, he has reported more typo type of graphic errors and daring fireball
[01:15:04.320 --> 01:15:10.560]  than I could ever count. We put him on retainer at Securosis like eight years ago.
[01:15:12.800 --> 01:15:17.520]  For his typo reporting ability. I just said, just go in and fix it. He's a savant. He's
[01:15:17.520 --> 01:15:23.120]  an absolute emails. He's an absolute savant. And I agree with, I would say his hit rate. Number one,
[01:15:23.120 --> 01:15:27.520]  he catches all the mistakes and mistakes of course get fixed. And when he has a quibble,
[01:15:27.520 --> 01:15:32.000]  like a suggestion for this would be better. I would say his batting average is somewhere
[01:15:32.000 --> 01:15:37.840]  between 90 and 95%. And I would say no more than one out of 10, but probably closer to one out of
[01:15:37.840 --> 01:15:44.000]  20 times. Will I actually reject his suggestion and stick with what I have? Yeah. We haven't like
[01:15:44.000 --> 01:15:49.680]  edit some of our more complicated research papers that we do. And occasionally he'll have a miss on
[01:15:49.680 --> 01:15:54.080]  those, which changes the context. And he doesn't just edit me. It's my, you know, I've got two
[01:15:54.080 --> 01:15:59.600]  partners these days at Securosis and he edits all of us. And now with the new company, the startup,
[01:15:59.600 --> 01:16:03.520]  I just give him the email of our marketing guy. And so he'll send him all the corrections.
[01:16:04.720 --> 01:16:09.520]  So he's, he's pretty good. Before we move on. Let me, let's just talk about the various ways
[01:16:09.520 --> 01:16:14.800]  that people can send private messages to each other. And I'm curious what you would recommend.
[01:16:14.800 --> 01:16:21.760]  I almost exclusively use iMessage simply because everybody I communicate, almost everybody I
[01:16:21.760 --> 01:16:30.160]  communicate with is on iMessage. And I sent, I can't even remember the last time I sent an SMS,
[01:16:30.160 --> 01:16:37.760]  other than when I do get some kind of like confirmation from some service or something,
[01:16:37.760 --> 01:16:46.800]  you know, everything I get by SMS is automated. And that is a weird aspect of being in the United
[01:16:46.800 --> 01:16:50.640]  States. I totally get it. Every time I write about iMessage, I have readers all around the
[01:16:50.640 --> 01:16:57.040]  world and I totally get it that in Asia and Europe, iMessage is not really a big player
[01:16:57.040 --> 01:17:02.240]  and WhatsApp is huge and there's other ones, other services in other countries. I totally get it.
[01:17:02.240 --> 01:17:08.000]  But here for me personally, iMessage is everybody I want to talk to is on iMessage. And then second
[01:17:08.000 --> 01:17:13.920]  to that, and this is what you and I have used a lot is Twitter DM. Not because I think Twitter DM
[01:17:13.920 --> 01:17:19.200]  is secure, but because everything that goes over Twitter DM, it's almost more like a more
[01:17:19.200 --> 01:17:24.080]  convenient form of email. And, you know, I don't trust it any more than I trust email, which is to
[01:17:24.080 --> 01:17:33.040]  say in terms of security, not at all. And I have my Twitter DMs open. I did this a couple months
[01:17:33.040 --> 01:17:38.240]  ago and I thought it would be a disaster and I'd have to quick hurry, shut the door and close them.
[01:17:38.240 --> 01:17:45.280]  And it's actually worked out amazingly well. I get very little spam, maybe like two or three a week.
[01:17:47.280 --> 01:17:51.680]  And, you know, the ones that I get that I wouldn't have been able to get before, in other words,
[01:17:51.680 --> 01:17:58.320]  DMs from people who I don't follow are very, you know, they're nice. They're just from readers and
[01:17:58.320 --> 01:18:03.760]  readers at the website and listeners of the show. And the thing I kind of extra like about it is
[01:18:03.760 --> 01:18:13.760]  the nature of a Twitter DM user interface wise, promotes brevity. I mean, no offense to those of
[01:18:13.760 --> 01:18:19.040]  you send very long emails, but email, you know, it's very easy to write a long email and it's
[01:18:19.040 --> 01:18:24.560]  very, it's not so easy to write a very long DM. And so getting feedback from listeners and readers
[01:18:24.560 --> 01:18:27.920]  in the context of a DM is actually more convenient than an email for me.
[01:18:29.280 --> 01:18:35.840]  Yeah, I use iMessage for anything and everything I can. The problem is, is I have a wider range
[01:18:35.840 --> 01:18:41.440]  of people, I think, and quite a few in particular, like family members and some friends that are not
[01:18:41.440 --> 01:18:46.720]  on iMessage. And so then I've got to use SMS for them. I don't put anything in there that would be
[01:18:46.720 --> 01:18:52.320]  not sensitive, but at work I do. So there's times I need to exchange passwords and, you know,
[01:18:52.320 --> 01:18:59.040]  not a lot, but I'll just go to iMessage. I trust the encryption on that. Twitter DM, yeah, is
[01:18:59.040 --> 01:19:03.680]  same thing. Number two, I've got multiple ones here with various people, but again,
[01:19:03.680 --> 01:19:10.240]  I assume it's insecure, especially because I had some of my stuff hacked. So it wasn't me.
[01:19:10.240 --> 01:19:16.160]  It was, there was one of those big exposure things and this is a while ago now. And somebody
[01:19:16.160 --> 01:19:21.360]  got into Dan Kaminsky's Twitters and Dan and I had exchanged some messages about something. And
[01:19:21.360 --> 01:19:26.560]  one of the things was not horrible. It was mildly embarrassing. It was something like in, because
[01:19:26.560 --> 01:19:31.120]  it was about somebody else, but it was, it wasn't insulting in any way. But it was enough. I had to
[01:19:31.120 --> 01:19:36.080]  go talk to this person who I think was wicked drunk at the time. So he didn't care when I went
[01:19:36.080 --> 01:19:41.680]  to him, finally had the conversation, you know, and that re reinforced, you can't trust it.
[01:19:41.680 --> 01:19:48.000]  I hate to say it, I have to use Facebook stuff. I really try and minimize and compartmentalize it.
[01:19:48.000 --> 01:19:52.560]  Like I only use Facebook from my phone. I have it isolated out from like everything else and
[01:19:52.560 --> 01:19:57.440]  Facebook messenger. So I have to use that sometimes for like, you know, family members
[01:19:57.440 --> 01:20:03.920]  and friends that I don't talk to more than once every five years. And but the sensitive stuff,
[01:20:03.920 --> 01:20:08.960]  iMessage first and Signal or Wicker would be the two that I would drop back to after that.
[01:20:08.960 --> 01:20:15.120]  So, you know, Signal I know, and that is, you know, very well known, but I'm not familiar with
[01:20:15.120 --> 01:20:23.680]  Wicker at all. How do you spell it? I think it's W I C K R or W let me pull it up. Yeah, it's one
[01:20:23.680 --> 01:20:30.560]  of those different spellings. So it's W I C K R. And Wicker me is the name of the app. I guess they
[01:20:30.560 --> 01:20:34.160]  changed the name of the app. A friend of mine started the company. So someone I do stuff with
[01:20:34.160 --> 01:20:38.400]  at Def Con and everything, which is how I got involved there. So a bunch of the Def Con crew,
[01:20:38.400 --> 01:20:46.000]  because I work at Def Con, will use Wicker. But it's one of those two. But I like, I mean,
[01:20:46.000 --> 01:20:50.880]  I use either one of them like once a year at this point. The thing I don't like about Signal is that
[01:20:52.240 --> 01:20:58.160]  your ID is your phone number. And I don't like giving out my phone number. Like I've seen a lot
[01:20:58.160 --> 01:21:01.840]  of people in the press who who publicize, you know, you can reach me on Signal, you know,
[01:21:01.840 --> 01:21:10.000]  privately, and here's, here's my phone number. And I don't have I have secondary phone, I'm
[01:21:10.000 --> 01:21:15.520]  certainly not going to use my main phone number for it. But I have like a, you know, if I spend
[01:21:15.520 --> 01:21:19.840]  50 bucks a month for a second SIM that I keep in like an Android phone, just for testing, and to
[01:21:19.840 --> 01:21:24.800]  have another SIM, I could use that, I guess. And I don't really care if it's public. But I don't
[01:21:24.800 --> 01:21:27.520]  know, it still feels like an invitation to have my phone ringing off the hook.
[01:21:27.520 --> 01:21:33.920]  Yeah, well, and for me, it's people that I know in the industry, and who want to, they don't trust
[01:21:33.920 --> 01:21:39.200]  iMessage, or they use, or they use Android, which I don't understand any security pros that use
[01:21:39.200 --> 01:21:44.720]  Android. At this point, you just shouldn't be doing that. But But some people do, because,
[01:21:44.720 --> 01:21:51.200]  you know, freedom or something, I don't know. And so that's when I have to use drop back to Signal.
[01:21:51.200 --> 01:21:54.800]  Do you feel differently about Android? Do you feel differently about Google's like the Pixel phones?
[01:21:54.800 --> 01:22:00.160]  I would use a Pixel phone if I had to. They're not there yet. And that's the problem. Like,
[01:22:01.440 --> 01:22:05.280]  Apple is such a lead, because of how tightly integrated the hardware is,
[01:22:05.280 --> 01:22:11.120]  and their secure coprocessors are better than the Google Pixel, what they have available in
[01:22:11.120 --> 01:22:17.280]  those devices. But But yeah, I would be if I had to use, I tell everybody, if you have used Android
[01:22:17.280 --> 01:22:21.680]  use a Pixel, I wouldn't really even the Samsung's, I'm not only that would be like the third choice
[01:22:21.680 --> 01:22:31.680]  would be a Samsung. Yeah. All right, let's, let's, let's keep going. So a couple episodes ago,
[01:22:31.680 --> 01:22:35.760]  Joanna Stern was on the show, I think it was two episodes ago. And she even mentioned at the end
[01:22:35.760 --> 01:22:38.800]  of the show, I said, What are you working on? She said she was working on a piece about webcams and
[01:22:38.800 --> 01:22:47.440]  webcam security. And then that column dropped last week. And I was intrigued by she so she hired like
[01:22:47.440 --> 01:22:54.480]  a security researcher to help attack her, you know, knowingly, to see, you know, how he could
[01:22:54.480 --> 01:23:01.840]  take control of the webcam on her. She had like an HP notebook running Windows 10, and a MacBook
[01:23:01.840 --> 01:23:06.640]  running presumably Mojave. I don't know if she said but you know, recent version of Mac OS 10.
[01:23:08.240 --> 01:23:15.360]  And I was intrigued by the vectors that were used to try to take control of it. But I disagreed with
[01:23:15.360 --> 01:23:21.440]  her advice at the end. And therefore, I've sort of wrote a response to it. Probably everybody
[01:23:21.440 --> 01:23:26.320]  listening has read it, you know, I assume, I don't know, I don't assume that everybody in life has
[01:23:26.320 --> 01:23:30.160]  read the stuff I read it during fireball, but I sort of assumed that people who listen to this
[01:23:30.160 --> 01:23:36.000]  podcast have. So I won't rehash the whole thing. But I thought it was just very odd. I thought that
[01:23:36.000 --> 01:23:45.040]  her column sort of proved that webcams are pretty safe. If you're running, you know, recent versions
[01:23:45.040 --> 01:23:51.360]  of either Windows or Mac OS 10, and, and pay attention to the default security warnings
[01:23:51.360 --> 01:23:56.240]  for things that you've downloaded and things asking for permissions. And yet her advice,
[01:23:56.240 --> 01:24:00.080]  you know, she was like, I'm, I'm still putting a cover on my webcam thing.
[01:24:00.960 --> 01:24:05.680]  Yeah, I, you know, I've got friends and stuff who do that. And, and maybe it's because they
[01:24:05.680 --> 01:24:12.480]  haven't just dug into, you know, particularly on Apple what they do. So first of all, if you're on
[01:24:12.480 --> 01:24:16.960]  a recent Mac, and I don't know how many years back, probably about two or three years back,
[01:24:16.960 --> 01:24:25.440]  I don't really worry at all. The webcam is tied to the light. And there that's done in hardware.
[01:24:25.440 --> 01:24:29.520]  Now, I had asked Apple, I think a year or two ago, and I didn't get a response,
[01:24:31.120 --> 01:24:36.640]  which was, is it hardware locked? Or is it a firmware lock? The difference being,
[01:24:36.640 --> 01:24:40.880]  if it's hardware locked, there's like some circuit that triggers it. If the camera is on,
[01:24:40.880 --> 01:24:46.400]  the light is on, right. And firmware being there's very close to that. But there's like a little
[01:24:46.400 --> 01:24:50.640]  software decision that's made. But technically, you could blow through it by by compromising the
[01:24:50.640 --> 01:24:54.960]  firmware. Either way, that's not something that's happening under normal circumstances.
[01:24:54.960 --> 01:24:58.240]  No, and there was and it's one of those things where once something bad happens,
[01:24:58.800 --> 01:25:03.280]  it lands in people's memory, and then they never shake it. And there was an interesting story that
[01:25:03.280 --> 01:25:11.280]  the FBI had some sort of exploit, like in the mid 2000s, that could take control of a like a MacBook's
[01:25:11.280 --> 01:25:18.640]  webcam remotely. But that's a long time ago on very different hardware. Yeah, and Windows systems,
[01:25:18.640 --> 01:25:24.960]  I don't think most of them have that level of protection. But the operating system is a lot
[01:25:24.960 --> 01:25:29.360]  stronger as well these days than it used to be. So I mean, I actually have an app that will tell me
[01:25:29.360 --> 01:25:33.360]  if my microphone or my camera is on. The main reason I have that it's one of the ones from
[01:25:33.360 --> 01:25:37.200]  Patrick Wardle. He does a lot of security research. It's more for the microphone part
[01:25:37.200 --> 01:25:42.160]  than the camera part. I've got that attached and also see what I'm just curious, you know,
[01:25:42.160 --> 01:25:47.120]  what processes are turning on my mic throughout the course of the day? Yeah. How's Chrome gonna
[01:25:47.120 --> 01:25:52.160]  screw up my computer today is how I otherwise refer to it. But um, yeah, Patrick Wardle has
[01:25:52.160 --> 01:25:56.800]  a utility. Do you know the name of it off the top of your oversight oversight? And yeah, I know that
[01:25:56.800 --> 01:26:03.920]  the guys at the who do little snitch have a utility to really I miss so and I have little
[01:26:03.920 --> 01:26:08.880]  snitch on like everything I own. It's not part of little snitch, but they they have a utility
[01:26:08.880 --> 01:26:14.800]  that does the same thing and gives you a notice when somebody when when some anything is accessing
[01:26:14.800 --> 01:26:20.240]  the microphone. And that was part of what I wrote was that I don't get this paranoia about the
[01:26:20.240 --> 01:26:27.840]  camera. I kind of do. I don't want to be obtuse. Like I kind of get it that nobody wants to be
[01:26:27.840 --> 01:26:33.120]  surreptitiously photographed. And, you know, there are people out there doing, you know,
[01:26:33.120 --> 01:26:38.720]  with bad intentions. And there are, you know, most people don't really know how a computer works. And
[01:26:38.720 --> 01:26:46.400]  so they have no idea whether they can trust that by going to xyz.com, whether xyz.com can access
[01:26:46.400 --> 01:26:53.040]  the, you know, can somebody clever who's making this website somehow turn on my webcam? And can
[01:26:53.040 --> 01:26:56.480]  they do it without having the light come on? I don't know. So I'm just going to cover it up.
[01:26:56.480 --> 01:27:03.920]  I mean, I mean, it is a real issue in a couple of cases. One, depending on like on Windows,
[01:27:03.920 --> 01:27:09.280]  I'd be much less confident. And it's Windows is way more secure than it used to be. But there's
[01:27:09.280 --> 01:27:13.600]  not the hardware software tie. So what we get with Macs, I mean, that's one of Apple's I got
[01:27:13.600 --> 01:27:18.320]  a piece I've been meaning to write for like a year, which is Apple's strategic security advantages.
[01:27:18.320 --> 01:27:22.800]  And owning the hardware and owning the software is just a massive advantage over everything else.
[01:27:22.800 --> 01:27:26.880]  That's why Google Pixel is, you know, the most secure of the Android devices, because Google
[01:27:26.880 --> 01:27:32.160]  has that that control of the entire ecosystem there. So I do worry a little bit more,
[01:27:32.720 --> 01:27:38.720]  even though Windows 10 is actually much better than it's ever been, and is really infection
[01:27:38.720 --> 01:27:43.200]  rates are a lot lower, it's still potentially more likely. On the Mac side, I don't worry about it at
[01:27:43.200 --> 01:27:47.600]  all if you know, unless I'm talking to somebody who's on a really old computer. And again,
[01:27:47.600 --> 01:27:52.080]  my definition of really old and yours might be different than Yeah, you know, other people out
[01:27:52.080 --> 01:27:57.840]  there that the objective development group is the people who make little snitch, they also make the
[01:27:57.840 --> 01:28:06.400]  excellent. One of my favorite utilities of all time, launch bar, but their, their app for letting
[01:28:06.400 --> 01:28:11.280]  you know when somebody has the microphone or the camera is called micro snitch. Anyway, I'll put
[01:28:11.280 --> 01:28:14.640]  a link to the in the show notes. I'm just having to use launch bar. It seems like everybody else
[01:28:14.640 --> 01:28:19.520]  is going to Alfred and stuff. And I love launch bar. I love launch bar. And I've looked at
[01:28:19.520 --> 01:28:27.120]  Alfred and I've, it looks like a great alternative. And, but I've, I've, I've never seen
[01:28:27.120 --> 01:28:31.840]  any, I don't see anything that would make me want to switch main if only because I've got these
[01:28:31.840 --> 01:28:38.000]  crazy muscle memory for launch bar. But anyway, but there are two great utilities. I don't know
[01:28:38.000 --> 01:28:44.400]  how anybody, a power user use goes without them. But yeah, I don't know. I just can't believe that
[01:28:44.400 --> 01:28:48.000]  people are so paranoid about the camera. I get it. I get it. You don't want pictures of you, but
[01:28:49.760 --> 01:28:54.480]  boy, I sure wouldn't want to be surreptitiously recorded either. And there is no indicator light
[01:28:54.480 --> 01:29:00.800]  for the microphone. And quite frankly, to me, it, it all comes down to the question of,
[01:29:00.800 --> 01:29:08.240]  do you trust the software running on your device? Yep. And I get it that there's people, you know,
[01:29:08.240 --> 01:29:11.840]  and in the old days, computers are so complicated. Now there's so much more complicated,
[01:29:11.840 --> 01:29:21.680]  like part of what makes on, on a really modern Mac book, the camera is for the webcam goes
[01:29:21.680 --> 01:29:28.640]  through the T2 security system. And that's an iOS computer. It's a computer in the computer.
[01:29:28.640 --> 01:29:37.040]  And it, it, it, you know, to use a variation of a word used by Jeff Bezos, it's a complexifier.
[01:29:38.320 --> 01:29:44.720]  It's really cool that there is an iOS computer running inside your Intel based Mac computer,
[01:29:45.360 --> 01:29:50.480]  doing a bunch of security related stuff, including completely controlling access to the camera.
[01:29:52.560 --> 01:29:57.840]  You know, so that nothing on the Mac can actually touch the camera, it has to make a request through
[01:29:57.840 --> 01:30:04.000]  an API and the API has to go through the T2, which is running iOS and has a secure enclave
[01:30:04.000 --> 01:30:09.520]  and all this other stuff. That's all cool. But it certainly, you know, it, it, it's complex for
[01:30:09.520 --> 01:30:14.560]  me to understand. And I, you know, have a degree in computer science and, and, and spend my days
[01:30:14.560 --> 01:30:18.480]  thinking about these things for a typical person. It's, it's a black box. I get it.
[01:30:19.520 --> 01:30:24.320]  Yeah, I'm not actually sure the T2 covers the camera. I'd actually pulled up that.
[01:30:24.320 --> 01:30:28.480]  I know it doesn't say so in that document, but I was told by somebody that it does go through
[01:30:28.480 --> 01:30:34.960]  there. I don't know. But since it's still just an API call that anything can make, I don't know.
[01:30:34.960 --> 01:30:39.760]  I think that's why Apple doesn't really call it out. It does go through the T2 though. And I think
[01:30:39.760 --> 01:30:46.080]  Face ID uses all the secure enclave stuff for sure. Yeah. Yeah. And I think that it's partly
[01:30:46.080 --> 01:30:52.240]  just to make it harder for anything on the Mac to access the firmware behind the camera that you
[01:30:52.240 --> 01:30:56.720]  can't even get to it because it's through there. So I don't think they brag about it in the white
[01:30:56.720 --> 01:31:01.520]  paper, but I know I I've been told, I don't know for a fact, but I've been told for a fact by
[01:31:01.520 --> 01:31:07.520]  somebody who would know that, that if you have a Mac with a T2, the webcam access goes through there.
[01:31:08.160 --> 01:31:13.280]  I had thought that, but I, I was right. That's I flat out pulled up the paper because I knew
[01:31:13.280 --> 01:31:16.720]  we're going to talk about this like, Oh, it's not in there. Yeah. So the cool thing that it
[01:31:16.720 --> 01:31:22.320]  does that the T2 ones do is that the microphone, and it is a physical disconnect when you close
[01:31:22.320 --> 01:31:28.480]  the lid on a modern Mac book that has a T2 the microphone is physically disconnected.
[01:31:28.480 --> 01:31:32.000]  So that there is, you know, there is the, the microphone actually doesn't even have
[01:31:32.000 --> 01:31:36.000]  an electronic connection. You know, it doesn't even get power when the lid is closed. It's
[01:31:36.000 --> 01:31:40.960]  actually physically disconnected. So you only have, you only, your microphone can theoretically
[01:31:40.960 --> 01:31:45.840]  only be used when the lid is open. You know, and that's also really good because I don't know how
[01:31:45.840 --> 01:31:49.680]  many like conference calls you're on. People are always closing their laptop lids and not realizing
[01:31:49.680 --> 01:31:54.080]  they're not running through the microphone on their headset and it stops that annoyance.
[01:31:54.080 --> 01:31:59.360]  Well, you know what? All right. So let me go through. So since I wrote my, my rejoinder to
[01:31:59.360 --> 01:32:04.880]  Joanna's piece on webcams, let me, you know, I heard from readers, you know, all very polite
[01:32:04.880 --> 01:32:09.360]  and it it's one of those things where, you know, we can agree to disagree. And it actually makes
[01:32:09.360 --> 01:32:16.320]  me very happy in this era of, of everything turns into a shouting argument and nobody listens to the
[01:32:16.320 --> 01:32:21.040]  other side where it was all reasonable. And it made me very happy about, you know, the level
[01:32:21.040 --> 01:32:29.920]  of discourse between me and my readers. But some, one of the reasons that, that readers said, well,
[01:32:29.920 --> 01:32:35.840]  I get what you're saying. I don't disagree with anything you're saying, but I can't live without
[01:32:35.840 --> 01:32:40.720]  a webcam cover, you know, like one of these little stick on things with a switch because I'm on
[01:32:40.720 --> 01:32:48.480]  conference calls all the time. And I want, you know, I, I just, I just want to know that I'm
[01:32:48.480 --> 01:32:51.440]  not being photographed when I don't think I'm being photographed totally reasonable. So, you
[01:32:51.440 --> 01:32:57.040]  know, that is that, that to me, isn't a voodoo. It's not a, I have no idea what an attacker could
[01:32:57.040 --> 01:33:00.800]  do. And so who knows what's running on my computer. It is, I know exactly what I'm running.
[01:33:00.800 --> 01:33:07.920]  I'm running whatever, you know, video conferencing software. And I want to physically control
[01:33:07.920 --> 01:33:12.000]  when I know that I'm being photographed or not. And that's totally reasonable.
[01:33:12.000 --> 01:33:15.440]  Yeah. And that goes back to the software trust, like everything I've got running,
[01:33:15.440 --> 01:33:19.280]  I've got to open it and turn it on, but I know there's things that people use in enterprises
[01:33:19.280 --> 01:33:23.840]  or whatever, where it could pop it open or they accidentally have it on when they join a call.
[01:33:23.840 --> 01:33:29.120]  Yeah. I'm cool with that. To be honest, there's something to be said for placebos, like,
[01:33:29.120 --> 01:33:33.680]  you know, it makes you feel better. It's like, my wife gives me crap all the time because I'm a
[01:33:33.680 --> 01:33:39.680]  scientific skeptic. I'm a paramedic. I got a bio, like in college, like somebody comes out with the
[01:33:39.680 --> 01:33:45.200]  latest, I'm doing this diet or that diet or this or that or chiropractic or whatever. It's like,
[01:33:45.200 --> 01:33:48.400]  yeah, none of that shit works, but I'm not going to tell you because it's not going to improve
[01:33:48.400 --> 01:33:53.600]  anything. And it's causing no harm to you to do that. Joanna's article though, I didn't like,
[01:33:53.600 --> 01:33:59.520]  because, and she is an exceptional journalist. Like, I feel like that I don't like to criticize
[01:33:59.520 --> 01:34:05.120]  that, you know, somebody who's that good at what she does, but all those articles,
[01:34:05.120 --> 01:34:10.960]  and this is a bigger issue, it's like really easy to fall into this trap when you're working with
[01:34:10.960 --> 01:34:16.960]  a security researcher, hacker type, and they may not even realize they're doing it where you want
[01:34:16.960 --> 01:34:20.640]  them to show you something. They come up with a way to do it, but you got to do like the four
[01:34:20.640 --> 01:34:25.840]  or five manual steps to allow it. And she detailed all of those out. What I've learned over the years
[01:34:25.840 --> 01:34:30.320]  though, is people lose that context when they read those articles, right? So I headline the
[01:34:30.320 --> 01:34:33.600]  first graph and the last graph, and then they forget everything in the middle.
[01:34:33.600 --> 01:34:38.080]  Right. That was, I don't think it was the very ending, but it was like the ending of part one
[01:34:38.080 --> 01:34:41.520]  of my thing, which is basically people are going to take away from this. The Wall Street Journal
[01:34:41.520 --> 01:34:46.960]  says you should cover up your webcam. Yeah. And that to me is the wrong conclusion. Right. And
[01:34:46.960 --> 01:34:54.320]  the steps she had to jump through, and to Microsoft's credit, to me, it was equally
[01:34:54.320 --> 01:35:00.720]  convoluted on the Windows machine with the modern, Windows 10 and Windows Defender running.
[01:35:02.000 --> 01:35:07.840]  You had to ignore some pretty, some pretty, really big warnings. Yeah. Really big warnings.
[01:35:07.840 --> 01:35:16.240]  You had to click through and the guys under which her Mac's camera was compromised
[01:35:16.240 --> 01:35:21.120]  was somebody applying for a job that she was looking for to help with video production and
[01:35:21.120 --> 01:35:26.880]  send her a resume that required LibreOffice. Number one, who sends the resume in that format?
[01:35:26.880 --> 01:35:33.040]  I mean, you know, who doesn't? Somebody who doesn't want the job? Right. And once opened,
[01:35:33.040 --> 01:35:39.920]  required decreasing the level of macro security in LibreOffice to a point where LibreOffice says,
[01:35:39.920 --> 01:35:46.720]  are you sure this is really insecure? I mean, whose resume has, A has macros period, but B has
[01:35:46.720 --> 01:35:52.240]  macros that require you to decrease the default security settings. And then even then, it still
[01:35:52.240 --> 01:36:00.560]  said, are you going to allow this app access to your camera? Cancel or OK? Yeah, it's so we have
[01:36:00.560 --> 01:36:05.360]  this thing on and the light went on. There was still, I haven't seen any proof. And again,
[01:36:05.360 --> 01:36:10.560]  and again, maybe there is a way and maybe it's a deep dark secret on the dark web. And it's tightly
[01:36:10.560 --> 01:36:15.200]  held secret that there's some kind of way to get the MacBook or modern MacBook camera on without
[01:36:15.200 --> 01:36:20.800]  having the light come on. But if there is, I haven't seen it. You know, it's a category of
[01:36:20.800 --> 01:36:26.800]  something that we call stunt hacking. And stunt hacking is where you do something big and
[01:36:26.800 --> 01:36:33.440]  exploitative to get attention frequently. Now, this was not directly that case. And I want to be
[01:36:33.440 --> 01:36:37.600]  really clear about that because the researcher wasn't like trying to trick her or make it out
[01:36:37.600 --> 01:36:41.600]  to be bigger than it was. This was a legitimate collaboration between the journalist and the
[01:36:41.600 --> 01:36:46.480]  researcher. And he's like, yeah, here's the technique. So it's not really stunt hacking
[01:36:46.480 --> 01:36:52.160]  in the same way. And sometimes stunt hacking can be good. So when Charlie Miller and Chris Vlasic
[01:36:52.160 --> 01:36:58.560]  like hacked cars, because that was legit, it was sensationalistic, but it also woke people up to an
[01:36:58.560 --> 01:37:04.560]  issue that was legitimately being ignored. The problem is when it goes bad. And that's why,
[01:37:05.120 --> 01:37:09.040]  like I am very sensitive to these kinds of articles, because the average reader is not
[01:37:09.040 --> 01:37:13.840]  going to know the difference. Years ago, like when TJX got hacked, I actually was working with
[01:37:13.840 --> 01:37:19.360]  60 Minutes. I got introduced to a 60 Minutes producer to do a whole piece on that kind of
[01:37:19.360 --> 01:37:25.120]  hacking. And if you remember, that was like they were using WEP for their network. And so the bad
[01:37:25.120 --> 01:37:31.280]  guy's war drive used the old, for those who don't remember, WEP was the not very secure Wi-Fi
[01:37:31.280 --> 01:37:36.640]  standard encryption, broke into their network, and they were able to sniff the credit cards and got
[01:37:36.640 --> 01:37:40.960]  many millions. And I remember it was like, it was enticing because I'm driving around with this
[01:37:40.960 --> 01:37:47.760]  producer. And he flew into Phoenix where I'm living these days or those days. And I'm driving,
[01:37:47.760 --> 01:37:51.840]  I think it was outside of a Home Depot. And I'm like, yep, okay, I'm doing war driving like, yep,
[01:37:51.840 --> 01:37:56.080]  that's insecure, that's insecure. And he really, he's like talking about where the cameras are
[01:37:56.080 --> 01:37:59.360]  going to be placed and where Leslie Stahl is going to sit and all this other stuff.
[01:37:59.360 --> 01:38:04.320]  And he wanted me to like, guarantee that that was not insecure, so she could walk into the office
[01:38:04.320 --> 01:38:08.880]  with the tape and show them, you know, have that have that gotcha moment. And I looked at the guy,
[01:38:08.880 --> 01:38:15.520]  I'm like, I'm not breaking the law to be on TV. Like, is that secure behind the scenes? I'm not
[01:38:15.520 --> 01:38:22.240]  going to fake it and say that it was definitively insecure. Because yeah, could I break that wireless
[01:38:22.240 --> 01:38:27.600]  network easy? Like at that point, it was so easy. Anybody could do it. But behind the scenes,
[01:38:27.600 --> 01:38:31.920]  maybe they have those connections encrypted. And I couldn't do anything bad. Like, I don't know,
[01:38:31.920 --> 01:38:36.400]  I'm not going to put my reputation on the line. And they ended up doing that story like,
[01:38:36.400 --> 01:38:41.440]  nine months later with other people. And which I was, I don't need that kind of fame. So I was
[01:38:41.440 --> 01:38:46.960]  totally fine with it. But it was really interesting. Because there's different desires,
[01:38:46.960 --> 01:38:51.280]  the journalists, you need the page views and everything. Some researchers do want to build
[01:38:51.280 --> 01:38:55.600]  up their reputations. But it's a fine line when you start writing these pieces without the right
[01:38:55.600 --> 01:38:59.920]  context. Is it sort of like, you know, like, hey, like in the physical world, like, hey, the front
[01:38:59.920 --> 01:39:06.080]  door is open, I can just open this front door. And I, I don't know if I can go in there and steal
[01:39:06.080 --> 01:39:10.240]  money from their cash register. I don't know if the cash register is locked or not. But I'm not
[01:39:10.240 --> 01:39:14.480]  going into the store. I'm not going into their store while it's closed in the middle of the night
[01:39:14.480 --> 01:39:20.800]  just to see, you know, it's even more like, I walk and I look, and I see the door cracked open,
[01:39:20.800 --> 01:39:25.840]  or the window cracked open, like, without even touching it. Yeah. Yeah. It's that
[01:39:26.720 --> 01:39:33.360]  pop it open and go in. No, because fucking Robocop might be there. Like, I'm not. Yeah, exactly.
[01:39:33.360 --> 01:39:37.840]  Now, admittedly, I think they probably had horrible security behind that. But um,
[01:39:37.840 --> 01:39:44.160]  and fully compromiseable, but I can't guarantee it. And I'm sure as hell not going on in national
[01:39:44.160 --> 01:39:49.120]  television and saying I could, I will say this, I heard from somebody who knows, I mentioned a few
[01:39:49.120 --> 01:39:55.120]  episodes ago that I'd mentioned that famously, everybody seems to think Mark Zuckerberg uses a
[01:39:55.120 --> 01:39:59.280]  webcam, a piece of tape over his webcam, because there was a picture of him once a couple years
[01:39:59.280 --> 01:40:04.000]  ago, sitting in front of a MacBook with a piece of tape over the webcam and a piece of tape over
[01:40:04.000 --> 01:40:08.960]  the microphone. And I mentioned, I don't know which episode was I mentioned a few episodes ago
[01:40:08.960 --> 01:40:13.680]  that I don't know if that was actually nobody's ever confirmed that that's his device. Somebody
[01:40:13.680 --> 01:40:18.080]  who works at Facebook wrote to me and say I can confirm that Mark uses Mark Zuckerberg does use
[01:40:18.800 --> 01:40:24.160]  a webcam cover. So there it is Mark Zuckerberg does but he said it's probably not even his choice.
[01:40:24.160 --> 01:40:29.600]  He's got like a team of like eight security people and just does whatever they say. But I will say
[01:40:29.600 --> 01:40:35.600]  this, a piece of tape over your microphone does not does not block the microphone. And I encourage
[01:40:35.600 --> 01:40:42.000]  anybody who thinks that it does to try it put a piece of tape over your phone that I've seen
[01:40:42.000 --> 01:40:47.920]  people do it. So the thing that I've seen that you can buy on Amazon is you can buy it's I don't
[01:40:47.920 --> 01:40:52.800]  know what they call it. But you can buy like a dummy microphone that plugs into the microphone
[01:40:52.800 --> 01:40:59.200]  jack. And it doesn't actually do anything. But when you have a microphone plugged in the microphone
[01:40:59.200 --> 01:41:04.080]  jack, the MacBook defaults to using that external microphone is the microphone. And so it's just
[01:41:04.080 --> 01:41:10.080]  like a dummy plug. Yeah, that would actually work to block your microphone. So if you actually do
[01:41:10.080 --> 01:41:16.000]  for whatever reason, want to block your Mac's microphone by default, I would suggest buying
[01:41:16.000 --> 01:41:21.360]  one of those guess what they cost exactly as much as you think they're like $4 at Amazon,
[01:41:21.360 --> 01:41:25.440]  but buy one of those don't don't don't think you can cover your microphone with a piece of tape.
[01:41:25.440 --> 01:41:30.960]  It doesn't work. And don't be a target because if the attacker actually really wants to get
[01:41:30.960 --> 01:41:36.560]  your microphone and they know, I mean, they can just look for multiple sound sources and switch.
[01:41:36.560 --> 01:41:41.280]  No, it's just I'm assuming that's probably not built into average malware. So that makes sense.
[01:41:41.280 --> 01:41:45.920]  Yeah. Basically, though, like I said before, it really does come down to do you trust the
[01:41:45.920 --> 01:41:50.240]  software running on your device? And if you don't, I mean, it's a game over. And you know,
[01:41:50.240 --> 01:41:53.520]  like, well, I don't know why anybody would trust their keyboard. If you don't, if you think that
[01:41:53.520 --> 01:41:58.640]  you've your computer is running software that could take control of your webcam, and could
[01:41:58.640 --> 01:42:02.560]  take control of it in a way that wouldn't even show the indicator light. I don't know why you
[01:42:02.560 --> 01:42:06.640]  trust your keyboard. Why do you ever? I mean, why? I don't know how you could live if you don't
[01:42:06.640 --> 01:42:12.160]  trust the software on your device. Yeah, I mean, I mean, keyboards have firmware on them. And
[01:42:12.160 --> 01:42:15.520]  there's actually been stories, there's been versions of those that have been cracked. And
[01:42:16.240 --> 01:42:20.240]  but I mean, these are like rare. I mean, this is the whole like thing, like, what's the real
[01:42:20.240 --> 01:42:25.360]  risk to the average person? Like, I mean, you're kind of a target. I'm kind of a target in different
[01:42:25.360 --> 01:42:30.320]  ways, because of the target than an average person. Yeah, but not not a big target. And,
[01:42:32.160 --> 01:42:36.400]  you know, and so our risk assessments are different than like, you know, Jeff Bezos or
[01:42:36.400 --> 01:42:42.000]  Zuckerberg. And the nature of the kinds of attacks that that you're going to be subject to are going
[01:42:42.000 --> 01:42:45.920]  to be completely different, the average person just getting malware. If that malware can't,
[01:42:45.920 --> 01:42:49.600]  like pop that microphone right away, or get that webcam right away, they're moving on to the next
[01:42:49.600 --> 01:42:54.160]  one. Yeah. All right, let me take a break here and thank our next sponsor. So good friends it
[01:42:54.160 --> 01:43:01.760]  away. Look away makes some of the best luggage you're ever going to see. And look, they considered
[01:43:01.760 --> 01:43:05.440]  all types of travelers when they made their carry on and that's why they make it in two sizes.
[01:43:06.240 --> 01:43:09.680]  They have the carry on the regular carry on and then like the bigger carry on.
[01:43:11.200 --> 01:43:17.280]  And they come with an optional ejectable battery. I love this battery. I love it. You just have like
[01:43:17.280 --> 01:43:25.920]  this big, super long powered battery right on top of your carry on suitcase with two USB ports,
[01:43:25.920 --> 01:43:30.640]  one of them which does the PD so you get the extra power to get like 10 or 12 watts out of it.
[01:43:31.920 --> 01:43:36.480]  You just plug a cable in right in your suitcase, plug it in your phone while you're at the airport
[01:43:36.480 --> 01:43:40.320]  waiting for your seat. So every single seat for me, I can go anywhere in the airport and
[01:43:40.320 --> 01:43:45.040]  wherever I decided to sit, I have a charger near me because I've got my carry on with me because
[01:43:45.040 --> 01:43:50.960]  I have an away suitcase. I love it. I love this feature of it. It is true that in recent years,
[01:43:50.960 --> 01:43:56.480]  airlines have come out with these regulations on lithium ion batteries. That's why the away
[01:43:56.480 --> 01:44:01.760]  battery pops right out. You just push it in, pop it right out. So if you need to like gate check
[01:44:02.320 --> 01:44:07.600]  your suitcase and it has to go under and you can't put a battery in, it's like click it out,
[01:44:07.600 --> 01:44:12.240]  keep it with you at your seat and then you can pop it back in when you get your suitcase back.
[01:44:12.240 --> 01:44:18.960]  No problem. No tools required. Nothing like that. Couldn't be easier. It's really, I recommend the
[01:44:18.960 --> 01:44:23.040]  suitcase for that reason in and of itself. You don't have to carry a separate battery.
[01:44:24.960 --> 01:44:30.400]  They've got two types of material. They've got their lightweight, durable German polycarbonate
[01:44:30.400 --> 01:44:36.800]  and now they have suitcases made with a great aluminum alloy. You can charge your phone. Their
[01:44:36.800 --> 01:44:42.880]  battery can power your phone like an iPhone 10 S or tennis max like five times and it's completely
[01:44:42.880 --> 01:44:49.200]  TSA compliant. They have a great interior compression system that lets you pack more.
[01:44:49.200 --> 01:44:54.160]  They've got this great, it seems so simple, but my old carry on didn't have anything like it. It's
[01:44:54.160 --> 01:44:58.880]  this great little system and these two straps you can put button down shirts in there, put this
[01:44:58.880 --> 01:45:04.560]  strap over it and it keeps the keeps the shirts from getting wrinkled up. It's a great system.
[01:45:04.560 --> 01:45:08.400]  My shirts used to come out of the suitcase all wrinkled up. Now they don't. I love it.
[01:45:08.400 --> 01:45:13.600]  They've got four 360 degrees spinner wheels. The wheels are fantastic. I've had my away suitcase
[01:45:13.600 --> 01:45:16.880]  for years now. I don't know how many years they've been sponsoring this show, but I've had the same
[01:45:16.880 --> 01:45:21.040]  suitcase ever since they started sponsoring the show. It still looks brand new. It looks brand
[01:45:21.040 --> 01:45:28.000]  new. The wheels still spin like new. We have terminals at Philadelphia where they go downhill
[01:45:28.000 --> 01:45:32.800]  or uphill. The wheels spin so good that it actually, I actually have to make sure I hold
[01:45:32.800 --> 01:45:36.400]  onto the suitcase, right? Because it would just go flying away with me as it goes downhill.
[01:45:36.400 --> 01:45:41.040]  They spin so great. Even comes with a TSA approved combination lock.
[01:45:43.040 --> 01:45:48.880]  It comes with a removable washable laundry bag. Can't say enough about how great the away
[01:45:48.880 --> 01:45:52.480]  suitcases are. They have other ones that aren't carry on size, but the carry on is the one that
[01:45:52.480 --> 01:45:56.480]  I live and die by because I like to travel just with the carry on. I don't like to check baggage
[01:45:56.480 --> 01:46:03.760]  check baggage. And they have a special offer for listeners of the show. 20 bucks off a suitcase
[01:46:03.760 --> 01:46:11.680]  by visiting away, travel.com. That's away, travel.com slash talk show away, travel.com
[01:46:11.680 --> 01:46:17.440]  slash talk show. And just remember this promo code talk show 20 talk show. Cause this is the
[01:46:17.440 --> 01:46:23.520]  talk show 20 cause you'll save 20 bucks off a suitcase. So go to away, travel.com slash talk
[01:46:23.520 --> 01:46:28.320]  show. Use that URL down there. You came from the show and remember this code talk show 20
[01:46:30.880 --> 01:46:35.120]  and you'll save 20 bucks. Great product. I would recommend it even if they weren't a sponsor.
[01:46:36.720 --> 01:46:40.720]  You know, I've listened to the show. I've not heard that ad. I'm like looking at the website
[01:46:40.720 --> 01:46:47.120]  now cause I travel a lot. I will say this. I wouldn't put, wouldn't put gold bullion in
[01:46:47.120 --> 01:46:52.720]  a suitcase if it's got a TSA approved lock. I can't, I mean, it's the best you can do.
[01:46:52.720 --> 01:46:57.840]  It's the best you're allowed to do is a TSA approved lock. Let's just face it. A TSA approved
[01:46:57.840 --> 01:47:03.600]  lock is not really much of a lock. I can't, can't have a, can't have rich mogul on the show and let
[01:47:03.600 --> 01:47:08.560]  it, let it go that, but do you remember that story where somebody took a picture of the TSA guy
[01:47:08.560 --> 01:47:13.600]  holding up like all the keys on the key ring and then everybody like, Hey, let's go 3d print all
[01:47:13.600 --> 01:47:20.480]  the TSA locks. Oh my God. That was, can't make this up. That was a good one. Yeah. I can't say
[01:47:20.480 --> 01:47:27.360]  that I use the lock. Well, but it's, you know, pretty basic airport security as to, you know,
[01:47:27.920 --> 01:47:32.160]  I don't think you can go more than 50 feet without hearing a recorded voice telling you not to leave
[01:47:32.160 --> 01:47:39.280]  your bag unattended. I'm like really particular about how I organize the inside of my bags.
[01:47:39.280 --> 01:47:43.920]  Yeah. Like travel just cause it's gotta be the same. You spend enough time on the road. You don't
[01:47:43.920 --> 01:47:48.800]  want to think about it when you show up someplace at 2 AM. Which is why, sorry, I'll stop browsing
[01:47:48.800 --> 01:47:54.560]  their website now. Go back to the interview. Let me think about what else here. Let me look
[01:47:54.560 --> 01:47:59.040]  at my list and what else do we have to talk about? We're still, we still have a fair amount. Let me
[01:47:59.040 --> 01:48:06.320]  say this while we're talking about, I mean trusting the, the, the software on your device.
[01:48:07.280 --> 01:48:13.360]  There's been a story, it started with Facebook and then it expanded to Google, but, but Facebook for
[01:48:13.360 --> 01:48:21.120]  a while had a, this a VPN that they're quote unquote giving away free. It was called a VONO,
[01:48:21.120 --> 01:48:25.520]  a VON, a VONO, a VON. I, I get it wrong. It doesn't matter. They had to get rid of it.
[01:48:26.080 --> 01:48:32.720]  But basically they were using this VPN ostensibly to provide you with security with a VPN,
[01:48:32.720 --> 01:48:36.720]  but they were using it to snoop on all of the VPN traffic to see what people were doing on their
[01:48:36.720 --> 01:48:40.880]  phones and using it to figure out, Hey, everybody's using this WhatsApp, even though they
[01:48:40.880 --> 01:48:44.880]  want $20 billion, it's worth it because look at, look at the, you know, look what we can show
[01:48:44.880 --> 01:48:50.480]  people are doing. Well, Apple put the kibosh on that. And then like immediately afterwards,
[01:48:50.480 --> 01:48:57.360]  Facebook started distributing an app and paying people, including kids down to 13 years old,
[01:48:57.360 --> 01:49:04.480]  like 20 bucks a month or something to install this, which was effectively giving, you know,
[01:49:04.480 --> 01:49:09.120]  Facebook all of their traffic. And they were just, you know, you think, well, that can't go
[01:49:09.120 --> 01:49:12.000]  through the app store. Well, it wasn't going through the app store. They were distributing
[01:49:12.000 --> 01:49:16.560]  it as an enterprise beta, which is not what the enterprise beta system was meant for.
[01:49:18.080 --> 01:49:22.080]  So tech crunch and uncovered that turned out Google was doing something similar and
[01:49:22.080 --> 01:49:25.600]  Google got on top of it. And rather than taking Facebook's route of,
[01:49:25.600 --> 01:49:29.120]  we didn't think we were doing anything wrong. Google was like, Ooh, we're sorry.
[01:49:30.960 --> 01:49:36.320]  But there was, you know, an embarrassing few days for Facebook. And I think for Google too,
[01:49:36.320 --> 01:49:40.800]  where, where Apple had revoked their enterprise certificates, which meant that all of their
[01:49:40.800 --> 01:49:46.880]  beta software inside the company was, was inoperative. So even like beta versions of
[01:49:46.880 --> 01:49:51.680]  Facebook and I guess, Instagram and all sorts of customers. It wasn't just beta. It was their,
[01:49:51.680 --> 01:49:57.120]  their internal app certificate. So any enterprise apps beta or not. Yeah. And like Facebook has an
[01:49:57.120 --> 01:50:01.040]  app, I forget what they call it, but it's effectively like a separate version, a shadow
[01:50:01.040 --> 01:50:05.680]  version of Facebook just for employees where they, that's how they communicate with each other.
[01:50:05.680 --> 01:50:08.320]  All of those things stopped working cause they revoked these certificates.
[01:50:11.120 --> 01:50:16.240]  But it turns out, and I, I did, I really had no idea about this, but as this story was unfolding
[01:50:16.240 --> 01:50:19.280]  and I was writing about it, people would write to me and say, you know, this is a bigger story
[01:50:19.280 --> 01:50:25.680]  than you think. And they'd like link to you know, various things, all sorts of companies are mis
[01:50:25.680 --> 01:50:33.200]  were, are, are present tense misusing enterprise certificates in the same way to effectively allow
[01:50:33.200 --> 01:50:38.080]  what we've always thought wasn't really allowed on the iPhone, which is sideloading, which is loading
[01:50:38.080 --> 01:50:44.640]  native apps not through the app store. I mean, there's gambling apps, there's porno apps,
[01:50:44.640 --> 01:50:48.160]  there's just all sorts of apps that aren't going through the app store that you can get,
[01:50:50.240 --> 01:50:56.400]  you know, just by installing a beta certificate. Well, I mean, I knew sideloading was going on. I
[01:50:56.400 --> 01:51:00.160]  mean, I work with enterprises, so I see those, you know, all the time as part of their, you know,
[01:51:00.160 --> 01:51:06.000]  legitimate internal apps and stuff that they get loaded. And I knew, I've seen this abused,
[01:51:06.000 --> 01:51:11.520]  actually, in malware. So the one kind of tricky malware thing that that does work on iOS, if you
[01:51:11.520 --> 01:51:16.960]  can pull it off is, if you get someone to deploy one of those device profiles, then they can have
[01:51:16.960 --> 01:51:21.200]  full access and sniff all the content on your device at that point, potentially, depending on
[01:51:21.200 --> 01:51:26.000]  how all that stuff's configured. I just had no idea it was being used for, you know, all these
[01:51:26.000 --> 01:51:31.680]  other side store things. And so it makes me wonder, like, we use Circle here at home to
[01:51:32.320 --> 01:51:37.920]  monitor the kids. And it's an app, it's like the Circle by Disney thing. So I got the little puck,
[01:51:37.920 --> 01:51:42.640]  and it actually does some things that are normally bad for security, but are usable to monitor kids'
[01:51:42.640 --> 01:51:46.640]  activity. So we can limit the amount of YouTube time and stuff that they've got with the younger
[01:51:46.640 --> 01:51:51.600]  kids, it's great. And they have a version that will work on devices. And I know you have to use
[01:51:51.600 --> 01:51:57.280]  a device certificate and basically what these apps do. So kind of makes me curious if there are lines
[01:51:57.280 --> 01:52:01.440]  and if like, that's considered legitimate or not, and they don't hide that that's what it does. So
[01:52:01.440 --> 01:52:06.400]  I'm kind of hoping that is real. Have you ever seen the site builds be, you know, like building
[01:52:06.400 --> 01:52:14.640]  builds.io. And they have the store. I mean, go check this out. I mean, it's, it's unbelievable.
[01:52:14.640 --> 01:52:19.120]  I mean, and it's chock full of, I mean, it seems like the main thing that builds.io has
[01:52:19.120 --> 01:52:31.520]  are game emulators, like Game Boy emulator and a PSP emulator and an S super, because all these
[01:52:31.520 --> 01:52:35.280]  emulators are not allowed in the App Store for copyright reasons. So you can't put like a
[01:52:35.280 --> 01:52:40.400]  Nintendo Game Boy emulator, you know, in the App Store, because Apple isn't going to let it fly
[01:52:41.040 --> 01:52:46.000]  for copyright reasons, because the only way to play the games is with, you know, using game
[01:52:46.000 --> 01:52:54.400]  ROMs that you don't have legal right to. But this builds.io is just chock a block full of emulators.
[01:52:54.400 --> 01:53:02.400]  And like you said, Bitcoin miners and stuff to itorrent clients. All sorts of stuff. There's
[01:53:02.400 --> 01:53:06.160]  some other I forget, I just linked to it yesterday, and they already got shut down after I linked to
[01:53:06.160 --> 01:53:10.000]  it. But it's like a whack a mole type thing where they, you know, every couple of weeks, they just
[01:53:10.000 --> 01:53:15.760]  come up this somebody told me they just come up with a new name. And just go to a new site and
[01:53:15.760 --> 01:53:20.400]  have a new certificate. And it's just from some random company in China. But effectively, it's
[01:53:20.400 --> 01:53:26.800]  just a way to to stream pirated movie and TV content. I mean, this I had no idea that I mean,
[01:53:26.800 --> 01:53:28.320]  there's a this is a giant.
[01:53:30.240 --> 01:53:36.160]  Now I had, I'm looking at this now. And all the Yeah, I had no idea this. Oh, yeah, all these.
[01:53:36.720 --> 01:53:43.520]  Yep. It's a box. Yeah, I mean, it's just, it's rampant. It's absolutely like Facebook is far
[01:53:43.520 --> 01:53:47.520]  from the exception. And if they ever looked at I mean, it almost makes me I don't I'm not I can't
[01:53:47.520 --> 01:53:52.000]  say I'm sympathetic to Facebook. But it almost makes me think that maybe they had an excuse here
[01:53:52.000 --> 01:53:58.560]  like, hey, everybody's doing this. Like there is a rampant market inside loaded iPhone, iOS native
[01:53:58.560 --> 01:54:06.240]  apps that go through the developer certificate system. And I my question, and I wrote yesterday,
[01:54:06.240 --> 01:54:10.720]  I really don't know the answer. Is this something that Apple was was blind to? Or are they
[01:54:10.720 --> 01:54:13.840]  purposefully turning a blind eye to it, and they kind of know what's going on? And
[01:54:14.480 --> 01:54:15.920]  for whatever reason, they accept it?
[01:54:17.120 --> 01:54:23.200]  I mean, there's, I don't know, it's tough to police that. But Apple does have a little bit
[01:54:23.200 --> 01:54:28.880]  of money to hire cops. Like, yeah, I don't know. I mean, I, you know, certainly, I know,
[01:54:28.880 --> 01:54:33.440]  enterprises use this in all sorts of different ways, typically legitimate, but I've seen ones
[01:54:33.440 --> 01:54:38.560]  that are, you know, kind of kind of on the line in terms of employee monitoring sorts of things.
[01:54:38.560 --> 01:54:41.680]  But at that point, that's usually like work devices, they own it, it's not the same thing
[01:54:41.680 --> 01:54:45.600]  as a consumer. And I think that's what those certificates were meant for these ones, I
[01:54:46.720 --> 01:54:49.840]  looking at that build IO, I had no idea it was that big. I mean, I knew
[01:54:49.840 --> 01:54:51.600]  it was happening. I didn't know it was that big.
[01:54:51.600 --> 01:54:56.320]  It does this. It, it seems like it would be a big job to police this,
[01:54:56.320 --> 01:55:02.560]  but it doesn't seem like it would be too big a job for a couple of a company of Apple's resources to
[01:55:02.560 --> 01:55:07.760]  police with a reasonably sized force. And I've been saying for a while, I suggested a couple
[01:55:07.760 --> 01:55:13.600]  weeks ago, like the effectively that I think Apple should have effectively like a bunco squad
[01:55:13.600 --> 01:55:21.280]  for the app store, which would be not entirely, you know, in the way that like the
[01:55:23.600 --> 01:55:26.720]  hopefully everything that goes to the app store, it gets reviewed by the app store reviewers,
[01:55:26.720 --> 01:55:32.400]  and they would catch things like blatant copyright violation, like a game that uses Mario as a
[01:55:32.400 --> 01:55:37.280]  character that is not from Nintendo. Well, you would hope that Apple's app reviewers would
[01:55:37.280 --> 01:55:43.520]  would catch stuff like that. But there are all sorts of other rip offs, there are a ton,
[01:55:43.520 --> 01:55:51.680]  there's just a cottage industry in rip offs, where people find popular apps in category x,
[01:55:51.680 --> 01:55:55.520]  let's just say weather apps, and they find a popular weather app, and then they make an
[01:55:55.520 --> 01:56:01.200]  app that is like a visual clone of a popular app and, and give it a name that is super similar,
[01:56:01.200 --> 01:56:08.320]  and then hope that when you search for that app, their, their, you know, rip off version shows up
[01:56:08.320 --> 01:56:12.400]  high in the results, and they make money. And there's all sorts of goofy, look at the top
[01:56:12.400 --> 01:56:16.720]  grossing charts, you can, it's not that hard to find a bunch of apps that make you think like,
[01:56:16.720 --> 01:56:21.600]  why is this there, you know, like, why are there antivirus utilities that are doing well
[01:56:22.240 --> 01:56:27.680]  in the iOS app store? Like what in the world, you know, given the sandbox restrictions of
[01:56:27.680 --> 01:56:31.040]  of App Store apps, what in the world could you know, whatever you want to say about what
[01:56:31.920 --> 01:56:38.400]  antivirus software is useful for on the desktop? And how many of them are actually useful? And how
[01:56:38.400 --> 01:56:45.920]  many of them aren't on iOS? It's almost ridiculous. Why are they doing so well? And and I saw I sort
[01:56:45.920 --> 01:56:51.040]  of feel like outside the App Store review system, Apple should have a team that is specifically
[01:56:51.040 --> 01:56:56.720]  looking for fraud in the way that a police bunco squad is looking for things like three card money
[01:56:56.720 --> 01:57:02.320]  games and pickpockets and stuff like that. I mean, I think whether or not it's going on now,
[01:57:02.320 --> 01:57:07.840]  it's inevitable. Because I mean, let's think back to the whole reason iOS is so secure. I mean,
[01:57:07.840 --> 01:57:13.360]  you know, remember, Apple pre iOS security was kind of an obscurity thing. And Apple realized
[01:57:13.360 --> 01:57:19.840]  they can't grow the market, unless people can trust their devices. And if the App Store gets
[01:57:19.840 --> 01:57:23.920]  too overloaded, and which is, I think it is approaching that, because I run into the same
[01:57:23.920 --> 01:57:28.560]  issues. I mean, I install apps all the time. And I'm not infrequently I end up installing the wrong
[01:57:28.560 --> 01:57:35.040]  one, because it's plus plus weather, whatever, as opposed to, you know, weather. I mean, you know,
[01:57:35.040 --> 01:57:40.000]  how Apple works. Once the reputation damage hits a particular point, then they they're usually on
[01:57:40.000 --> 01:57:45.760]  it. I think it's just flown under the radar. And it seems like this last year, last 12 months,
[01:57:45.760 --> 01:57:49.680]  it's been kind of becoming part of the social consciousness a little bit more.
[01:57:49.680 --> 01:57:57.200]  I would hope so, you know, and then you get to the other aspect of this, which another recent
[01:57:57.200 --> 01:58:02.720]  scandal in the App Store with these screen recording frameworks. And I don't know if screen
[01:58:02.720 --> 01:58:06.800]  recording is quite the right word. It's not like they were taking movies, but you know, more or
[01:58:06.800 --> 01:58:10.320]  less these frameworks that you can install third party frameworks, and you put them in your app,
[01:58:10.320 --> 01:58:16.240]  and then it'll, you know, give the, you know, let's say you have an app, and then you install
[01:58:16.240 --> 01:58:21.440]  this framework, and then they'll give you feedback on all of your users on what buttons they pressed
[01:58:21.440 --> 01:58:28.320]  and what path they took through, like the first run and all of this stuff, which in theory is
[01:58:28.320 --> 01:58:34.080]  useful, but it's you would think would be, you know, odd to be restricted to your beta testing.
[01:58:34.800 --> 01:58:40.000]  And if you're going to use it in production should have some kind of warning and opt in,
[01:58:40.000 --> 01:58:46.160]  opt out type thing before it starts going. And it was, you know, the story was scandalous enough.
[01:58:46.160 --> 01:58:51.200]  There's another one that was broken by TechCrunch to their credit, that Apple, you know, last weekend
[01:58:51.200 --> 01:58:57.440]  was going through and systematically looking for apps that have these frameworks in them,
[01:58:57.440 --> 01:59:02.560]  and then sending developers notices that, you know, you have 24 hours of submitted updated
[01:59:02.560 --> 01:59:08.160]  app that complies with the terms of the App Store. And again, it gets to a point I wanted to
[01:59:08.160 --> 01:59:13.120]  make with you is that this isn't necessarily a security violation, but it's definitely a
[01:59:13.120 --> 01:59:18.560]  privacy violation. Because you have a reason you have reason to expect that the buttons you click
[01:59:18.560 --> 01:59:22.320]  as you go through an app that you've just downloaded aren't being sent back to the developer.
[01:59:24.240 --> 01:59:32.800]  Yeah, I, both of these issues together are really interesting, because their abuse of the system. So
[01:59:32.800 --> 01:59:36.080]  you talk about trusting the software, I actually come from a different perspective,
[01:59:36.080 --> 01:59:40.720]  I don't trust all the software I run on my systems, which is why I love iOS,
[01:59:40.720 --> 01:59:44.960]  because it compartmentalize it. So I don't trust Facebook worth, Facebook worth a shit,
[01:59:44.960 --> 01:59:50.240]  and I have to use it for various things, family related. And like, I'm in the 501st
[01:59:50.240 --> 01:59:54.080]  Legion, that's all Facebook, or not all Facebook, but a lot of it, the communications is there.
[01:59:54.080 --> 01:59:59.200]  So I need to be on that. But I only run it on iOS. And with all the restrictions around it,
[01:59:59.200 --> 02:00:05.040]  because I trust that Apple has given me a secure compartmentalized platform. So even if I want to
[02:00:05.040 --> 02:00:10.640]  use something untrusted, it's in its own container. And the sideloading of the apps is one way that
[02:00:10.640 --> 02:00:15.600]  can just blow past that. And the screen recording frameworks, it's not as bad because it's it's
[02:00:15.600 --> 02:00:20.320]  still within that app. But if you're not communicating that to the people using it,
[02:00:20.320 --> 02:00:25.840]  and in particular, like, like some apps are fine. But if you're if it's an app that has like private
[02:00:25.840 --> 02:00:32.400]  info, that you think, I don't know, I don't like that. I just don't like slimy, you know,
[02:00:32.400 --> 02:00:36.880]  vendor stuff, if they disclosed it to the users, totally different. I mean, then, you know,
[02:00:36.880 --> 02:00:40.320]  everybody does click streaming now on their apps, like they're, they're keeping track of everything
[02:00:40.320 --> 02:00:44.800]  you click on a web app, as it is, but it's different than like full on screen recording
[02:00:44.800 --> 02:00:52.640]  in my book. And I don't mean to be holier than now. I don't but you know, I had an app in the
[02:00:52.640 --> 02:00:57.760]  App Store recently, you know, Vesper, the text editor, little note taking app, it never in a
[02:00:57.760 --> 02:01:03.920]  million years would have occurred to us to do that. Like it and I realized our app is relatively
[02:01:03.920 --> 02:01:10.880]  simple. And we didn't raise, we didn't have big venture capital, things to satisfy. And we ended
[02:01:10.880 --> 02:01:16.720]  up not being successful. So maybe we should have I don't know. But I, the reason why we failed,
[02:01:16.720 --> 02:01:21.360]  though, wasn't because we didn't track what users were tapping and what they were typing. And
[02:01:22.640 --> 02:01:26.800]  it, I don't know, I wouldn't want that information. You know what I mean? Like,
[02:01:26.800 --> 02:01:33.200]  I feel like that's something that the industry I hope is sort of coming to grips with is that,
[02:01:33.200 --> 02:01:38.640]  that for a long time now, it has been if you can collect it, collect it, because of course,
[02:01:38.640 --> 02:01:44.160]  we want this information, we can do something with it. Whereas I feel like people, companies
[02:01:44.160 --> 02:01:50.000]  are starting to come to grips with the idea of, we don't want it, we don't want this information
[02:01:50.000 --> 02:01:54.800]  on our hands, we don't want it to be there, we don't want to have to protect it. Like if we
[02:01:54.800 --> 02:02:00.080]  don't have it, we can't be hacked and have it stolen. I wish more people thought like that.
[02:02:00.080 --> 02:02:05.120]  Actually, I don't because it would dramatically affect my income. Right. And but everything you
[02:02:05.120 --> 02:02:13.040]  collect as a company about your your users is is a liability in a sense. And maybe you'll come to
[02:02:13.040 --> 02:02:19.200]  the conclusion that the benefits of that data are outweigh the liability. But you can't overlook that
[02:02:19.200 --> 02:02:24.240]  it is a liability because now you've got this thing to protect. Whereas if you don't collect
[02:02:24.240 --> 02:02:28.240]  it at all, you've got nothing to lose. Well, like we have trackers in our product,
[02:02:28.240 --> 02:02:35.360]  it's in beta right now. But they are for errors. They are we track which of the features they're
[02:02:35.360 --> 02:02:43.600]  using or not using not not kind of a screen capture thing. And and if it was a consumer
[02:02:43.600 --> 02:02:47.920]  based app, there's just a level of stuff that that I wouldn't want. And to be honest, I mean,
[02:02:47.920 --> 02:02:52.880]  I think a lot of the a lot of this data correlation and big science and AI stuff like we
[02:02:52.880 --> 02:02:59.360]  hear I think most of us is crap. Like, like, I know Amazon sends everything to Facebook, because
[02:02:59.360 --> 02:03:04.240]  I hurt my knee. I bought it like this thing fitness device to do rehab on it. And I'm in
[02:03:04.240 --> 02:03:11.040]  Facebook and I see ads for the thing I just bought on Amazon. You know, I paid for it. I don't need
[02:03:11.040 --> 02:03:16.320]  two of them. And it lasts more than like the two months since I've had it. And and you know,
[02:03:16.320 --> 02:03:20.720]  it's not like one of them is some rinky dink company. These are two of the big five.
[02:03:20.720 --> 02:03:25.040]  Right. You bought it on Amazon, and the ads are showing up on Facebook. It's not
[02:03:25.040 --> 02:03:29.680]  not seen a single ad that would actually drive me to click on it and buy something
[02:03:29.680 --> 02:03:31.440]  more often than not, it's stuff I've already bought.
[02:03:32.240 --> 02:03:39.680]  Yeah. I recently I don't know, I think I mentioned this a few times recently, but for years up until
[02:03:39.680 --> 02:03:44.960]  just like, maybe like two months ago, I didn't have any ads on Instagram. And I don't know why
[02:03:44.960 --> 02:03:49.360]  I don't know if it. I don't know if it's because I literally signed up on the first day that
[02:03:49.360 --> 02:03:55.520]  Instagram was public. I wasn't on the beta, but I knew people on the beta and I thought that looks
[02:03:55.520 --> 02:03:59.760]  like a neat thing. I'm going to keep my eye open. And then, you know, the day that it launched,
[02:03:59.760 --> 02:04:05.120]  I signed up because I wanted at Gruber. I don't know if it's because I signed up early. I don't
[02:04:05.120 --> 02:04:10.320]  know if it's because I'm slightly internet famous and I got whitelisted for a while. I don't know,
[02:04:10.320 --> 02:04:16.240]  but I went years without getting ads. Now I get Instagram ads and I tapped one one time and then
[02:04:16.240 --> 02:04:22.400]  every single ad I saw after that was for the exact same thing. Everything. I mean,
[02:04:22.400 --> 02:04:26.800]  and that's not tracking across. That's just within the app, you know, but it is.
[02:04:27.440 --> 02:04:33.360]  It was a cars for kids, cars for kids. But it's crazy. It's absolutely crazy. But the
[02:04:33.360 --> 02:04:41.440]  stuff that happens cross company like that is really maddening. And I don't know how they don't
[02:04:41.440 --> 02:04:49.680]  think it's creepy. Like in the real world, if you went into Nordstrom and you were looking at black
[02:04:49.680 --> 02:04:55.920]  boots and you thought, well, you know what? Maybe I want my wife to give her opinion on this. I like
[02:04:55.920 --> 02:04:59.280]  these, but I'm not going to buy them right now. Okay, thanks. Thanks for letting me try them on.
[02:04:59.280 --> 02:05:03.360]  And then you leave and then you go down the street. If somebody like in the next door
[02:05:03.360 --> 02:05:07.200]  down there is like, Hey, Hey, Rich, you want some black boots? Wouldn't you be freaked out?
[02:05:07.200 --> 02:05:14.240]  You'd be freaked out. That would be so weird. I had a, just yesterday, I had a conversation
[02:05:14.240 --> 02:05:18.800]  with somebody and they work at one of the major brands on the security team there. And this is
[02:05:18.800 --> 02:05:23.920]  work-related stuff. And it came up there, big data analytics, like the stuff we're talking about
[02:05:23.920 --> 02:05:28.880]  right now. And I can't give you any context. Like this is like literally household name kind of a
[02:05:28.880 --> 02:05:35.520]  thing. And the guy was laughing. He's like, yeah, we've not seen a single bump in sales because of
[02:05:35.520 --> 02:05:42.000]  this. What I'm guessing is a multimillion dollar program, but it just creeps the out of me.
[02:05:42.000 --> 02:05:49.360]  It goes back to, to the sympathy I have for people who do run webcam blockers, right? Because you
[02:05:49.360 --> 02:05:56.320]  know, something's creepy going on in your computing life where you buy a knee brace at Amazon and all
[02:05:56.320 --> 02:06:01.920]  of a sudden Facebook is showing you, you know, therapeutic knee braces, you know, something
[02:06:01.920 --> 02:06:06.960]  creepy is going on. So where do you, you know, you don't understand what's going on. I don't even
[02:06:06.960 --> 02:06:11.120]  understand exactly how that connection is made. Why not just play it safe and cover up your
[02:06:11.120 --> 02:06:16.720]  webcam? I get it. You know, so I, I, I stand behind my rebuttal to Joanna's piece, but I still,
[02:06:16.720 --> 02:06:22.560]  I have deep sympathy for the people who are just like, see what, just what they see that is going
[02:06:22.560 --> 02:06:26.560]  on in front of their eyes with stuff like that. I can see why they're like, screw this, I'm covering
[02:06:26.560 --> 02:06:32.400]  up. Well, and related the whole thing, like, was Facebook listening into conversations and using
[02:06:32.400 --> 02:06:37.520]  that to drop ads in which we don't think they were, but, but it's not implausible. Like I fully
[02:06:37.520 --> 02:06:41.360]  believe if Facebook thought they could get away with it, they would do that. Right. And you know,
[02:06:41.360 --> 02:06:44.800]  like I've pointed out several times, the microphone doesn't have an indicator light.
[02:06:46.080 --> 02:06:50.880]  So I'm, I kind of hope, I almost think someone moved to Europe, like the GDPR stuff is finally
[02:06:50.880 --> 02:06:55.200]  starting to hit over there and, and actually have an effect, the privacy regulations they have.
[02:06:55.200 --> 02:06:59.040]  I don't think we're going to have anything here, but I do have a feeling, I don't know, maybe the
[02:06:59.040 --> 02:07:04.560]  optimist in me that, you know, there's kind of the potential for a generational shift. I'm okay
[02:07:04.560 --> 02:07:09.360]  giving away some of my privacy. Like I go to Disney world, I get the magic band, they attract
[02:07:09.360 --> 02:07:13.520]  me and my kids and everything we do. And in that context, that isolated context, for whatever
[02:07:13.520 --> 02:07:19.680]  reason, I've made the informed decision. They're not hiding what they're doing there. Within my
[02:07:19.680 --> 02:07:24.080]  computing, I use all Apple, everything, because when they went all in on privacy, which was only
[02:07:24.080 --> 02:07:30.000]  what like, within the last five years, that they started really, really putting that into place,
[02:07:30.000 --> 02:07:33.920]  knowing that I had a nice safe place to go where my privacy was respected, I'll pay extra money
[02:07:33.920 --> 02:07:39.200]  for that. And that's my choice. But the average person doesn't, you know, it's just a markets
[02:07:39.200 --> 02:07:44.320]  thing. And they don't understand necessarily. Although I think the survey data is starting
[02:07:44.320 --> 02:07:46.800]  to show people kind of understand more, and they're not okay with it.
[02:07:46.800 --> 02:07:54.800]  Trenton Larkin Yeah, I would say Apple's. It's a big, slow ship, but they started steering
[02:07:54.800 --> 02:07:58.960]  in the direction of privacy more, I would say about five years is when they went public with
[02:07:58.960 --> 02:08:03.760]  it as something that they touted. But like, one of my favorite stories I got from from somebody
[02:08:03.760 --> 02:08:11.680]  at Apple was the on the creation of iMessage was when they had the idea for iMessage. And that they
[02:08:11.680 --> 02:08:17.200]  had the idea that we could, you know, we could do this end to end encrypted thing. Or we could do,
[02:08:17.200 --> 02:08:23.600]  you know, just do our own messaging service and, and, and sort of usurp SMS when you're
[02:08:23.600 --> 02:08:30.240]  communicating iPhone to iPhone, or and then, you know, they clearly had it in mind, Apple device
[02:08:30.240 --> 02:08:36.960]  to Apple device. And one of the dictums that came down from the very top, I guess, yeah, because
[02:08:36.960 --> 02:08:41.600]  when I message came out, Steve Jobs was still there. But, you know, from Steve Jobs down, it
[02:08:41.600 --> 02:08:48.480]  came from whatever we do engineer this such that we don't have the messages, we don't want them.
[02:08:49.440 --> 02:08:54.160]  We are we don't have them in any form that's readable, you know, that it was never an
[02:08:54.160 --> 02:09:01.440]  afterthought. It was it was from, it was just a notion with no code or diagram. But let's,
[02:09:01.440 --> 02:09:05.120]  you know, one of the top level bullet points of iMessage from the beginning was,
[02:09:05.120 --> 02:09:09.760]  let's engineer this from the ground up where we never have the plain text of these messages ever.
[02:09:11.840 --> 02:09:16.480]  And for the reason of we don't want them, we simply don't want it, there's nothing good can
[02:09:16.480 --> 02:09:19.040]  come of us having it. So let's engineer it that way.
[02:09:20.880 --> 02:09:25.680]  And if you look back, and when they just are hiring history for people on some of the security
[02:09:25.680 --> 02:09:31.200]  and privacy team, you can kind of start seeing when when people who were really well, kind of
[02:09:31.200 --> 02:09:36.400]  kind of respected and cared about that stuff started going in as well. When was iMessage?
[02:09:36.400 --> 02:09:41.360]  When did that first get? Was it with iPhone one? No, no, no, it was later. I'm gonna say like,
[02:09:41.360 --> 02:09:47.920]  2009 was when it was announced famously with. Oh, maybe I'm conflating it with FaceTime. I
[02:09:47.920 --> 02:09:54.240]  don't know. I'm gonna guess 2009. Doesn't matter. Yeah. Yeah. I mean, it takes a long time, but it's
[02:09:54.240 --> 02:09:58.400]  so embedded into their culture. Now, if it had happened in the 80s, I could tell you exactly
[02:09:58.400 --> 02:10:06.560]  what year it was. Yeah. But yeah, that but that's been, you know, but again, that sort of thing that
[02:10:06.560 --> 02:10:12.720]  I've been talking about where they, they, they're, they've been conscious for a while of, let's not
[02:10:12.720 --> 02:10:16.080]  collect anything we don't want to collect, because bad things can happen.
[02:10:17.760 --> 02:10:22.000]  You know, having covered data security, like that was when I used to work at Gartner, like over a
[02:10:22.000 --> 02:10:26.800]  decade ago, that was the part of it that I covered. And one of the top recommendations is, like,
[02:10:26.800 --> 02:10:30.880]  like don't have data that's going to create a liability. All right, here we go. I got the
[02:10:30.880 --> 02:10:37.280]  Wikipedia history. iMessage was announced by Scott Forstall at the WWDC 2011 keynote,
[02:10:37.280 --> 02:10:45.280]  June 6, 2011. Yeah. So there we go. 2011. Yeah, exactly. What were you? I'm sorry,
[02:10:45.280 --> 02:10:49.120]  I interrupted you. No, no, I was saying like, way back when we were advising companies,
[02:10:49.120 --> 02:10:54.160]  don't keep data if you don't want that liability. But in particular, it's on the marketing side,
[02:10:54.160 --> 02:11:00.400]  or anybody in the advertising base has just slurped up that stuff forever. And sometimes
[02:11:00.400 --> 02:11:05.680]  there's interesting things that kind of bite you in the butt. There was, God, was it Dropbox,
[02:11:05.680 --> 02:11:12.560]  I think had to do a disclosure related to GDPR. Like they weren't tracking the data. But what
[02:11:12.560 --> 02:11:19.280]  happened was, there was one part of their system that was saving a log file down that people didn't
[02:11:19.280 --> 02:11:26.160]  even know was being saved down onto that system. And I could be totally wrong if it was Dropbox or
[02:11:26.160 --> 02:11:30.320]  Twitter. I mean, it was just one of the big kind of consumer names. And they fully disclosed it,
[02:11:30.320 --> 02:11:33.840]  closed it down. And a bunch of people started screaming their heads off. And it was like, no,
[02:11:33.840 --> 02:11:38.560]  this is a legitimate technical error. Like somebody didn't turn off this one log thing.
[02:11:38.560 --> 02:11:40.880]  And it was just sitting there in a secure area to begin with.
[02:11:42.560 --> 02:11:46.320]  All right, let me take a break here and thank our third and final sponsors, our good friends at
[02:11:46.320 --> 02:11:52.560]  Squarespace. Look, if you need a new website, or if you have an old website that needs to be redone,
[02:11:53.600 --> 02:12:00.160]  you should do it at Squarespace. Squarespace has everything you need to host, build, design,
[02:12:00.160 --> 02:12:07.280]  update, keep updated a website, everything from domain name registration to picking from a slew
[02:12:07.280 --> 02:12:13.200]  of professionally designed templates, all of which work responsibly on everything from phones
[02:12:13.200 --> 02:12:20.720]  to big screens on the desktop. Everything, any feature you could want on a website, a store
[02:12:20.720 --> 02:12:27.280]  with a catalog and SKUs and do commerce right there, you could do it in Squarespace. A portfolio,
[02:12:27.280 --> 02:12:32.000]  like if you're a designer, and you want to have a portfolio of your work, your personal portfolio,
[02:12:32.000 --> 02:12:39.040]  you can do it right there in Squarespace. Blog, podcast, host it right on Squarespace. Update it.
[02:12:39.040 --> 02:12:43.680]  Update it. How do you update your blog? Go into Squarespace. That's how you post. Everything is
[02:12:43.680 --> 02:12:51.040]  there. It is a terrific service. You can do it all visually. You drag and drop. There's no question
[02:12:51.040 --> 02:12:54.960]  why you're making changes to your Squarespace website, what it's going to look like, because
[02:12:54.960 --> 02:13:00.000]  what you do is right there in the browser window. And when you drag something from the left side to
[02:13:00.000 --> 02:13:04.880]  the right side, you see it right there. It's what you see as you're designing is exactly what people
[02:13:04.880 --> 02:13:11.600]  see when they're visiting your site. It's a great service. They've been sponsoring this show for a
[02:13:11.600 --> 02:13:18.080]  long time, and they keep sponsoring it because people who listen to the show keep signing up
[02:13:18.080 --> 02:13:25.280]  to make new websites at Squarespace. They have great customer support. They have great analytics.
[02:13:25.280 --> 02:13:29.920]  I always mention this because I just love it. Their analytics is such a great interface,
[02:13:29.920 --> 02:13:35.200]  such great data presentation. So once you have a Squarespace website and you want to see where
[02:13:35.200 --> 02:13:38.480]  people are coming from, how many people are visiting, where are they coming from,
[02:13:38.480 --> 02:13:42.960]  you can check it all out in their built-in analytics. Really, really nice.
[02:13:44.880 --> 02:13:48.640]  You can start building your website today at squarespace.com. They have a free trial,
[02:13:48.640 --> 02:13:53.920]  so you don't have to pay anything. See if it works for you before you pay. But then when you do pay,
[02:13:53.920 --> 02:14:00.160]  remember this code, talk show, just T-A-L-K-S-H-O-W. Use that at checkout, and you get 10%
[02:14:00.160 --> 02:14:04.880]  off. And you can pay for up to a year in advance. You can save 10% on the whole year.
[02:14:06.400 --> 02:14:10.320]  It's a really great service. They keep sponsoring the show. I can't thank them enough.
[02:14:10.320 --> 02:14:15.680]  Go to squarespace.com slash talk show and remember that code. Same thing as the URL slug
[02:14:15.680 --> 02:14:21.760]  talk show, and you'll get 10% off your first purchase. That's squarespace.com slash talk show.
[02:14:21.760 --> 02:14:24.720]  All right, we're getting towards the end right here, Rich.
[02:14:27.120 --> 02:14:32.880]  There's this keychain stealer story, which I can't let go by without talking about it.
[02:14:33.920 --> 02:14:37.840]  What the hell happened with this? This seems like a nightmare.
[02:14:39.440 --> 02:14:45.920]  Yeah, again, so this is full on like an overlap of, I don't know, I'm not going to say
[02:14:45.920 --> 02:14:51.760]  stunt hacking in this one. So a younger researcher released a video, had a way to get into the
[02:14:51.760 --> 02:14:55.760]  keychain and steal all the passwords on an entire system, so not just the current user,
[02:14:56.480 --> 02:15:01.280]  and built a tool to go ahead and do that. The functionality of the tool, like Patrick Wardle,
[02:15:01.280 --> 02:15:05.600]  we were talking about earlier, the kid sent him the code. He validated that the whole thing worked,
[02:15:05.600 --> 02:15:09.120]  put a video out there and said he wasn't going to release any more information because Apple
[02:15:09.120 --> 02:15:14.160]  doesn't have a bug bounty program. For the Mac. They don't have a bug bounty program for the Mac.
[02:15:14.160 --> 02:15:18.960]  And even for iOS, it's limited. It's like invite only.
[02:15:19.760 --> 02:15:28.640]  Invite only. So this is like a full on firestorm of disclosure and was it real, was it not real?
[02:15:28.640 --> 02:15:32.640]  So I sent you a link, Dan Gooden did an article over at Ars Technica, which is really good that,
[02:15:33.360 --> 02:15:38.000]  well, yeah, you had to have some pretty deep access onto the system to make this work anyway.
[02:15:38.000 --> 02:15:42.080]  So it is the kind of thing you would put into malware if you're writing malware,
[02:15:42.080 --> 02:15:45.280]  but you've got to get that malware to run in the first place to pull those out.
[02:15:45.280 --> 02:15:48.880]  And that doesn't work under certain conditions. It only had access to certain,
[02:15:48.880 --> 02:15:54.000]  not the iCloud key chain, but the local key chain kind of a thing. So
[02:15:56.160 --> 02:16:00.960]  the issues for me that I thought were fascinating. One is like, I get it. It's your vulnerability.
[02:16:00.960 --> 02:16:06.880]  It's not really a vulnerability. It's your exploit. And you discovered it. And as a
[02:16:06.880 --> 02:16:12.160]  security researcher, it is your right to do with it what you want. But what's the public good here?
[02:16:12.160 --> 02:16:17.360]  Like, you know, releasing a video and saying this is possible, and then saying you're not going to
[02:16:17.360 --> 02:16:22.640]  disclose stuff unless you get paid. Like, you know what, take it, go sell it to Saudi Arabia or
[02:16:22.640 --> 02:16:27.280]  something. Or, you know, the US government, preferably, or whatever, if you want to make
[02:16:27.280 --> 02:16:33.600]  money off of it, if Apple doesn't have that, or release the information responsibly to Apple,
[02:16:33.600 --> 02:16:38.160]  like notify Apple and clean up Apple used to be kind of dicks about that. Now they're pretty good
[02:16:38.160 --> 02:16:44.480]  or much better than they used to be ever much more positive things about really, really positive
[02:16:44.480 --> 02:16:49.520]  things in many cases about researcher relationships. You know, but releasing the
[02:16:49.520 --> 02:16:54.080]  video and creating the hype and doing it that way. I don't know, it's just like, not whatever.
[02:16:55.360 --> 02:16:58.960]  In his defense. I mean, it's not really a defense. But, you know, he is 18.
[02:16:58.960 --> 02:17:06.160]  So maybe it's the folly of youth. Yeah, it's true. And and some of that's a culture of the
[02:17:06.160 --> 02:17:11.840]  on the security side, and particularly the research exploit side where you have to build
[02:17:11.840 --> 02:17:16.800]  up your reputation by kind of releasing stuff in public. And then you get the better to be blunt,
[02:17:16.800 --> 02:17:22.000]  the better job opportunities and stuff. In some cases, it's a weird light. It's like actors,
[02:17:22.000 --> 02:17:26.560]  kind of a meritocracy. It's a meritocracy, but it's not at the same time. It puts Apple in a
[02:17:26.560 --> 02:17:30.720]  weird spot, though, because it's like, you know, it's certainly I'm sure it caught the attention
[02:17:30.720 --> 02:17:35.840]  of Apple security team and the keychain team and they would like to have some, they'd like to see
[02:17:35.840 --> 02:17:42.000]  the code and and fix it. I don't know that they have enough to go on to fix it. You know, so
[02:17:42.000 --> 02:17:47.840]  here's this thing that is, you know, and Patrick Wardle is, you know, has a good reputation. And
[02:17:47.840 --> 02:17:53.120]  it was a former NSA consultant or whatever, whatever he did, you know, used to work with
[02:17:53.120 --> 02:17:58.560]  the NSA and certainly is highly reputable and vouch for it, you know, had access to the code
[02:17:58.560 --> 02:18:05.760]  and said, Yes, you know, does what he says. So it's out there, but Apple has no way to fix it
[02:18:05.760 --> 02:18:11.680]  other than independently finding, you know, the bug that he was using. And I, you know,
[02:18:11.680 --> 02:18:17.200]  from what he showed, it seems very difficult. You know, the proof of concept video doesn't really
[02:18:17.200 --> 02:18:23.600]  show you the how at all. It just shows you that it works. Yeah. And I mean, Apple can likely,
[02:18:23.600 --> 02:18:29.200]  you know, they got smart people, there's enough details in the blog post. And even if you if you
[02:18:29.200 --> 02:18:32.800]  look at the updated posts, he's good. You know, he even says, quote, this is not a security bug
[02:18:32.800 --> 02:18:39.840]  in OS 10. Everything works as designed. This is post exploitation technique. And so I don't
[02:18:39.840 --> 02:18:44.560]  understand what that means. What does that mean? Yeah. So for compromising a system like this,
[02:18:44.560 --> 02:18:48.080]  there's a couple things you need to do. First thing is you need to find that open door. So
[02:18:48.080 --> 02:18:53.360]  that's the vulnerability. So somebody has a crappy lock on their door, the window lock doesn't work.
[02:18:53.360 --> 02:18:58.880]  And that's the thing that you can use to get into the system. Sometimes it's tricking the user to
[02:18:58.880 --> 02:19:04.160]  install something with privileges. I mean, that's the way all that targeted phishing stuff works.
[02:19:04.160 --> 02:19:09.920]  Sometimes it's what we call drive by. If you get like a drive by browser exploit means you
[02:19:09.920 --> 02:19:15.520]  hit a website and it exploits you. It's harder today, we used to have a lot of those. Or there's
[02:19:16.080 --> 02:19:21.200]  some other kind of virus or something that could be transmitted through email, whatever, there's
[02:19:21.200 --> 02:19:26.320]  a variety of techniques. So you got to get your toes, your fingers into that system with that
[02:19:26.320 --> 02:19:31.040]  initial exploit. Once you get that exploit, or that you have to have the vulnerability,
[02:19:31.040 --> 02:19:36.080]  then you have to be able to exploit that. And so if you think about system integrity protection
[02:19:36.080 --> 02:19:43.680]  and ASLR and a variety of, and kernel ASLR, all of those things that you see announced at WWDC
[02:19:43.680 --> 02:19:49.520]  or in those security papers, that's all to reduce what if that vulnerability is exploitable.
[02:19:50.080 --> 02:19:54.640]  So on iOS, all the compartmentalization is even if there's a malicious app, it doesn't have access
[02:19:54.640 --> 02:19:59.120]  to much, it's kind of like stuck in its own little sandbox. So it shouldn't be able to affect other
[02:19:59.120 --> 02:20:03.280]  apps. So you've got the vulnerability, then you have to be able to exploit the vulnerability.
[02:20:03.280 --> 02:20:08.480]  And then you have post exploitation. And that depends on what you're doing. So one of the
[02:20:08.480 --> 02:20:13.280]  things you likely want to do is escalate your privileges to get up to like administrator or
[02:20:13.280 --> 02:20:18.880]  root level. And then there's all sorts of other stuff you do. That's where you install the webcam
[02:20:18.880 --> 02:20:24.880]  sniffer and microphone sniffer and keyboard sniffers or Bitcoin miners. That's all post
[02:20:24.880 --> 02:20:29.600]  exploitation. Have the vulnerability exploited, get your footprint, then do the bad stuff. And
[02:20:29.600 --> 02:20:34.560]  this is a part of the bad stuff. So this is post exploitation, which means you already have to
[02:20:34.560 --> 02:20:40.800]  compromise that system for this thing to work. And it does some things that are bad, but it's,
[02:20:40.800 --> 02:20:47.760]  you know, it's not an actual vulnerability on the system. And Apple does have things to limit
[02:20:48.560 --> 02:20:54.400]  post exploitation in various areas of the system. T2 chip, you can't exploit and
[02:20:54.400 --> 02:21:01.120]  nail the boot ROM. So, for example, one of the things, think about jailbreaks and tethered
[02:21:01.120 --> 02:21:05.760]  versus untethered jailbreaks. So a lot of your listeners probably know the tethered jailbreak,
[02:21:05.760 --> 02:21:10.400]  like you got to redo it every time you reboot your phone, because the boot of the system,
[02:21:10.400 --> 02:21:15.120]  the secure boot process kicks out whatever you did. It's like running in memory. The moment you
[02:21:15.120 --> 02:21:19.840]  power down and power back up, it reboots, it's gone. The T2 chip, for example, on the newer,
[02:21:19.840 --> 02:21:25.440]  I mean, really not a lot of systems have it brings those same protections to the Mac. And there was
[02:21:25.440 --> 02:21:30.800]  earlier stuff to help reduce the persistence of what you could do. So that's kind of that whole
[02:21:30.800 --> 02:21:36.880]  chain of stuff. Yeah, well, and it won't be that long before I, everything like that has to start
[02:21:36.880 --> 02:21:42.000]  slow, like the T2, you know, and it won't, you know, five, six years from now isn't that long.
[02:21:42.000 --> 02:21:47.200]  And then all of a sudden, the vast majority of Macs in active use will have a T2, you know,
[02:21:47.200 --> 02:21:52.160]  it's just how it works. You got to get started. It's one of the cool things about Apple is because
[02:21:52.160 --> 02:21:56.160]  again, they control both that hardware and software. There are other cryptographic
[02:21:56.160 --> 02:22:01.600]  copro or sorry, security coprocessors on the market. Like you go to AMD, and you read through
[02:22:01.600 --> 02:22:07.680]  their stuff, or I'm sorry, arm, and the arm specs have that in there. AMD has chips. And in every
[02:22:07.680 --> 02:22:12.240]  case, it's this secure, you know, it's a system on chip thing that's designed specifically,
[02:22:12.240 --> 02:22:18.320]  and they all have similar functions. Apple's more mature, but because they have both the hardware
[02:22:18.320 --> 02:22:23.200]  and the software, they can do cool things like embed certificates for software updates. So you
[02:22:23.200 --> 02:22:28.160]  can like completely wipe your Mac and start over using a secure, you know, kind of chain because
[02:22:28.160 --> 02:22:34.400]  Apple's actual company certificates embedded. Yeah. Last thing I wanted to talk about, and it's
[02:22:34.400 --> 02:22:39.920]  not really, it's not really, I mean, hopefully it fits for good, but there's that group FaceTime bug
[02:22:39.920 --> 02:22:45.520]  a couple of weeks ago. And that was the other thing that people brought up in response to my,
[02:22:45.520 --> 02:22:49.680]  hey, I don't think you need, I really don't think you need a webcam cover. And they were like, well,
[02:22:49.680 --> 02:22:54.400]  what about the group FaceTime bug that just shows bugs can, you know, if they can, if they're a bug
[02:22:54.400 --> 02:23:01.520]  like that can slip by, I don't trust anything. And I have to say, it's sort of bad timing on
[02:23:01.520 --> 02:23:10.080]  that part. Like it really was a bad bug. Uh, but I can see how it happened. You know, it was just
[02:23:10.080 --> 02:23:15.200]  like a weird, it's like a weird bug in the flow of answering a phone call, you know, a group
[02:23:15.200 --> 02:23:21.120]  FaceTime call where it was like accepting the group FaceTime call before you actually accepted
[02:23:21.120 --> 02:23:28.000]  it and left it connected. Yeah. I mean, but you can see how that spooks people, right?
[02:23:28.000 --> 02:23:34.000]  Yeah. I mean, it's a full legit spook. Like when I read it, I'm like, oh crap,
[02:23:34.000 --> 02:23:39.120]  I think it was an airport. Cause I travel a lot. Do I need to turn my FaceTime off? All right. Well,
[02:23:39.120 --> 02:23:43.440]  somebody calls me on FaceTime other than my wife, then I'll turn FaceTime off. And within an hour,
[02:23:43.440 --> 02:23:48.400]  it was blocked anyway. Um, I mean, these bugs, even if you're really good at security,
[02:23:48.400 --> 02:23:53.840]  they're going to happen sometimes. Uh, Apple does miss stuff like everybody. They're much better
[02:23:53.840 --> 02:24:00.400]  than they, you know, were 10 years ago, 15 years ago. And some of these like bugs,
[02:24:00.400 --> 02:24:06.000]  like people get all, you know, crazy about some of the lock screen bugs, um, that come up and the
[02:24:06.000 --> 02:24:11.920]  lock screen bypasses. Yeah. That's exactly the same sort of thing. Yeah. You can fuzz against
[02:24:11.920 --> 02:24:17.040]  some of these situations and others, it's actually hard to build a test harness to find all of them.
[02:24:17.040 --> 02:24:19.760]  So I don't know the details about how this one slipped through the cracks.
[02:24:19.760 --> 02:24:25.040]  But this, to me, this was a good security story. Like it was bad that it happened. Um,
[02:24:25.040 --> 02:24:29.200]  the reporting and the disclosure stuff was messed up. Kids get a bug bounty by the way.
[02:24:29.200 --> 02:24:36.320]  So that's cool. Um, and, and we know Apple is, is going to fix that. I mean, they,
[02:24:36.320 --> 02:24:40.640]  they publicly apologize for that. Uh, and they blocked at the servers like within an hour.
[02:24:40.640 --> 02:24:44.640]  Like I had people still on Twitter, like three hours later, turn off FaceTime, turn off. No,
[02:24:44.640 --> 02:24:50.240]  and then I'm like, why would you turn off FaceTime? And they're like, well, just in case. Okay. I
[02:24:50.240 --> 02:24:56.960]  mean, have fun with that guys. Yeah. I don't know. I wonder how many people have turned
[02:24:56.960 --> 02:25:03.600]  off FaceTime and left it off in the, in the, in the light of this, it was never more than a
[02:25:03.600 --> 02:25:11.600]  group FaceTime bug. And they did, like you said, I mean, ideally the, the, the, the, the, the,
[02:25:11.600 --> 02:25:18.400]  I mean, ideally the, the family's report would have been somehow escalated and been seen by the
[02:25:18.400 --> 02:25:24.240]  right people and they would have taken action sooner, but it does seem like they were able
[02:25:24.240 --> 02:25:30.000]  to disable it at the server side as quickly as you could expect them to once it did escalate.
[02:25:31.360 --> 02:25:36.480]  Yeah. And then, and individual FaceTime still worked, which was great. So, I mean,
[02:25:36.480 --> 02:25:39.680]  there was the escalation of course, with all of these things in it, and this ties into the
[02:25:39.680 --> 02:25:44.000]  previous story, the whole bug bounty thing comes up and I will say, like, I never thought Apple
[02:25:44.000 --> 02:25:48.320]  had to do a bug bounty. I think they can be valuable. They can not be valuable. It depends
[02:25:48.320 --> 02:25:53.440]  on how you do it. I do think though, now that Apple's had it out there, like if you're going
[02:25:53.440 --> 02:25:57.760]  to do it, really do it. And then they've got a little bit of a half-ass and Microsoft does a
[02:25:57.760 --> 02:26:03.040]  similar thing where there's very definitive bounties for very high value windows exploits.
[02:26:03.040 --> 02:26:10.800]  But ignoring the Mac or whatever, you know, that makes it a little tougher story for them
[02:26:10.800 --> 02:26:15.120]  to kind of justify that. All right. Last but not least, I want to talk about Amazon buying
[02:26:15.120 --> 02:26:22.480]  Eero because Eero, I have to say, is a long time sponsor of this podcast in particular,
[02:26:22.480 --> 02:26:30.000]  but during Fireball, they've sponsored weeks at during Fireball as well. I have Eero Wi-Fi
[02:26:30.000 --> 02:26:36.560]  network equipment. That's what gives me Wi-Fi at home. I'm still using them. I plan to continue
[02:26:36.560 --> 02:26:44.160]  using them. But I have to say, I'm a little disappointed by the news. And I want to talk
[02:26:44.160 --> 02:26:46.480]  about it in an episode when they're not sponsoring the show.
[02:26:47.920 --> 02:26:53.600]  Yeah, I mean, I'm on Ubiquiti myself because of not privacy concerns, just because of the nature,
[02:26:53.600 --> 02:27:00.080]  because I could. Yeah, well, Ubiquiti, from what I understand is higher end, it takes a little bit
[02:27:00.080 --> 02:27:05.600]  more expertise to set up, maybe a lot more expertise. It's good stuff. I've heard nothing
[02:27:05.600 --> 02:27:12.960]  but good things about Ubiquiti's Wi-Fi stuff. I like the sort of just plug it in and forget it
[02:27:12.960 --> 02:27:19.840]  aspect of the Eero thing because I'm lazy. And I hear too, you know, at least till now I do trust
[02:27:19.840 --> 02:27:26.480]  them. And it really is that the ease of setup makes it really easy to recommend to friends and
[02:27:26.480 --> 02:27:32.000]  family. I mean, again, the Eero is not sponsoring this episode of the show. But I will say their
[02:27:32.000 --> 02:27:38.560]  stuff is really, really easy to set up and their app has a really, really good interface for
[02:27:38.560 --> 02:27:47.440]  configuring stuff. Anything that a normal person would want to configure on their Wi-Fi. And I
[02:27:47.440 --> 02:27:54.160]  guess the thing that I wrote was that, look, I liked Eero as an independent company. Just
[02:27:54.160 --> 02:27:58.000]  having a Wi-Fi company that was just doing their own thing. I kind of knew in the back of my head
[02:27:58.000 --> 02:28:03.680]  that the end result for them was probably going to be an acquisition. And I was sort of hoping
[02:28:03.680 --> 02:28:09.600]  it was going to be Apple for privacy reasons. And that maybe that would be Apple's re-entry
[02:28:09.600 --> 02:28:16.000]  in the Wi-Fi market. You know, I could think of worse places than Amazon, you know, could have
[02:28:16.000 --> 02:28:23.680]  been Facebook. But the truth is owning a Wi-Fi base station, you know, gives you everything you
[02:28:23.680 --> 02:28:27.440]  would want in terms of like that stuff that Facebook was doing with their VPN in terms of
[02:28:27.440 --> 02:28:30.880]  like, we'd like to know what people are doing with their network traffic. Well, if you control the
[02:28:30.880 --> 02:28:38.880]  base station, you could do that. Yeah, I had really mixed feelings. So it was Eero or Ubiquiti
[02:28:38.880 --> 02:28:43.600]  when I was deciding on my network. And the main reason I went with Ubiquiti is because I've got
[02:28:43.600 --> 02:28:49.280]  the background and I like to tinker with that stuff. I had a different one before that was an
[02:28:49.280 --> 02:28:54.720]  Eero competitor that's kind of tanked, Luma. Because I had known some people over at the
[02:28:54.720 --> 02:29:02.800]  company when they started that. So here's, put the security hat on, here's a risk assessment.
[02:29:04.000 --> 02:29:08.640]  One is they can't do that without at least notifying you. Right. Track all that info. So
[02:29:08.640 --> 02:29:13.600]  you're covered on there or FTC violations or what's left of the federal government could go
[02:29:13.600 --> 02:29:18.160]  after them. And then they would lock themselves out of like the European market and stuff where
[02:29:18.160 --> 02:29:24.720]  you just can't do that. Two, your cable company or whoever you get your internet from tracks
[02:29:24.720 --> 02:29:30.960]  absolutely everything anyway, as does your phone company. Like that traffic is just there. I hate
[02:29:30.960 --> 02:29:40.960]  it. I've looked at VPNing and I do use a couple of VPNs for at times and mostly when I use Facebook,
[02:29:40.960 --> 02:29:49.200]  just to fuck with Facebook. But the even like I've got gig ethernet, there's not a gig ethernet VPN
[02:29:49.200 --> 02:29:53.280]  supported anywhere I can get my hands on. So there's like a sacrifice you've got to do there.
[02:29:53.280 --> 02:29:57.600]  So that data is getting out anyway and Amazon or whoever could buy it. So I don't mean to diminish
[02:29:57.600 --> 02:30:02.640]  the risk, but the creep factor is really there. And just the fact that that was everybody's first
[02:30:02.640 --> 02:30:08.720]  response. Like you, when Eero got acquired, it's like, damn it, now Amazon's going to read all my
[02:30:08.720 --> 02:30:17.440]  websites. And I have an echo in the kitchen, right next to our home pods. Same here.
[02:30:18.240 --> 02:30:24.960]  I have one. So it's not like I'm anti put, and I've got another one in my living room, which I
[02:30:24.960 --> 02:30:30.000]  really probably could disconnect because I don't use it anymore. But more or less just for
[02:30:30.000 --> 02:30:37.040]  controlling lights and shades verbally. So it's not like I'm opposed to putting Amazon internet
[02:30:37.040 --> 02:30:44.640]  connected devices in my home. I don't know. And like I wrote yesterday, there was a,
[02:30:45.200 --> 02:30:50.080]  I forget who had the story, but somebody asked Amazon for comment, do you plan to change the
[02:30:50.080 --> 02:30:56.640]  terms of service for Eero? And they were like, no, we don't at this time. And I get it. Like I
[02:30:56.640 --> 02:31:01.360]  even wrote, I get it. I get it. They're not going to say no, they're not going to, the ink isn't
[02:31:01.360 --> 02:31:08.160]  even dry on this. It hasn't even been approved. It's not a finalized acquisition. Although I can't
[02:31:08.160 --> 02:31:13.280]  imagine why it wouldn't go through, especially in the Trump administration. And that's not
[02:31:13.280 --> 02:31:21.760]  anti-Trumpism. That's just general Republicanism as being more amenable to, you know, less likely
[02:31:21.760 --> 02:31:29.520]  to pursue, you know, to look askance at an acquisition for anti-competitive reasons.
[02:31:31.280 --> 02:31:36.960]  I mean, I would bet big money that this will, you know, that Amazon will successfully acquire Eero.
[02:31:36.960 --> 02:31:45.920]  Well, it's like, I mean, it just somehow it's like, I don't know. And again, I don't think
[02:31:45.920 --> 02:31:50.160]  that if you're an existing Eero customer, you have as much to worry about as what does it mean going
[02:31:50.160 --> 02:31:56.240]  forward when there are new devices that come with new terms of service? Like, I don't really,
[02:31:56.240 --> 02:32:01.360]  you know, Amazon's not stupid. And I don't think they're evil. I really don't, you know, so they're
[02:32:01.360 --> 02:32:07.600]  not going to do something dumb and, you know, turn your existing Eero's into spy spying devices
[02:32:07.600 --> 02:32:14.240]  without telling you. Well, like, I mean, the vast majority of my professional works on Amazon,
[02:32:14.240 --> 02:32:18.080]  with Amazon Web Services stuff and doing assessments, building things and everything
[02:32:18.080 --> 02:32:24.240]  else. I mean, Amazon, you know, it's another big company. I think what this comes down to in a lot
[02:32:24.240 --> 02:32:29.920]  of these privacy related issues, because I mean, that's a big chunk of what we've talked about
[02:32:29.920 --> 02:32:34.160]  through the last couple hours is like, it's just privacy after privacy after privacy.
[02:32:34.880 --> 02:32:43.680]  And it's a really personal thing. You know, it's like, all of our lines are very personal and not
[02:32:43.680 --> 02:32:47.840]  always logical. Like, I don't do the webcam stickers, but I won't use an Android phone.
[02:32:48.400 --> 02:32:53.440]  I minimize Google services and Facebook, even though I have to use Google for a ton of work
[02:32:53.440 --> 02:32:59.040]  stuff. And then I'm like, well, the Google, the G Suite stuff, they have privacy because you're
[02:32:59.040 --> 02:33:02.800]  paying for it. It's enterprise versus like, there's just all these lines there. And it's
[02:33:02.800 --> 02:33:07.760]  becoming, you know, it's hard to navigate it. But I think when like, with the Eero, you know,
[02:33:07.760 --> 02:33:12.320]  it's something you've come to trust and expect to behave a certain way. And when that it's that
[02:33:12.320 --> 02:33:17.360]  fear that that's going to change. It's just totally legitimate. Well, and the other the flip
[02:33:17.360 --> 02:33:21.680]  side of it, the elephant in the room is the fact that Apple has exited this market, the consumer
[02:33:22.320 --> 02:33:26.240]  Wi Fi base station market. And like you, you've just you've said, like, at least two or three
[02:33:26.240 --> 02:33:31.600]  times during the show that you've, you've gone all in on Apple stuff because you trust them.
[02:33:33.760 --> 02:33:38.880]  As a as a company that more and more promotes itself as a protector of personal privacy,
[02:33:38.880 --> 02:33:48.080]  it just feels like an abdication for Apple to exit the this market. Like I, you know, I presume
[02:33:48.080 --> 02:33:52.160]  I can only presume that there are good business reasons for it that they weren't making significant
[02:33:52.160 --> 02:33:57.200]  money with the airport base stations. You know, if they were I can't, you know, why in the world
[02:33:57.200 --> 02:34:02.320]  would they get out of it. But as a company that wants to promote themselves as hey,
[02:34:02.960 --> 02:34:08.160]  get into our ecosystem, and we were going to protect your privacy because we have no interest
[02:34:08.160 --> 02:34:15.520]  or financial reason otherwise. Boy, without if you if you don't have a base station, you can trust
[02:34:15.520 --> 02:34:20.480]  it's all from moot anyway, right? Like your your your devices, your Mac and your iPhone and your
[02:34:20.480 --> 02:34:28.480]  iPad, you know, can all be as private as as Apple can possibly make them. And if your Wi Fi network
[02:34:28.480 --> 02:34:32.640]  is transmitting everything you do to Amazon, it's all for moot.
[02:34:34.400 --> 02:34:39.120]  That is the single most perplexing thing in Apple's product line to me, even over,
[02:34:39.840 --> 02:34:43.840]  you know, various, we can get all the debates about the Mac books or the different iPad sizes,
[02:34:44.480 --> 02:34:48.880]  you know, or an iPod touches still around. I mean, things like that. The most perplexing
[02:34:48.880 --> 02:34:56.640]  to me is if we look at Apple's longer term strategy of TV, and HomePod, and watches,
[02:34:56.640 --> 02:35:04.800]  and phones, and iPads, and everything else within the home. And as they expand out with HomeKit,
[02:35:05.760 --> 02:35:10.960]  like, like, how do they not have the thing that ties all of those together?
[02:35:10.960 --> 02:35:17.520]  Yeah. I mean, it's it's like the linchpin of it all. And one of the biggest obstacles of getting
[02:35:17.520 --> 02:35:21.840]  somebody moved up on technology with like friends and family who aren't good at this stuff
[02:35:21.840 --> 02:35:26.720]  is routers. Yeah. Like I can't just I can't point them to something and say it just works. I can't
[02:35:26.720 --> 02:35:32.800]  cut it out because a euro or like the neck your Orbeez and stuff are much better. But or with
[02:35:32.800 --> 02:35:37.280]  imagine if it had all the password sharing features that we now have between iOS devices,
[02:35:37.280 --> 02:35:41.360]  like, I mean, there's just so much I don't, I don't know why it's not there.
[02:35:41.360 --> 02:35:46.240]  Yeah, there's a lot that Apple could do with a modern, you know, you know, because the airport
[02:35:46.240 --> 02:35:52.080]  that they sort of stepped away from was years old at the time anyway, you know, that it had been
[02:35:52.080 --> 02:35:55.760]  years since it had been significantly updated. And like you said, there's all sorts of stuff
[02:35:55.760 --> 02:36:01.840]  they've done recently, like with the password sharing stuff. You know, that that if man if it
[02:36:01.840 --> 02:36:05.600]  was built into the base station level would be even more convenient. You know, there's all sorts
[02:36:05.600 --> 02:36:09.760]  of little things like that, like the whole I don't think they've they did an airport update in the
[02:36:09.760 --> 02:36:18.400]  whole era of what was the name of it was like a catch all phrase for this AC. No, but what's the
[02:36:18.400 --> 02:36:24.800]  integrations between like iPhone and, and Mac continuity, continuity, right, right in the era
[02:36:24.800 --> 02:36:30.880]  of continuity. I mean, surely there's ways to do things, you know, that that could make continuity
[02:36:30.880 --> 02:36:37.280]  even more fluid and less of a delay, like when you go to the sharing sheet on iOS, and you want to
[02:36:37.280 --> 02:36:42.960]  open up the current web page on your Mac, you know, just if the base station is looking for
[02:36:42.960 --> 02:36:46.960]  stuff like that, I mean, surely they could make it more convenient. I mean, in the password sharing
[02:36:46.960 --> 02:36:51.920]  thing for the Wi Fi is so cool. Every time it doesn't come up very often. But when it does,
[02:36:51.920 --> 02:36:55.920]  I'm like, Oh, yeah, that's pretty cool. Yeah, I've always closed that window, because I'm
[02:36:55.920 --> 02:37:01.440]  tapping other stuff, because it's so cool. I forget it's there. Well, and I mean, like,
[02:37:01.440 --> 02:37:08.160]  some of the more, you know, just getting devices hooked onto the network. Or, man, I just want my
[02:37:08.160 --> 02:37:14.480]  iTunes Wi Fi backups to work. Like, those don't work on any of my I can't get it to work on any
[02:37:14.480 --> 02:37:19.120]  iOS devices anymore. And it's probably some networking thing that a tool like that could
[02:37:19.120 --> 02:37:22.960]  could just deal with that for me. Yeah, it does. You know, whether we're, you know, whether they
[02:37:22.960 --> 02:37:26.960]  were making a lot of money on it or not, it would, it just seems baffling to me that they got out of
[02:37:26.960 --> 02:37:31.200]  that market. And it's like, I don't know what they, you know, what is Apple's recommendation?
[02:37:31.200 --> 02:37:37.040]  For Okay, so what should I use for my Wi Fi? What do they what can Apple say that they stand behind
[02:37:37.040 --> 02:37:43.120]  and say, here's what we think you should use instead? Yeah, I mean, again, it's just astounding.
[02:37:43.120 --> 02:37:47.760]  It is the glue for their entire ecosystem in the home, and they have nothing for it. Yeah.
[02:37:48.480 --> 02:37:52.960]  It seems untenable, to be honest, like, I don't know, you've got all the little birdies. I don't
[02:37:52.960 --> 02:37:57.040]  have little birdies anymore. Well, I don't have any little birdies that have said anything about
[02:37:57.040 --> 02:38:02.400]  airport or anything along those lines. So, you know, if there's something cooking, I have no
[02:38:02.400 --> 02:38:08.320]  idea. I hope there is. But it sure looks like they're just leaving it aside. And, and, boy,
[02:38:08.320 --> 02:38:14.640]  it really could use Apple. Apple, are you listening? Because I really Yeah, actually,
[02:38:14.640 --> 02:38:19.680]  I mean, I'm, I'm all in on this other stuff. Now is expensive to set it up. But for everybody,
[02:38:19.680 --> 02:38:23.840]  every place else I am, I'd love to have it. Yeah. All right. Rich mogul, thank you for being on the
[02:38:23.840 --> 02:38:31.360]  show. Everybody, your long standing website, and consultancy has been Cirque, secure OSIS, SCC,
[02:38:32.720 --> 02:38:39.360]  you are OS is.com. Probably the easiest way to get there, though, is just google rich mogul,
[02:38:39.360 --> 02:38:43.760]  mo g, ull. And you've got a new now what's your new you mentioned this at the outset of the show,
[02:38:43.760 --> 02:38:48.240]  you got a new startup you're you're you're working at? Tell me it's, it's pretty cool. It's
[02:38:48.240 --> 02:38:54.480]  called disrupt ops. So it's cloud operations and security automation. And basically, I was talking
[02:38:54.480 --> 02:39:00.400]  before about Amazon Web Services, we built a platform that can kind of go in and not just find
[02:39:00.400 --> 02:39:04.480]  problems with your environment, but automatically remediate those integrate with your existing
[02:39:04.480 --> 02:39:08.960]  workflows, do things like, hey, we found this public s3 bucket, we'll send a notification to
[02:39:08.960 --> 02:39:13.440]  the right person. They don't close it in three days, we shut it down. So it's like all this
[02:39:13.440 --> 02:39:21.280]  really cool, like software defined workflows and automation to help secure cloud environments. And
[02:39:21.280 --> 02:39:25.760]  then also, like save money, like, you know, we, hey, you got all this stuff you're not using,
[02:39:25.760 --> 02:39:29.760]  why don't you let us shut it down, stuff like that. Yeah, I just saw somebody somebody recently
[02:39:29.760 --> 02:39:36.720]  who had like a photo sharing thing had a s3 bucket that was public and should not have been public
[02:39:36.720 --> 02:39:41.600]  and had guessable names of people were fishing all sorts of pictures out of there that they
[02:39:41.600 --> 02:39:46.800]  shouldn't have. That's a common one. Yeah. And the weird thing is, is it's like in that is,
[02:39:46.800 --> 02:39:50.560]  in a lot of ways, my CEO, and I fight on that one all the time. I'm like, it's so easy. We
[02:39:50.560 --> 02:39:54.400]  shouldn't talk about it. He goes, everybody still screws it up. Yeah, I don't know why.
[02:39:54.400 --> 02:39:59.680]  I don't know why. But public s3 buckets are a huge one. Yeah. So that's great. So it's
[02:39:59.680 --> 02:40:06.960]  disrupt ops.com. Yep. Is the name of the website. You're and your title is VP of product. Yeah,
[02:40:06.960 --> 02:40:16.960]  actually. Oh, yeah, important. No, not really. Though, people can also and perhaps where people
[02:40:16.960 --> 02:40:21.200]  listening to the show are most familiar with your name is your long, long standing contributor to
[02:40:21.200 --> 02:40:30.240]  tidbits where you write on security related issues. tidbits.com. And on Twitter, our mogul,
[02:40:30.240 --> 02:40:36.800]  and that's with two else. You know, my wife, I told my wife said, who's on the show? And I said,
[02:40:36.800 --> 02:40:41.760]  somebody knew rich, rich mogul. And, and she said, she said, that's his real name.
[02:40:42.800 --> 02:40:46.800]  And I have to admit, I'd never occurred to me, I all the years I've known you, and I've known
[02:40:46.800 --> 02:40:51.760]  you at least 10 years. It never really occurred to me that your name is almost like a comic book
[02:40:51.760 --> 02:40:58.000]  character. So I didn't know I it never clicked for me until I was like a junior in college,
[02:40:58.000 --> 02:41:04.000]  because I'm at a bar. I shoot you not I'm at a bar. And I'm like hitting on a girl. I go,
[02:41:04.000 --> 02:41:08.800]  Hi, I'm rich. Like, I can tell you the name of the bar. Speaking of memory. It was catacombs
[02:41:09.520 --> 02:41:14.160]  in Boulder, Colorado, at the bar, and I'm like, Hey, I'm rich. And she looked at me. She goes,
[02:41:14.160 --> 02:41:22.960]  That's nice. Like, Oh, damn it. Why did I not figure that out earlier?
[02:41:23.840 --> 02:41:29.120]  Jeff, Jeff Bezos, a rich mogul. You the rich mogul?
[02:41:29.120 --> 02:41:35.200]  Yeah, yeah, I don't know if I and I look a little like Louis CK. So I'm in bad company these days.
[02:41:35.200 --> 02:41:39.840]  Yeah. Keep that on the down low. All right. Thank you, Rich. I appreciate it.
[02:41:39.840 --> 02:42:00.560]  Thanks a lot, John. It was fun.
